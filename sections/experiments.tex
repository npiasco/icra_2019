\begin{figure}
	\center
	\includegraphics[width=\linewidth]{vect/dataset_ex}
	\caption{\label{fig:dataset} \textbf{Testing sequences examples:} we compare our new localization method on four challenging localization sequences. The number under the query set name indicates the amount of images to localize according to the 1688 reference images.}
\end{figure}

\section{Experiments}
\label{sec:experiments}

\subsection{Dataset}
\label{subsec:dataset}
	We have tested our new method on the \textit{Oxford Robotcar} public dataset~\cite{Maddern2016}. This is a common dataset used for image based localization~\cite{}(Sattler benchmarking) and loop closure algorithm involving neural networks training~\cite{} (day to night for loop closure).
		
\noindent\textbf{Training data.}
	We use dataset temporal redundancy to build our images triplets to train our CNN. We build 400 triplets using three runs acquired at dates: \texttt{2015-05-19, 2015-08-28} and \texttt{2015-11-10}. We selected a different area of city Oxford than the one used for training our networks for validation.
	Depth modality from \textit{Oxford Robotcar} dataset is extracted from the lidar point cloud. When re-projected in the image frame coordinate, it produces a sparse depth map. Deep convolutional neural networks require dense data as input, so we pre-process these sparse modality maps with inpainting algorithm from~\cite{Bevilacqua2017} in order to make them dense.

\noindent\textbf{Testing data.} We propose four testing scenarios on the same spatial area, separate from both training and testing area. Reference dataset is composed of 1688 images taken every 5 meters along a path of 2 km. The fourth query sets are:
	\paragraph{Sunny/Overcast} queries have been acquired during a sunny day, whereas the reference data have been acquired when the weather was overcast.
	\paragraph{Long-term} queries have been acquired 7 mouths after the reference images under similar weather conditions.
	\paragraph{Winter/Summer} queries have been acquired during a snowy day.
	\paragraph{Night/Day} queries have been acquired at night, resulting in radical visual changes compare to the reference images.

Query examples are presented in figure~\ref{fig:dataset}.
	
\noindent\textbf{Evaluation metric.} For a given query, reference images are ranked according the cosine similarity score computed over their descriptors. To evaluate the localization performances, we consider two evaluation metrics:
	\setcounter{paragraph}{0}
	\paragraph{Recall @N} we plot the recall curve regarding the number $N$ of returned candidates, {\it i.e.} a query is considered well localized if one of the top $N$ retrieved images lies inside the $25m$ radius of the ground truth query position.
	\paragraph{Top-1 recall @D} We compute the distance between the top ranked returned database image position and the query ground truth position, and report the percentage of queries located under a threshold D (from 15 to 50 meters), like in~\cite{Zamir2014}. This metric qualifies the accuracy of the localization system.

\subsection{Implementation details}
\label{subsec:implementation}

For our methods implementation we use Pytorch as deep learning framework, ADAM stochastic gradient descent algorithm for the CNN training with learning rate set to 1e-4, weight decay to 1e-3 and $\lambda$ in triplet loss equations~\ref{eq:triplet_loss},~\ref{eq:depth_triplet_loss},~\ref{eq:cat_triplet_loss} equal to 0.1. We use batch size between 10 and 25 triplets depending of the size of the system to train, convergence is occurring rapidly and take around 30 to 50 epochs. We perform both positive and negative hard minning, as in~\cite{Radenovic2017}. Images and depth maps are resized to 224x224 pixels for training and testing.

\noindent\textbf{Encoder architectures.} We test Alexnet (A) and Resnet18 (R) network architectures for features map extraction. We drop the last pooling layer and the fully connected part for both architectures. The size of the final features block is 256*13*13 for Alexnet and 512*7*7 for Resnet. Initial weights are the ones obtained by training the whole network on ImageNet dataset.

We always use Alexnet encoder to extract features from raw depth map, reconstructed depth map, or hallucinated depth map. Indeed the quality of our depth map are very low, we have found that using deeper network do not significantly improves localization results.

\noindent\textbf{Descriptor architectures.} We test the two state of the art image descriptor MAC~\cite{Radenovic2017} and NetVLAD~\cite{Arandjelovic2017}. MAC is a simple global pooling method that takes the maximum of each feature map from the encoder output. MAC combined with Alexnet encoder produces a 256 dimensions descriptor, whereas combined with Resnet architecture is produce a 512 dimensions descriptor.

NetVLAD is a trainable pooling layer that mimic VLAD aggregation method. For all the experiments, we set the number of clusters to 64, resulting on a 16~384 dimensions descriptor if it is used with Alexnet features and 32~768 dimensions if Resnet features is used instead. The dimension of the final descriptor can be drastically reduced with PCA-whitening post-processing.

Finally, both MAC and NetVLAD descriptors are $L_{2}$ normalized.

\noindent\textbf{Decoder architecture.} The decoder used in our proposal (see figure~\ref{fig:our_method}) is based on Unet architecture and inspired by network generator from~\cite{} (pix2pix). Dimension up-sampling is performed through inverse-convolutions layers. Decoder weights are initialized randomly.

\subsection{Results}
\label{subsec:results}

\begin{figure}
	\center
	\begin{minipage}{0.49\linewidth}
		\includegraphics[width=\linewidth]{plot/fig/rgb_r_trunc_distance}	
	\end{minipage}
	\begin{minipage}{0.49\linewidth}
		\includegraphics[width=\linewidth]{plot/fig/rgb_r_trunc_recall}	
	\end{minipage}
	\caption{\label{fig:trunc_resnet} \textbf{Resnet18 (R) versus truncated Resnet18 (Rt) in combination with NetVLAD pooling:} we show the importance of spatial resolution of the deep feature maps of the encoder used with NetVLAD layer. The truncated version of Resnet18, more than two time lighter the complete one, achieve much better localization results.}
\end{figure}

\begin{figure*}
	\center
	\begin{minipage}{0.14\linewidth}
		\includegraphics[trim={90 140 95 100},clip,width=\linewidth]{plot/fig/legend}	
	\end{minipage}
	\begin{minipage}{0.85\linewidth}
	
	\begin{minipage}{0.49\linewidth}		
		\center
		\includegraphics[width=0.49\linewidth]{plot/fig/sun_distance}	
		\includegraphics[width=0.49\linewidth]{plot/fig/sun_recall}
		
		{\scriptsize a) Sunny/Overcast}
	\end{minipage}
	\begin{minipage}{0.49\linewidth}
		\center
		\includegraphics[width=0.49\linewidth]{plot/fig/snow_distance}	
		\includegraphics[width=0.49\linewidth]{plot/fig/snow_recall}
		
		{\scriptsize b) Winter/Summer}
	\end{minipage}
	
	\begin{minipage}{0.49\linewidth}
		\center
		\includegraphics[width=0.49\linewidth]{plot/fig/lt_distance}	
		\includegraphics[width=0.49\linewidth]{plot/fig/lt_recall}

		{\scriptsize c) Long-term}		
	\end{minipage}
	\begin{minipage}{0.49\linewidth}
		\center	
		\includegraphics[width=0.49\linewidth]{plot/fig/night_distance}	
		\includegraphics[width=0.49\linewidth]{plot/fig/night_recall}
	
		{\scriptsize d) Night/Day}
	\end{minipage}
	
	\end{minipage}

	\caption{\label{fig:results} \textbf{Comparison of our method versus hallucination network and networks trained with only images}: our method (-o-) is superior in almost every scenario compare to hallucination network (-x-). It also beats by a significant margin networks trained with only images (\texttt{--}). NetVLAD descriptors (\textcolor{blue}{blue} and \textcolor{cyan}{cyan} curves) are superior to MAC (\textcolor{red}{red} and \textcolor{magenta}{magenta} curves), specially in term of accuracy (Recall@D curve). Night/day dataset remains the most challenging one.  Best view in colors.}
\end{figure*}

\noindent\textbf{Baselines.} We compare our method with two state of the art baselines:
    \paragraph{RGB only (RGB)} simple networks composed of encoder + descriptor trained with only images, without side depth maps information. We test 4 variants of networks by combining Alexnet (A) or Resnet (R) encoder with MAC or NetVLAD descriptor pooling.
    \paragraph{RGB with depth side information (RGBtD)} networks that use peer of aligned \{image/depth map\} during training step. We compare the proposed approach (our) with our version of hallucination network~\cite{Hoffman2016} (hall). We follow training procedure of~\cite{Hoffman2016} to train the hallucination network, whereas our proposal is trained as explained in~\ref{subsec:training}. 

\noindent\textbf{Truncated Resnet.} We found that NetVLAD descriptor combined with Resnet architecture, RGB + NetVLAD (R), do not perform well. NetVLAD can be view as a pooling method that acts on local deep features densely extracted from the input image. We argue that the spatial resolution of the features block obtained with Resnet encoder is too low compare to other architecture (for instance 13*13 for Alexnet compare to 7*7 for Resnet for an 224*224 input image). We propose a truncated version of Resnet encoder (Rt), created by drooping the end of the network after the 13th convolutional layer. Thus we obtain a features block with greater spatial resolution: 256*14*14 compare to 512*7*7. Recall results on the \textit{Sunny/Overcast} query set for both architectures are presented in figure~\ref{fig:trunc_resnet}. As the truncated version of Resnet encoder clearly dominated the full one, we use the truncated version for the following experiments.

\noindent\textbf{Discussion.} Localization results on the four query sets are presented on figure~\ref{fig:results}. 

Superior compare to hallucination network.

Recall @1

MAC (A) absolute +7\% /RGB +5\% /HALL

NetVLAD (A) absolute +3.3\% /RGB, /HALL failed

Top-1 recall @15

MAC (A) ab +5.6 /MAC +2.3 /HALL

NetVLAD (A) ab +1.66 /RGB

NetVLAD (R) ab +0.33 /RGB

Top-1 recall @50

MAC (A) ab +6 /MAC +3.66 /HALL

NetVLAD (A) ab +3 /RGB

NetVLAD (R) ab +1 /RGB

Highest increase with snow dataset.

On snow mean R@1 increase and mean Top-1 recall @15 mean Top-1 recall @50 

\noindent\textbf{Contribution of the depth information.} In this paragraph we investigate the impact on localization performances provided by the side geometry information on our method. To ensure a fair comparison in term of number of trainable parameters, we introduce RGB$^+$ network that has the same architecture as our proposed method. We train RGB$^+$ with images only to compare the localization results against our method that use side depth information. For training RGB$^+$, we simply remove the loss introduced in equation~\ref{eq:generator}, and make the weights of the decoder trainable when optimizing triplets losses constraints. Results of this experiment with encoder architecture Alexnet are presented in table~\ref{tab/eval_depth}.

\input{tab/depth_eval}

Increasing the size of the system results on a better localization (RGB$^{+}$ + MAC versus RGB + MAC) on the two easiest query sets. Surprisingly RGB$^{+}$ system decrease localization performances on the winter queries compare to RGB. The system has probably over-fitted on the training data that are visually close to queries of ``Sunny/Overcast" set and ``Long-term" set, but quiet different from the queries of ``Winter/Summer" set (see figure~\ref{fig:dataset}). RGBtD + MAC system always produces higher localization results compare to RGB$^{+}$ + MAC, which shows that the side depth information provided during training is wisely used to describe the image location. 

\noindent\textbf{Night to day localisation.} Figure~\ref{fig:ft_night}~\ref{fig:mod_ex}

\begin{figure}
	\center
	\begin{minipage}{0.49\linewidth}
		\includegraphics[width=\linewidth]{plot/fig/nightft_distance}	
	\end{minipage}
	\begin{minipage}{0.49\linewidth}
		\includegraphics[width=\linewidth]{plot/fig/nightft_recall}	
	\end{minipage}
	
	\includegraphics[trim={100 58 165 300},clip,width=0.4\linewidth]{plot/fig/legend_night}
	\caption{\label{fig:ft_night} \textbf{Results on Night/Day query set after fine tuning:} we are able to drastically improve localization performance for the Night/Day challenging scenario by only fine tune the decoder part of our network with weakly annotated data. Best viewed in color.}
\end{figure}

\begin{figure}
	\center
	\includegraphics[width=\linewidth]{vect/mod_ex}	
	\caption{\label{fig:mod_ex} \textbf{Effect of fine tuning with night images on decoder output:}. Decoder trained with daylight images is unable to reconstruct the scene geometry (bottom line). Fine tuning the network with less than 1000 peers \{image, depth map\} acquired by night highly improves the decoder depth map generation. Best viewed in color.}
\end{figure}
