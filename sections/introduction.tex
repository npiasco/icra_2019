\section{Introduction}
\label{sec:intro}

Visual-Based Localization (VBL) is a central topic in robotics and computer vision applications~\cite{Piasco2017}. It consists in retrieving the location of a visual query according to a known absolute reference. VBL is used in many applications such as autonomous driving, augmented reality, robot navigation or SLAM loop closing. In this paper, we address VBL as an image retrieval problem where an input image is compared to a reference pool of localized images. This image-retrieval-like problem is two-step: descriptor computation for both the query and the reference images and similarity association across the descriptors. Since the reference images are associated to a location, by ranking images according to their similarity scores we obtain an approximate location for the query. Numerous works have introduced image descriptors well suited for image retrieval for localization~\cite{Arandjelovic2017,Kim2017a,Gordo2017,Radenovic2017,Liu2018}. 

One of the main challenges of image-based localization remains the mapping of images acquired under changing conditions: cross-season images matching~\cite{Naseer2017a}, long-term localization~\cite{Toft2018}, day to night place recognition~\cite{Torii2015}, etc. Recent approaches use complementary information in order to address these visually challenging localization scenarios (geometric information through point cloud~\cite{Sattler2018,Schonberger2018} or depth maps~\cite{Christie2016}, semantic information~\cite{Ardeshir2014,Christie2016,Naseer2017a}). However geometric or semantic information are not always available, especially in robotic applications when the sensor or the computational load on the robot is limited.

In this paper, we propose a image descriptor that learns, from an image, the corresponding scene geometry, in order to deal with challenging outdoor large-scale image-based localization scenarios. We introduce geometric information during the training step to make our new descriptor robust to visual changes that occur between images taken at different times. Once trained, our system is only used on images to construct a expressive descriptor for image retrieval. This kind of system design is also known as side information learning~\cite{Hoffman2016}, as it uses geometric and radiometric information only during the training step and just radiometric data for the image localization. Our method is especially well-suited for robotic long-term localization when the perceptive sensor on the robot is limited to a camera~\cite{Middelberg2014}, while having access to the full scene geometry off-line~\cite{Paparoditis2012,Maddern2016,Wang2016}. 

The paper is organized as follows. In section~\ref{sec:related_work}, we first revisit recent works related to our method, including:~state of the art image descriptors for large scale outdoor localization, method for localization in changing environment and side information learning approaches. In section~\ref{sec:method}, we describe in detail our new image descriptor trained with side depth information. We illustrate the effectiveness of the proposed method on four challenging scenarios in section~\ref{sec:experiments}. Section~\ref{sec:conclusion} finally concludes the paper.