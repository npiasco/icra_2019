\section{Related Work}
\label{sec:related_work}

\vspace{4pt}\noindent\textbf{Image descriptor for outdoor visual localization.} Standard image descriptors for image retrieval in the context of image localization are usually built by combining sparse features with an aggregation method, such as BoW or VLAD. Specific features re-weighting scheme dedicated to image localization have been introduced in~\cite{Arandjelovic2014}. Authors of~\cite{Sattler2016} introduce a re-ranking routine to improve the localization performances on large-scale outdoor area. More recently, \cite{Arandjelovic2017} introduces NetVLAD, a convolutional neural network that is trained to learn a well-suited image representation for image localization. Numerous other CNN image descriptors have been proposed in the literature~\cite{Kim2017a,Gordo2017,Radenovic2017,Sunderhauf2015a,Liu2018} and achieve state of the art results in image retrieval for localization. Therefore we use CNN image descriptors as base component in our system.

\vspace{4pt}\noindent\textbf{Localization in challenging condition.} In order to deal with visual changes in images taken at different times, \cite{Naseer2018} uses a combination of handcrafted and learned descriptors. \cite{Garg2018} introduces temporal consistency by using a sequence of images, while in our proposal we use only one image as input for our descriptor. In~\cite{Porav2018}, authors synthesize new images to match the appearance of reference images, for instance they synthesized daytime images from night images. Numerous works~\cite{Stenborg2018,Toft2018,Naseer2017a} enhance their visual descriptors by adding semantic information. Although semantic representation is robust for long term localization, it may be costly to obtain. Other methods rely on geometric information like point clouds~\cite{Sattler2018,Schonberger2018}, or 3D structures~\cite{Torii2015}. Geometric information has the advantage of remaining more stable across time comparing to visual information but is not always available. That is why we decide to use depth information as side information in combination with radiometric data to learn a powerful image descriptor.

\vspace{4pt}\noindent\textbf{Learning with side information.} Recent work from~\cite{Li2017b} casts the side information learning problem as a domain adaptation problem, where source domain includes multiples modalities and the target domain is composed of a single modality. Another successful method have been introduced in~\cite{Hoffman2016}: authors train a deep neural network to hallucinate features from a depth map only presented during the training process to improve objects detection in images. The closest work to ours, presented in~\cite{xu2017learning}, uses recreated thermal images to improve pedestrian detection on standard images only. Our system, inspired by~\cite{xu2017learning}, learns how to produce depth maps from images to enhance the description of these images.
