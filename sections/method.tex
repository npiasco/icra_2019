\section{Method}
\label{sec:method}

\begin{figure}
	\center
	\includegraphics[width=\linewidth]{vect/our_method}
	\caption{\label{fig:our_method} \textbf{Image descriptors training with auxiliary depth data:} two encoders are used for extracting deep features map from the main image modality and the auxiliary reconstructed depth map (inferred from our deep decoder). These features are used to create intermediate descriptors that are finally concatenated in one final image descriptor.}
\end{figure}

\begin{figure}
	\center
	\includegraphics[width=\linewidth]{vect/hall_method}
	\caption{\label{fig:hall_method} \textbf{Hallucination network for image descriptors learning:} we train an hallucination network, inpired from~\cite{Hoffman2016}, for the task of global image description. Unlike the proposed method (see figure~\ref{fig:our_method}), hallucination network reproduces feature maps that would have been obtained by a network trained with depth map rather than the deep map itself.}
\end{figure}



\subsection{Overview}
\label{subsec:overview}
We design a new global image description for the task of image based localization. We first extract dense feature maps from an input image with a convolutional neural network encoder. This feature maps is subsequently used to build a compact representation of the scene. State of the art features aggregation methods can be used to construct the image descriptor, such as MAC~\cite{Radenovic2017} or NetVALD~\cite{Arandjelovic2017}. We enhance this standard image descriptor with side depth map information that is only available during the training process. To do so, a deep neural fully convolutional neural network decoder is used to reconstruct the corresponding depth map according to the image. The reconstructed depth is then used to extract a global depth map descriptor. We follow the same procedure used for image: we extract deep feature maps with an encoder before building the descriptor. Finally, the image descriptor and the depth map descriptor are concatenated into a single descriptor. Figure~\ref{fig:our_method} summarizes the whole process of our new method.

\subsection{Training routine}
\label{subsec:training}
We follow standard procedure of descriptor learning methods for training our system. Trainable parameters are $\theta_{I}$ the weights of the image encoder + descriptor, $\theta_{D}$ the weights of the reconstructed depth map encoder + descriptor and $\theta_{G}$ the weights of the decoder used for depth map generation. We use triplet losses as optimization function:
\begin{multline}
	\label{eq:triplet_loss}
	L_{\theta_{I}} = max\left(0, \lambda + \norm{f_{\theta_{I}}(q) - f_{\theta_{I}}(q^+)}_2  \right. \\	
	\left. - \norm{f_{\theta_{I}}(q) - f_{\theta_{I}}(q^-)}_2 \right),
\end{multline}
where $f_{\theta_{I}}(x)$ is the global descriptor of image $x$ and $\{q, q^+, q^-\}$ represent images triplet with $q$ the anchor, $q^+$ the positive example and $q^-$ the negative example. $\lambda$ is an hyperparameter controlling the margin between positive and negative examples. We train the depth map encoder + descriptor in a same manner, equation~\ref{eq:triplet_loss} becoming:
\begin{multline}
	\label{eq:depth_triplet_loss}
	L_{\theta_{D}}  = max\left(0, \lambda + \norm{f_{\theta_{D}}(G(q)) - f_{\theta_{D}}(G(q^+))}_2  \right. \\	
	\left. - \norm{f_{\theta_{D}}(G(q)) - f_{\theta_{D}}(G(q^-))}_2 \right),
\end{multline}
where $f_{\theta_{D}}(x)$ is the global descriptor of depth map $x$ and $G(x)$ is the reconstructed depth map of image $x$ by the decoder.

The final image descriptor is trained with the following loss:
\begin{multline}
	\label{eq:cat_triplet_loss}
	L_{\theta_{I},\theta_{D}} = max\left(0, \lambda + \norm{F_{\theta_{I},\theta_{D}}(q) - F_{\theta_{I},\theta_{D}}(q^+)}_2  \right. \\	
	\left. - \norm{F_{\theta_{I},\theta_{D}}(q) - F_{\theta_{I},\theta_{D}}(q^-)}_2 \right),
\end{multline}
where $F(x)$ denote the concatenation of image descriptor and depth map descriptor:
\begin{equation}
	\label{eq:cat_function}
	F_{\theta_{I},\theta_{D}}(x) = \left[ f_{\theta_{I}}(x), f_{\theta_{D}}(G(x)) \right].
\end{equation}

In order to train the depth map generator, we use a simple $L_1$ loss function:
\begin{equation}
	\label{eq:l1_loss}
	L_{\theta_{G}} = \norm{z_d - G_{\theta_{G}}(z_i)}_{1},
\end{equation}
where $\{z_i, z_d\}$ pair denotes image $z_i$ and corresponding depth map $z_d$.

The whole system is trained according to the following equations:
\begin{align}
	\label{eq:sys_optimization}
	\left( \theta_{I}, \theta_{D} \right) & := arg\,\underset{\theta_{I}, \theta_{D}}{min} \left[ L_{\theta_{I}} + L_{\theta_{D}} + L_{\theta_{I},\theta_{D}} \right], \\
	\left( \theta_{G} \right) & := arg\,\underset{\theta_{G}}{min} \left[ L_{\theta_{G}} \right]. 
\end{align}

We can notice that encoders weights ($\theta_{I}, \theta_{D}$) optimization is unrelated to the decoder weights ($\theta_{G}$) optimization.

\begin{equation}
	\label{eq:generator}
	\hat{z_d} =  G_{\theta_{G}}(E_{\theta_{I}}(z_i)),
\end{equation}

where $\hat{z_d}$ is the reconstructed depth map of image $z_i$ inferred by the decoder and $E_{\theta_{I}}(z_i)$ represents the deep representation of image $z_i$ computed by the encoder.


\subsection{Fine tuning with weakly annotated data}
\label{subsec:data}
As we use triplet margin losses for training encoders weights $(\theta_{I}, \theta_{D})$, we need to gather images triplets. Triplets can be constructed thanks to an absolute position information about images, like GPS-tag~\cite{Arandjelovic2017,Liu2018} or by autonomous computer vision algorithm like Structure from Motion (SfM)~\cite{Godard2017,Radenovic2017,Kim2017a}. In addition, images used for triplets creation must represent the same scenes over different period of time to make the image descriptor robust to visual changes.

In contrast the decoder part of our system $(\theta_{G})$ only needs image depth map aligned pair to be trained. Such data can be easily collected by a mapping mobile system, without using any post-processing. That means we have much more data available for training the decoder part of our system rather that the encoders one. We will show in practice how this can be exploited to fine tune the decoder part to deal with complex localization scenarios in part~\ref{subsec:results}.

\input{tab/data_requirement}

\subsection{Hallucination network for image description}
\label{subsec:hall}
We compare our method with the state of the art system that is capable of training a system with side information, named hallucination network~\cite{Hoffman2016}. Hallucination network is originally designed for object detection and classification in an image. We adapt the work of~\cite{Hoffman2016} to create an image descriptor system beneficing for depth map side modality during training. The system is presented in figure~\ref{fig:hall_method}. The main difference with our proposal is that the side information (\textit{i.e.} the depth map) is reconstructed at a deep feature level rather than at the raw data level. We refer readers to~\cite{Hoffman2016} for more information about the hallucination network.

Advantage of hallucination network over our proposal is that it does not required a decoder network, resulting on a architecture lighter than our method. However, the training procedure of the hallucination network is more complicated and required more complex data that the proposed method. Data requirement for both methods are reported in table~\ref{tab/data}. Hallucination network needs a pre-training step, where image encoder and depth map encoder are trained separately from each other before a final optimization step with the hallucination part of the system. We do not need such initialization for training our system.


