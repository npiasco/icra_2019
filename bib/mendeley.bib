@article{Jegou2012,
author = {J{\'{e}}gou, Herv{\'{e}} and Perronnin, Florent and Douze, Matthijs and Sanchez, Jorge and Schmid, Cordelia and P{\'{e}}rez, Patrick and Schmid, Cordelia},
file = {:home/nathan/Documents/Mendeley Desktop/J{\'{e}}gou et al/IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)/J{\'{e}}gou et al.{\_}2012{\_}Aggregating local image descriptors into compact codes.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
pages = {1--12},
title = {{Aggregating local image descriptors into compact codes}},
year = {2012}
}
@article{Xu2017a,
abstract = {Existing manifold learning methods are not appropriate for image retrieval task, because most of them are unable to process query image and they have much additional computational cost especially for large scale database. Therefore, we propose the iterative manifold embedding (IME) layer, of which the weights are learned off-line by unsupervised strategy, to explore the intrinsic manifolds by incomplete data. On the large scale database that contains 27000 images, IME layer is more than 120 times faster than other manifold learning methods to embed the original representations at query time. We embed the original descriptors of database images which lie on manifold in a high dimensional space into manifold-based representations iteratively to generate the IME representations in off-line learning stage. According to the original descriptors and the IME representations of database images, we estimate the weights of IME layer by ridge regression. In on-line retrieval stage, we employ the IME layer to map the original representation of query image with ignorable time cost (2 milliseconds). We experiment on five public standard datasets for image retrieval. The proposed IME layer significantly outperforms related dimension reduction methods and manifold learning methods. Without post-processing, Our IME layer achieves a boost in performance of state-of-the-art image retrieval methods with post-processing on most datasets, and needs less computational cost.},
archivePrefix = {arXiv},
arxivId = {1707.09862},
author = {Xu, Jian and Wang, Chunheng and Qi, Chengzuo and Shi, Cunzhao and Xiao, Baihua},
eprint = {1707.09862},
file = {:home/nathan/Documents/Mendeley Desktop/Xu et al/arXiv preprint/Xu et al.{\_}2017{\_}Iterative Manifold Embedding Layer Learned by Incomplete Data for Large-scale Image Retrieval.pdf:pdf},
journal = {arXiv preprint},
number = {8},
pages = {1--11},
title = {{Iterative Manifold Embedding Layer Learned by Incomplete Data for Large-scale Image Retrieval}},
url = {http://arxiv.org/abs/1707.09862},
volume = {14},
year = {2017}
}
@article{Taneja2014,
author = {Taneja, Aparna and Ballan, Luca and Pollefeys, Marc},
doi = {10.1007/978-3-319-16814-2_7},
file = {:home/nathan/Documents/Mendeley Desktop/Taneja, Ballan, Pollefeys/Proceedings of the Asian Conference on Computer Vision (ACCV)/Taneja, Ballan, Pollefeys{\_}2014{\_}Never get lost again Vision based navigation using streetview images.pdf:pdf},
isbn = {9783319168135},
issn = {16113349},
journal = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
pages = {99--114},
title = {{Never get lost again: Vision based navigation using streetview images}},
volume = {9007},
year = {2014}
}
@inproceedings{Zitnick2014,
author = {Zitnick, C. Lawrence and Doll{\'{a}}r, Piotr},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
organization = {Springer},
pages = {391--405},
title = {{Edge boxes: Locating object proposals from edges}},
year = {2014}
}
@article{Dube2016,
abstract = {Loop-closure detection on 3D data is a challenging task that has been commonly approached by adapting image-based solutions. Methods based on local features suffer from ambiguity and from robustness to environment changes while methods based on global features are viewpoint dependent. We propose SegMatch, a reliable loop-closure detection algorithm based on the matching of 3D segments. Segments provide a good compromise between local and global descriptions, incorporating their strengths while reducing their individual drawbacks. SegMatch does not rely on assumptions of "perfect segmentation", or on the existence of "objects" in the environment, which allows for reliable execution on large scale, unstructured environments. We quantitatively demonstrate that SegMatch can achieve accurate localization at a frequency of 1Hz on the largest sequence of the KITTI odometry dataset. We furthermore show how this algorithm can reliably detect and close loops in real-time, during online operation. In addition, the source code for the SegMatch algorithm will be made available after publication.},
archivePrefix = {arXiv},
arxivId = {1609.07720},
author = {Dub{\'{e}}, Renaud and Dugas, Daniel and Stumm, Elena and Nieto, Juan and Siegwart, Roland and Cadena, C{\'{e}}sar},
eprint = {1609.07720},
file = {:home/nathan/Documents/Mendeley Desktop/Dub{\'{e}} et al/arXiv preprint/Dub{\'{e}} et al.{\_}2016{\_}SegMatch Segment based loop-closure for 3D point clouds.pdf:pdf},
journal = {arXiv preprint},
title = {{SegMatch: Segment based loop-closure for 3D point clouds}},
url = {http://arxiv.org/abs/1609.07720},
year = {2016}
}
@inproceedings{Knopp2010,
abstract = {We seek to recognize the place depicted in a query image using a database of “street side” images annotated with geolocation in- formation. This is a challenging task due to changes in scale, viewpoint and lighting between the query and the images in the database. One of the key problems in place recognition is the presence of objects such as trees or road markings, which frequently occur in the database and hence cause significant confusion between different places. As the main contri- bution, we show how to avoid features leading to confusion of particular places by using geotags attached to database images as a form of supervi- sion.We develop a method for automatic detection of image-specific and spatially-localized groups of confusing features, and demonstrate that suppressing them significantly improves place recognition performance while reducing the database size. We show the method combines well with the state of the art bag-of-features model including query expan- sion, and demonstrate place recognition that generalizes over wide range of viewpoints and lighting conditions. Results are shown on a geotagged database of over 17K images of Paris downloaded from Google Street View.},
author = {Knopp, Jan and Sivic, Josef and Pajdla, Tomas},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-642-15549-9_54},
file = {:home/nathan/Documents/Mendeley Desktop/Knopp, Sivic, Pajdla/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Knopp, Sivic, Pajdla{\_}2010{\_}Avoiding confusing features in place recognition.pdf:pdf},
isbn = {3642155480},
issn = {03029743},
number = {PART 1},
pages = {748--762},
title = {{Avoiding confusing features in place recognition}},
volume = {6311 LNCS},
year = {2010}
}
@article{Svarm2016,
annote = {R{\'{e}}sum{\'{e}}
Article sur la localisation bas{\'{e}}e vision se basant sur une pipeline classique d'image retriveal (SIFT + FLANN) qui int{\'{e}}gre une m{\'{e}}thode de rejection rapide des outliers en se basant sur l'hypoth{\`{e}}se que l'on connait les orientations {\`{a}} priori des images (et la hauteur de cam{\'{e}}ra par rapport au sol).
Cette approche s'inspire de m{\'{e}}thodes d'optimisation cherchant {\`{a}} g{\'{e}}n{\'{e}}rer des critical points (KKT). Les auteurs commencent par trouver un set de candidats potentiels et appliquent une m{\'{e}}thode rapide de rejection se basant sur la projection d'un c{\^{o}}ne de confiance compotant les inliers, le proj{\'{e}}tent en 2D pour r{\'{e}}duire les dimensions du probl{\`{e}}me. Il prouvent ensuite l'exsitance d'un algorithme {\`{a}} temps polynomial (O(n{\^{}}5)) qui permet de retrouver la position 3D de la cam{\'{e}}ra {\`{a}} partir des candidats restant (apr{\`{e}}s l'{\'{e}}tape de r{\'{e}}jection). Application de la m{\'{e}}thode pour retrouver les positions des cam{\'{e}}ras dans un mod{\`{e}}le reconstruit par SfM.

Ce que je n'ai pas compris
- J'ai rien compris de la m{\'{e}}thode de r{\'{e}}jection

Ce qui est int{\'{e}}ressant
- Peut se plugger {\`{a}} en sorti d'autre m{\'{e}}thode d'image based localization
- Peu supporter un tr{\`{e}}s grand nombre d'outliers

Critiques
- La m{\'{e}}thode appliqu{\'{e}}e quasi-directement (apr{\`{e}}s un simple SIFT + FLANN) n'est pas vraiment plus rapide ni plus pr{\'{e}}cise que l'{\'{e}}tat de l'art

A approfondir
- La m{\'{e}}thode

Computational load
Offline: - 
Online: 0.3s dans la plus part des cas

Scalability
Scale: Place, City
Scalability potential: Plutot bonne vu l'approche},
author = {Svarm, Linus and Enqvist, Olof and Kahl, Fredrik and Oskarsson, Magnus},
doi = {10.1109/TPAMI.2016.2598331},
file = {:home/nathan/Documents/Mendeley Desktop/Svarm et al/IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)/Svarm et al.{\_}2016{\_}City-Scale Localization for Cameras with Known Vertical Direction.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
number = {c},
pages = {1--1},
title = {{City-Scale Localization for Cameras with Known Vertical Direction}},
url = {http://ieeexplore.ieee.org/document/7534854/},
volume = {8828},
year = {2016}
}
@article{Tolias2014,
abstract = {This paper proposes a query expansion technique for image search that is faster and more precise than the existing ones. An enriched representation of the query is obtained by exploiting the binary representation offered by the Hamming Embedding image matching approach: the initial local descriptors are refined by aggregating those of the database, while new descriptors are produced from the images that are deemed relevant. The technique has two computational advantages over other query expansion techniques. First, the size of the enriched representation is comparable to that of the initial query. Second, the technique is effective even without using any geometry, in which case searching a database comprising 105k images typically takes 79 ms on a desktop machine. Overall, our technique significantly outperforms the visual query expansion state of the art on popular benchmarks. It is also the first query expansion technique shown effective on the UKB benchmark, which has few relevant images per query. ?? 2014 Elsevier Ltd.},
author = {Tolias, Giorgos and J{\'{e}}gou, Herv{\'{e}}},
doi = {10.1016/j.patcog.2014.04.007},
file = {:home/nathan/Documents/Mendeley Desktop/Tolias, J{\'{e}}gou/Pattern Recognition/Tolias, J{\'{e}}gou{\_}2014{\_}Visual query expansion with or without geometry Refining local descriptors by feature aggregation.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Hamming embedding,Image retrieval,Query expansion},
number = {10},
pages = {3466--3476},
publisher = {Elsevier},
title = {{Visual query expansion with or without geometry: Refining local descriptors by feature aggregation}},
url = {http://dx.doi.org/10.1016/j.patcog.2014.04.007},
volume = {47},
year = {2014}
}
@article{Qu2016,
annote = {R{\'{e}}sum{\'{e}}
Comparaison des descripteurs SURF et SIFT (impl{\'{e}}mentation OpenCV) pour faire de l'odom{\'{e}}trie visuelle. Detection des descripteur en utilisant une grille pour mieux r{\'{e}}partir la densit{\'{e}} des descripteurs dans l'espace (un peu comme DenseSIFT), puis reconstruction des poses cons{\'{e}}cutives par algo des 5 pts pour les deux premi{\`{e}}res images (dont on connait la position), puis reconstruction de la pose par correspondance 2D/3D sur les points reconstruits initiallement, puis LBA pour am{\'{e}}liorer la pose. Comparaison des descripteurs seulon 4 crit{\`{e}}res : la stabilit{\'{e}}, la variance, la pr{\'{e}}cision de la loc et le cout calculatoire. Conclusion que SIFT est mieux.

Ce que je n'ai pas compris
- Ce que repr{\'{e}}sente conceptuellement la mesure de la variance.

Ce qui est int{\'{e}}ressant
- Pr{\'{e}}cison de la m{\'{e}}thode
- Local BA (tr{\`{e}}s rapide)

Critiques
- Adapt{\'{e}} seulement au probl{\`{e}}me de localisation pr{\'{e}}cises
- Temps de calcul trop long
- Dataset pas challenging

Computational load
Offline: - 
Online: {\textgreater} 2s

Scalability
Scale: -
Scalability potential: -},
author = {Qu, Xiaozhi and Soheilian, Bahman and Habets, Emmanuel and Paparoditis, Nicolas},
doi = {10.5194/isprsarchives-XLI-B3-685-2016},
file = {:home/nathan/Documents/Mendeley Desktop/Qu et al/The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences/Qu et al.{\_}2016{\_}Evaluation of SIFT and SURF for vision based localization.pdf:pdf},
issn = {16821750},
journal = {The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {Feature Extraction,Local Bundle Adjustment,Performance evaluation.,Vision Based Localization},
number = {July},
pages = {685--692},
title = {{Evaluation of SIFT and SURF for vision based localization}},
volume = {41},
year = {2016}
}
@inproceedings{Bay2006,
author = {Bay, Herbert and Tuytelaars, Tinne and {Van Gool}, Luc},
booktitle = {Proceedings of the IEEE European conference on computer vision (ECCV)},
organization = {Springer},
pages = {404--417},
title = {{Surf: Speeded up robust features}},
year = {2006}
}
@inproceedings{Chen2014a,
author = {Chen, Lin and Li, Wen and Xu, Dong},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Chen, Li, Xu/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Chen, Li, Xu{\_}2014{\_}Recognizing RGB Images by Learning from RGB-D Data.pdf:pdf},
title = {{Recognizing RGB Images by Learning from RGB-D Data}},
year = {2014}
}
@article{DeTone2017,
abstract = {We present a point tracking system powered by two deep convolutional neural networks. The first network, MagicPoint, operates on single images and extracts salient 2D points. The extracted points are "SLAM-ready" because they are by design isolated and well-distributed throughout the image. We compare this network against classical point detectors and discover a significant performance gap in the presence of image noise. As transformation estimation is more simple when the detected points are geometrically stable, we designed a second network, MagicWarp, which operates on pairs of point images (outputs of MagicPoint), and estimates the homography that relates the inputs. This transformation engine differs from traditional approaches because it does not use local point descriptors, only point locations. Both networks are trained with simple synthetic data, alleviating the requirement of expensive external camera ground truthing and advanced graphics rendering pipelines. The system is fast and lean, easily running 30+ FPS on a single CPU.},
archivePrefix = {arXiv},
arxivId = {1707.07410},
author = {DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
eprint = {1707.07410},
file = {:home/nathan/Documents/Mendeley Desktop/DeTone, Malisiewicz, Rabinovich/arXiv preprint/DeTone, Malisiewicz, Rabinovich{\_}2017{\_}Toward Geometric Deep SLAM.pdf:pdf},
journal = {arXiv preprint},
keywords = {augmented reality,deep learning,geometry,slam,tracking},
title = {{Toward Geometric Deep SLAM}},
url = {http://arxiv.org/abs/1707.07410},
year = {2017}
}
@article{Brubaker2016,
abstract = {Accurate and efficient self-localization is a critical problem for autonomous systems. This paper describes an affordable solution to vehicle self-localization which uses odometry computed from two video cameras and road maps as the sole inputs. The core of the method is a probabilistic model for which an efficient approximate inference algorithm is derived. The inference algorithm is able to utilize distributed computation in order to meet the real-time requirements of autonomous systems in some instances. Because of the probabilistic nature of the model the method is capable of coping with various sources of uncertainty including noise in the visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, the proposed method is able to localize a vehicle to 4m on average after 52 seconds of driving on maps which contain more than 2,150km of drivable roads.},
annote = {R{\'{e}}sum{\'{e}}
Nouvelle m{\'{e}}thode de localization dans une carte {\`{a}} partir d'une trajectoire acquise en temps r{\'{e}}eel. Les auteurs utilisent une carte disponible sur OpenStreetMap, qu'ils convertissent en un mod{\`{e}}le qu'ils ont d{\'{e}}fini (en prennant s{\'{e}}parant les rues par des tron{\c{c}}ons {\`{a}} sens unique comportant un rayon de courbure et une longeur). L'inf{\'{e}}rence se fait {\`{a}} partir d'un filtre de Kalman et la mod{\'{e}}lisation statistique de la densit{\'{e}} de probabilit{\'{e}} par une mixture de gaussienne. Les auteurs nous informe qu'une mod{\'{e}}lisation compl{\^{e}}te de ce systeme explose plus la trajectoire grandi , et introduisent donc un certain nombre d'approximation pour que {\c{c}}a fonctionne. Ils testent finalement leur syst{\`{e}}me sur des cartes de grandeur 2 km². Ils d{\'{e}}testent {\'{e}}galement plusieurs m{\'{e}}thodes de calcul de la trajectoire : donn{\'{e}}es GPS, odometrie visuel mono/stereo.

Ce que je n'ai pas compris
- Pas lu dans les d{\'{e}}tails

Ce qui est int{\'{e}}ressant
- Original
- Compl{\'{e}}tement ind{\'{e}}pendant de l'apparence (sauf appareance de la map !)

Critiques
- Il faut {\~{}}1 min de traject en voiture pour {\^{e}}tre localiser

Computational load
Offline: ? Creation du modele de la map
Online: on sait pas vraiment pour l'inf{\'{e}}rence

Scalability
Scale: 2 km²
Scalability potential: 18km²},
archivePrefix = {arXiv},
arxivId = {1511.03240},
author = {Brubaker, Marcus A. and Geiger, Andreas and Urtasun, Raquel},
doi = {10.1109/TPAMI.2015.2453975},
eprint = {1511.03240},
file = {:home/nathan/Documents/Mendeley Desktop/Brubaker, Geiger, Urtasun/IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)/Brubaker, Geiger, Urtasun{\_}2016{\_}Map-based probabilistic visual self-localization.pdf:pdf},
isbn = {9781467388511},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
keywords = {Localization,OpenStreetMaps,Visual odometry},
number = {4},
pages = {652--665},
title = {{Map-based probabilistic visual self-localization}},
volume = {38},
year = {2016}
}
@article{Paulin2017,
abstract = {Convolutional neural networks (CNNs) have recently received a lot of attention due to their ability to model local stationary structures in natural images in a multi-scale fashion, when learning all model parameters with supervision. While excellent performance was achieved for image classification when large amounts of labeled visual data are available, their success for un-supervised tasks such as image retrieval has been moderate so far. Our paper focuses on this latter setting and explores several methods for learning patch descriptors without supervision with application to matching and instance-level retrieval. To that effect, we propose a new family of convolutional descriptors for patch representation , based on the recently introduced convolutional kernel networks. We show that our descriptor, named Patch-CKN, performs better than SIFT as well as other convolutional networks learned by artificially introducing supervision and is significantly faster to train. To demonstrate its effectiveness, we perform an extensive evaluation on standard benchmarks for patch and image retrieval where we obtain state-of-the-art results. We also introduce a new dataset called RomePatches, which allows to simultaneously study descriptor performance for patch and image retrieval.},
archivePrefix = {arXiv},
arxivId = {1603.00438},
author = {Paulin, Mattis and Mairal, Julien and Douze, Matthijs and Harchaoui, Zaid and Perronnin, Florent and Schmid, Cordelia},
doi = {10.1007/s11263-016-0924-3},
eprint = {1603.00438},
file = {:home/nathan/Documents/Mendeley Desktop/Paulin et al/International Journal of Computer Vision (IJCV)/Paulin et al.{\_}2017{\_}Convolutional Patch Representations for Image Retrieval An Unsupervised Approach.pdf:pdf},
isbn = {1126301609243},
issn = {15731405},
journal = {International Journal of Computer Vision (IJCV)},
keywords = {Convolutional Neural Networks,Instance-level retrieval,Low-level image description},
number = {1},
pages = {149--168},
title = {{Convolutional Patch Representations for Image Retrieval: An Unsupervised Approach}},
volume = {121},
year = {2017}
}
@article{Cummins2010,
abstract = {We outline an approach for using concentration inequalities to perform rapid approximate multi-hypothesis testing. In a scenario where multiple hypotheses are ranked according to a large set of features, our scheme improves the efficiency of selecting the best hypothesis by providing a {\&}{\#}x201C;bail-out threshold{\&}{\#}x201D; at which unpromising hypotheses can be excluded from further evaluation. We show how concentration inequalities can be used to derive principled bail-out thresholds, subject to a user-specified error tolerance. The technique is similar to the sequential probability ratio test, but is applicable in more general conditions. We apply the technique to improve the speed of the fast-appearance-based mapping system for appearance-based place recognition and mapping. The speed increase provided by the new approach is data dependent, but we demonstrate speed improvements of between 25x - 50x on real data, with only a slight degradation in accuracy.},
annote = {R{\'{e}}sum{\'{e}} 

M{\'{e}}thode de loop closure visuelle pour SLAM. Utilisation d'un arbre de Chow Liu pour mod{\'{e}}liser une distribution a partir de BoW. Utilisation de la probabilit{\'{e}} mod{\'{e}}liser par cette distribution pour d{\'{e}}finir si oui ou non le lieu observ{\'{e}} a d{\'{e}}j{\`{a}} {\'{e}}t{\'{e}} visit{\'{e}}. Description des images par SURF.

Ce que je n'ai pas compris

- Les proba et le Chow Lieu tree

Ce qui est int{\'{e}}ressant

- M{\'{e}}trique probabiliste pour {\'{e}}valuer le matching
- Temps de calcul faible

Critiques

- Tr{\`{e}}s orient{\'{e}} robotique SLAM
- Pas adapt{\'{e}} {\`{a}} de la grande regression de base de donn{\'{e}}es

A approfondir

- Chow Liu + Monte Carlo

Computational load
Offline: 1 ms/place
Online: 1 ms/place (max 5.9s)

Scalability
Scale: Place
Scalability potential: Faible},
author = {Cummins, Mark and Newman, Paul},
doi = {10.1109/TRO.2010.2080390},
file = {:home/nathan/Documents/Mendeley Desktop/Cummins, Newman/IEEE Transactions on Robotics (ToR)/Cummins, Newman{\_}2010{\_}Accelerating FAB-MAP with concentration inequalities.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics (ToR)},
keywords = {Computer vision,recognition,simultaneous localization and mapping (SLAM)},
number = {6},
pages = {1042--1050},
title = {{Accelerating FAB-MAP with concentration inequalities}},
volume = {26},
year = {2010}
}
@inproceedings{Zhou2016b,
author = {Zhou, Hao and Sattler, Torsten and Jacobs, David W.},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision Workshop (ECCVW)},
file = {:home/nathan/Documents/Mendeley Desktop/Zhou, Sattler, Jacobs/Proceedings of the IEEE European Conference on Computer Vision Workshop (ECCVW)/Zhou, Sattler, Jacobs{\_}2016{\_}Evaluating Local Features for Day-Night Matching.pdf:pdf},
keywords = {day-night image matching,evaluation,feature detector,illumination changes,performance},
title = {{Evaluating Local Features for Day-Night Matching}},
year = {2016}
}
@article{Nowicki2017,
author = {Nowicki, Micha{\l} R. and Wietrzykowski, Jan and Skrzypczy{\'{n}}ski, Piotr},
doi = {10.1007/s11277-017-4502-y},
file = {:home/nathan/Documents/Mendeley Desktop/Nowicki, Wietrzykowski, Skrzypczy{\'{n}}ski/Wireless Personal Communications/Nowicki, Wietrzykowski, Skrzypczy{\'{n}}ski{\_}2017{\_}Real-Time Visual Place Recognition for Personal Localization on a Mobile Device.pdf:pdf},
issn = {0929-6212},
journal = {Wireless Personal Communications},
keywords = {indoor localization {\'{a}} mobile,localization,visual place recognition {\'{a}}},
title = {{Real-Time Visual Place Recognition for Personal Localization on a Mobile Device}},
url = {http://link.springer.com/10.1007/s11277-017-4502-y},
year = {2017}
}
@article{Cao2015,
abstract = {Recognizing the location of a query image by matching it to a database is an important problem in computer vision, and one for which the representation of the database is a key issue. We explore new ways for exploiting the structure of a database by representing it as a graph, and show how the rich information embedded in a graph can improve a bag-of-words-based location recognition method. In particular, starting from a graph on a set of images based on visual connectivity, we propose a method for selecting a set of sub graphs and learning a local distance function for each using discriminative techniques. For a query image, each database image is ranked according to these local distance functions in order to place the image in the right part of the graph. In addition, we propose a probabilistic method for increasing the diversity of these ranked database images, again based on the structure of the image graph. We demonstrate that our methods improve performance over standard bag-of-words methods on several existing location recognition datasets. View full abstract},
author = {Cao, Song and Snavely, Noah},
doi = {10.1007/s11263-014-0774-9},
file = {:home/nathan/Documents/Mendeley Desktop/Cao, Snavely/International Journal of Computer Vision (IJCV)/Cao, Snavely{\_}2015{\_}Graph-Based Discriminative Learning for Location Recognition.pdf:pdf},
isbn = {1063-6919},
issn = {15731405},
journal = {International Journal of Computer Vision (IJCV)},
keywords = {Discriminative learning,Image graphs,Location recognition},
number = {2},
pages = {239--254},
title = {{Graph-Based Discriminative Learning for Location Recognition}},
volume = {112},
year = {2015}
}
@inproceedings{Hazirbas2016,
abstract = {In this paper we address the problem of semantic labeling of indoor scenes on RGB-D data. With the availability of RGB-D cameras, it is expected that additional depth measurement will improve the accu- racy. Here we investigate a solution how to incorporate complementary depth information into a semantic segmentation framework by making use of convolutional neural networks (CNNs). Recently encoder-decoder type fully convolutional CNN architectures have achieved a great suc- cess in the field of semantic segmentation. Motivated by this observation we propose an encoder-decoder type network, where the encoder part is composed of two branches of networks that simultaneously extract features from RGB and depth images and fuse depth features into the RGB feature maps as the network goes deeper. Comprehensive exper- imental evaluations demonstrate that the proposed fusion-based archi- tecture achieves competitive results with the state-of-the-art methods on the challenging SUN RGB-D benchmark obtaining 76.27{\%} global ac- curacy, 48.30{\%} average class accuracy and 37.29{\%} average intersection- over-union score.},
author = {Hazirbas, Caner and Ma, Lingni and Domokos, Csaba and Cremers, Daniel},
booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
doi = {10.1007/978-3-319-54181-5_14},
file = {:home/nathan/Documents/Mendeley Desktop/Hazirbas et al/Proceedings of the Asian Conference on Computer Vision (ACCV)/Hazirbas et al.{\_}2016{\_}FuseNet Incorporating depth into semantic segmentation via fusion-based CNN architecture.pdf:pdf},
isbn = {9783319541808},
issn = {16113349},
title = {{FuseNet: Incorporating depth into semantic segmentation via fusion-based CNN architecture}},
volume = {10111 LNCS},
year = {2016}
}
@inproceedings{Kim2017a,
annote = {Augmentation du CNN NetVLAD pour la geo-localisation d'image en introduisant un reseau (CRN) permettant de modifier le poid de certaine zone de l'image. Les auteurs r{\'{e}}cup{\`{e}}re la carte d'activation convolutionnel en sortie du reseau, et pour chaque feature ({\`{a}} un endroit {\{}i,j{\}} de la map), ils prennent en compte les features adjacentes ({\`{a}} trois {\'{e}}chelles diff{\'{e}}rentes), ce qui permet de prendre en compte le contexte de la feature. Le CRN prend donc en entr{\'{e}} la carte d'activation conv et resort un masque qui permet de pond{\'{e}}rer l'importance des features de la carte d'activation. En gros apr{\`{e}}s ils multiplient la carte d'activation originel par ce masque et font la concat{\'{e}}nation par NetVLAD.
Au niveau de l'entrainement ils reprennent le m{\^{e}}me sh{\'{e}}ma que NetVLAD, avec des triplet (I, I+, I-) et une triplet loss function. Pour cr{\'{e}}er les triplets ils v{\'{e}}rifient que I et I+ sont suffisament proche par tag GPS + spatial verification (SIFT + appariment + RANSAC). Pour les I- ils font un peu pareil pour s'assurer que m{\^{e}}me si les tags GPS sont {\'{e}}loign{\'{e}}, il n'y ait pas de landmark commun visible sur I et I-. Pour I et I+ et ils v{\'{e}}rifient {\'{e}}galement que les {\'{e}}chelles des images sont suffisament prochent (pas plus que *2).},
author = {Kim, Hyo Jin and Dunn, Enrique and Frahm, Jan-Michael},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Kim, Dunn, Frahm/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Kim, Dunn, Frahm{\_}2017{\_}Learned Contextual Feature Reweighting for Image Geo-Localization.pdf:pdf},
title = {{Learned Contextual Feature Reweighting for Image Geo-Localization}},
year = {2017}
}
@article{Lowry2016,
abstract = {Visual place recognition is a challenging problem due to the vast range of ways in which the appearance of real-world places can vary. In recent years, improvements in visual sensing capabilities, an ever-increasing focus on long-term mobile robot autonomy, and the ability to draw on state-of-the-art research in other disciplines—particularly recognition in computer vision and animal navigation in neuroscience—have all contributed to significant advances in visual place recognition systems. This paper presents a survey of the visual place recognition research landscape. We start by introducing the concepts behind place recognition—the role of place recognition in the animal kingdom, how a “place” is defined in a robotics context, and the major components of a place recognition system. Long-term robot operations have revealed that changing appearance can be a significant factor in visual place recognition failure; therefore, we discuss how place recognition solutions can implicitly or explicitly account for appearance change within the environment. Finally, we close with a discussion on the future of visual place recognition, in particular with respect to the rapid advances being made in the related fields of deep learning, semantic scene understanding, and video description.},
author = {Lowry, Stephanie and S{\"{u}}nderhauf, Niko and Newman, Paul and Leonard, John J. and Cox, David and Corke, Peter and Milford, Michael J.},
doi = {10.1109/TRO.2015.2496823},
file = {:home/nathan/Documents/Mendeley Desktop/Lowry et al/IEEE Transactions on Robotics (ToR)/Lowry et al.{\_}2016{\_}Visual Place Recognition A Survey.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics (ToR)},
keywords = {Place recognition,Visual place recognition},
number = {1},
pages = {1--19},
title = {{Visual Place Recognition: A Survey}},
volume = {32},
year = {2016}
}
@inproceedings{Guzman-rivera2014,
abstract = {We address the problem of estimating the pose of a cam- era relative to a known 3D scene from a single RGB-D frame. We formulate this problem as inversion of the gener- ative rendering procedure, i.e., we want to find the camera pose corresponding to a rendering of the 3D scene model that is most similar with the observed input. This is a non- convex optimization problem with many local optima. We propose a hybrid discriminative-generative learning archi- tecture that consists of: (i) a set of M predictors which generateM camera pose hypotheses; and (ii) a ‘selector' or ‘aggregator' that infers the best pose from the multiple pose hypotheses based on a similarity function. We are interested in predictors that not only produce good hypotheses but also hypotheses that are different from each other. Thus, we pro- pose and study methods for learning ‘marginally relevant' predictors, and compare their performance when used with different selection procedures. We evaluate our method on a recently released 3D reconstruction dataset with challeng- ing camera poses, and scene variability. Experiments show that our method learns to make multiple predictions that are marginally relevant and can effectively select an accurate prediction. Furthermore, our method outperforms the state- of-the-art discriminative approach for camera relocalization.},
annote = {R{\'{e}}sum{\'{e}}

Am{\'{e}}lioration de la m{\'{e}}thode de localisation {\`{a}} partir d'image RGBD {\'{e}}voqu{\'{e}}e dans Shotton et al. 2013. Ajout d'une {\'{e}}tape de refinement apr{\`{e}}s la pr{\'{e}}dicion de la pose et ajout d'un set de pr{\'{e}}dicteurs disciminant les uns des autres pour am{\'{e}}liorer les r{\'{e}}sultats (type boosting).

Ce que je n'ai pas compris

- Tout l'aspect technique {\&} les notations
- 3DDT pour le calcul de similitude
- C'est quoi la diff{\'{e}}rence du CVPR+M-best compar{\'{e}}e {\`{a}} leur m{\'{e}}thode ?

Ce qui est int{\'{e}}ressant

- L'am{\'{e}}lioration des performances par rapport {\`{a}} l'{\'{e}}tat de l'art
- La m{\'{e}}thode de refinement (pas vraiment explicit{\'{e}} dans le papier)

Critiques

- Temps de calcul non d{\'{e}}taill{\'{e}} mais ils pr{\'{e}}cisent que c'est computionnaly intensive
- Pas de nouvelle m{\'{e}}thode par rapport au calcul de pose
- Pas de challenging matching et dataset petit en taille

A approfondir

- Les m{\'{e}}thodes qui utilisent plusieurs classifieurs
- La m{\'{e}}thode d'alignement avec l'algo de Levenberg-Marquardt

Computational load
Offline: More than Shotton et al. 2013 but paralizable
Online: More than Shotton et al. 2013 but paralizable

Scalability
Scale: Room
Scalability potential: Unknow},
author = {Guzman-Rivera, Abner and Pushmeet, Kohli and Glocker, Ben and Shotton, Jamie and Sharp, Toby and Fitzgibbon, Andrew and Izadi, Shahram},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Guzman-Rivera et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Guzman-Rivera et al.{\_}2014{\_}Multi-Output Learning for Camera Relocalization.pdf:pdf},
keywords = {3D models,Alignement,ICP,Indoor,Machine learning,RGBD request,Supervised learning,Visual place recognition},
mendeley-tags = {3D models,Alignement,ICP,Indoor,Machine learning,RGBD request,Supervised learning,Visual place recognition},
pages = {1--6},
title = {{Multi-Output Learning for Camera Relocalization}},
year = {2014}
}
@article{Li2017a,
author = {Li, Ke and Zou, Changqing and Bu, Shuhui and Liang, Yun and Zhang, Jian and Gong, Minglun},
doi = {10.1016/j.patcog.2017.06.036},
file = {:home/nathan/Documents/Mendeley Desktop/Li et al/Pattern Recognition/Li et al.{\_}2017{\_}Multi-modal Feature Fusion for Geographic Image Annotation.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {cnns,convolutional neural networks,deep learning,geographic image annotation,multi-modal},
number = {Ke Li},
title = {{Multi-modal Feature Fusion for Geographic Image Annotation}},
year = {2017}
}
@inproceedings{Kendall2015,
abstract = {We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 6 degree accuracy for large scale outdoor scenes and 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples. PoseNet code, dataset and an online demonstration is available on our project webpage, at http://mi.eng.cam.ac.uk/projects/relocalisation/},
annote = {R{\'{e}}sum{\'{e}}

CNN entrain{\'{e}} pour d{\'{e}}terminer la position d'une image RGB (int{\'{e}}rieur ou ext{\'{e}}rieur). Base d'entrainement g{\'{e}}n{\'{e}}r{\'{e}} par SfM sur une s{\'{e}}quence vid{\'{e}}o 2D.

Ce que je n'ai pas compris

Comment entrainer le r{\'{e}}seau sur des donn{\'{e}}es dont on n'a pas la position ? (Places dataset)
Qu'est ce qu'un softmax layers ? Comment {\c{c}}a marche t-SNE ?
Quelle sont les traitments fait sur l'image (image Gray) ?
C'est quoi la regression ?

Ce qui est int{\'{e}}ressant

- La rapidit{\'{e}} de l'algorithme
- La robustesse de l'algorithme (flou, de nuit, brouillard, occlusion partielle)
- Peu d'{\'{e}}chantillon pour l'apprentissage

Critiques

- La faible pr{\'{e}}cision ("taux de r{\'{e}}ussite" {\textless} 50{\%})
- Pas d'utilisation de donn{\'{e}}es RGB-D
- Un entrainement par lieu
- La mani{\`{e}}re de calculer la diff{\'{e}}rence d'orientation lors de l'apprentissage (diff{\'{e}}rence de quaternions)

A approfondir

- La m{\'{e}}thode qui outperforme le CNN (SCoRe Forest)
- On utilise cette m{\'{e}}thode pour d{\'{e}}terminer quelle(s) sph{\`{e}}re(s) on va choisir pour le recallage pr{\'{e}}cis
- Modification de l'image en input (filtre de Canny, changement d'espace colorimetrique, HOG, etc.)
- Taille maximal de la zone pouvant {\^{e}}tre trait{\'{e}}e (ils disent qu'ils veulent l'investiguer dans un future papier) ?

Computational load
Offline: CNN pretrained
Online: Near real-time (6ms)

Scalability
Scale: Place
Scalability potential: Theorically possible},
archivePrefix = {arXiv},
arxivId = {1505.07427},
author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.336},
eprint = {1505.07427},
file = {:home/nathan/Documents/Mendeley Desktop/Kendall, Grimes, Cipolla/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Kendall, Grimes, Cipolla{\_}2015{\_}PoseNet A Convolutional Network for Real-Time 6-DOF Camera Relocalization.pdf:pdf},
isbn = {978-1-4673-8391-2},
keywords = {3D models,CNN,Global features,Indoor,Long terme place recognition,Machine learning,Outdoor,RGB request,SfM,Visual place recognition},
mendeley-tags = {3D models,CNN,Global features,Indoor,Long terme place recognition,Machine learning,Outdoor,RGB request,SfM,Visual place recognition},
title = {{PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization}},
year = {2015}
}
@inproceedings{Gallagher2009,
abstract = {Associating image content with their geographic locations has been increasingly pursued in the computer vision community in recent years. In a recent work, large collections of geotagged images were found to be helpful in estimating geo-locations of query images by simple visual nearest-neighbors search. In this paper, we leverage user tags along with image content to infer the geo-location. Our model builds upon the fact that the visual content and user tags of pictures can provide significant hints about their geo-locations. Using a large collection of over a million geotagged photographs, we build location probability maps of user tags over the entire globe. These maps reflect the picture-taking and tagging behaviors of thousands of users from all over the world, and reveal interesting tag map patterns. Visual content matching is performed using multiple feature descriptors including tiny images, color histograms, GIST features, and bags of textons. The combination of visual content matching and local tag probability maps forms a strong geo-inference engine. Large-scale experiments have shown significant improvements over pure visual content-based geo-location inference.},
author = {Gallagher, Andrew and Joshi, Dhiraj and Yu, Jie and Luo, Jiebo},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2009.5204168},
file = {:home/nathan/Documents/Mendeley Desktop/Gallagher et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Gallagher et al.{\_}2009{\_}Geo-location inference from image content and user tags.pdf:pdf},
isbn = {9781424439911},
pages = {55--62},
title = {{Geo-location inference from image content and user tags}},
year = {2009}
}
@article{Morago2016,
annote = {R{\'{e}}sum{\'{e}}
Methode pour matcher des images de facade de batiments qui comporte des motifs r{\'{e}}p{\'{e}}titifs. Necessite des images rectifi{\'{e}}es par vanishing point (pour avoir la facade face {\`{a}} la cam{\'{e}}ra), puis allignement en deux {\'{e}}tapes : d'abord selon la vertical puis selon l'horizontal. Pour l'alligenement vertical, les auteurs detectent des features par detecteur de "croisement de lignes", qui sont ensuite d{\'{e}}crit par SIFT + HOG. Creation d'un vecteur descripteur de l'image en projetant les features sur l'axe y et en consid{\'{e}}rant les features similaires comme une seule entit{\'{e}}. Matching des vecteurs et alignement vertical de l'image, puis alignement horizontal en cherchant des points saillants dans l'image et entre les images. Dernier phase de refinement une fois les deux transformation estim{\'{e}}es.

Ce qui est int{\'{e}}ressant
- VLFeat lib de traitement d'image
- Methode purement g{\'{e}}ometrique, pas d'apprentissage
- Vanishing point correction

Critiques
- Necessite des images urbaines a geometrie retiligne et rectifi{\'{e}}s

Computational load
Offline: -
Online: {\textgreater} 30s

Scalability
Scale: -
Scalability potential: -},
author = {Morago, Brittany and Bui, Giang and Duan, Ye},
doi = {10.1109/TIP.2016.2598612},
file = {:home/nathan/Documents/Mendeley Desktop/Morago, Bui, Duan/IEEE Transactions on Image Processing (ToIP)/Morago, Bui, Duan{\_}2016{\_}2D Matching Using Repetitive and Salient Features in Architectural Images.pdf:pdf},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing (ToIP)},
number = {c},
pages = {1--12},
title = {{2D Matching Using Repetitive and Salient Features in Architectural Images}},
volume = {7149},
year = {2016}
}
@article{Torii2015a,
author = {Torii, Akihiko and Sivic, Josef and Okutomi, Masatoshi and Pajdla, Tomas},
doi = {10.1109/TPAMI.2015.2409868},
file = {:home/nathan/Documents/Mendeley Desktop/Torii et al/IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)/Torii et al.{\_}2015{\_}Visual Place Recognition with Repetitive Structures.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
number = {11},
pages = {2346--2359},
title = {{Visual Place Recognition with Repetitive Structures}},
volume = {37},
year = {2015}
}
@inproceedings{Calonder2010,
abstract = {We propose to use binary strings as an efficient feature point descriptor, which we call BRIEF. We show that it is highly discriminative even when using relatively few bits and can be computed using simple intensity difference tests. Furthermore, the descriptor similarity can be evaluated using the Hamming distance, which is very efficient to com-pute, instead of the L2 norm as is usually done. As a result, BRIEF is very fast both to build and to match. We compare it against SURF and U-SURF on standard benchmarks and show that it yields a similar or better recognition performance, while running in a fraction of the time required by either.},
author = {Calonder, Michael and Lepetit, Vincent and Strecha, Christoph and Fua, Pascal},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-642-15561-1_56},
file = {:home/nathan/Documents/Mendeley Desktop/Calonder et al/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Calonder et al.{\_}2010{\_}BRIEF Binary robust independent elementary features.pdf:pdf},
isbn = {364215560X},
issn = {03029743},
number = {PART 4},
pages = {778--792},
pmid = {19500939},
title = {{BRIEF: Binary robust independent elementary features}},
volume = {6314 LNCS},
year = {2010}
}
@inproceedings{Gasparini2016,
author = {Gasparini, Riccardo and Alletto, Stefano and Serra, Giuseppe and Cucchiara, Rita},
booktitle = {Proceedings of the International Conference on Augmented Reality, Virtual Reality and Computer Graphics (SALENTO AVR)},
doi = {10.1007/978-3-319-40621-3},
file = {:home/nathan/Documents/Mendeley Desktop/Gasparini et al/Proceedings of the International Conference on Augmented Reality, Virtual Reality and Computer Graphics (SALENTO AVR)/Gasparini et al.{\_}2016{\_}Optimizing Image Registration for Interactive Applications.pdf:pdf},
isbn = {978-3-319-40620-6},
number = {June},
title = {{Optimizing Image Registration for Interactive Applications}},
url = {http://link.springer.com/10.1007/978-3-319-40621-3},
volume = {9768},
year = {2016}
}
@article{Tzeng2017,
abstract = {Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They also can improve recognition despite the presence of domain shift or dataset bias: several adversarial approaches to unsupervised domain adaptation have recently been introduced, which reduce the difference between the training and test domain distributions and thus improve generalization performance. Prior generative approaches show compelling visualizations, but are not optimal on discriminative tasks and can be limited to smaller shifts. Prior discriminative approaches could handle larger domain shifts, but imposed tied weights on the model and did not exploit a GAN-based loss. We first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and we use this generalized view to better relate the prior approaches. We propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard cross-domain digit classification tasks and a new more difficult cross-modality object classification task.},
archivePrefix = {arXiv},
arxivId = {1702.05464},
author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
eprint = {1702.05464},
file = {:home/nathan/Documents/Mendeley Desktop/Tzeng et al/arXiv preprint/Tzeng et al.{\_}2017{\_}Adversarial Discriminative Domain Adaptation.pdf:pdf},
journal = {arXiv preprint},
title = {{Adversarial Discriminative Domain Adaptation}},
url = {http://arxiv.org/abs/1702.05464},
year = {2017}
}
@inproceedings{Bian2017,
abstract = {Incorporating smoothness constraints into feature matching is known to enable ultra-robust matching. How- ever, such formulations are both complex and slow, making them unsuitable for video applications. This paper proposes GMS (Grid-based Motion Statistics), a simple means of en- capsulating motion smoothness as the statistical likelihood of a certain number of matches in a region. GMS enables translation of high match numbers into high match qual- ity. This provides a real-time, ultra-robust correspondence system. Evaluation on videos, with low textures, blurs and wide-baselines show GMS consistently out-performs other real-time matchers and can achieve parity with more so- phisticated, much slower techniques.},
author = {Bian, Jiawang and Lin, Wen-yan and Matsushita, Yasuyuki and Yeung, Sai-kit and Nguyen, Tan-dat and Cheng, Ming-ming},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Bian et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Bian et al.{\_}2017{\_}GMS Grid-based Motion Statistics for Fast, Ultra-robust Feature Correspondence.pdf:pdf},
title = {{GMS: Grid-based Motion Statistics for Fast, Ultra-robust Feature Correspondence}},
year = {2017}
}
@article{Dawood2016,
abstract = {This paper aims at demonstrating the usefulness of integrating virtual 3D models in vehicle localization systems. Usually, vehicle localization algorithms are based on multi-sensor data fusion. Global Navigation Satellite Systems GNSS, as Global Positioning System GPS, are used to provide measurements of the geographic location. Nevertheless, GNSS solutions suffer from signal attenuation and masking, multipath phenomena and lack of visibility, especially in urban areas. That leads to degradation or even a total loss of the positioning information and then unsatisfactory performances. Dead-reckoning and inertial sensors are then often added to back up GPS in case of inaccurate or unavailable measurements or if high frequency location estimation is required. However, the dead-reckoning localization may drift in the long term due to error accumulation. To back up GPS and compensate the drift of the dead reckoning sensors based localization, two approaches integrating a virtual 3D model are proposed in registered with respect to the scene perceived by an on-board sensor. From the real/virtual scenes matching, the transformation (rotation and translation) between the real sensor and the virtual sensor (whose position and orientation are known) can be computed. These two approaches lead to determine the pose of the real sensor embedded on the vehicle. In the first approach, the considered perception sensor is a camera and in the second approach, it is a laser scanner. The first approach is based on image matching between the virtual image extracted from the 3D city model and the real image acquired by the camera. The two major parts are: 1. Detection and matching of feature points in real and virtual images (three features points are compared: Harris corner detector, SIFT and SURF). 2. Pose computation using POSIT algorithm. The second approach is based on the on-board horizontal laser scanner that provides a set of distances between it and the environment. This set of distances is matched with depth information (virtual laser scan data), provided by the virtual 3D city model. The pose estimation provided by these two approaches can be integrated in data fusion formalism. In this paper the result of the first approach is integrated in IMM UKF data fusion formalism. Experimental results obtained using real data illustrate the feasibility and the performances of the proposed approaches.},
author = {Dawood, Maya and Cappelle, Cindy and {El Najjar}, Maan E. and Khalil, Mohamad and {El Hassan}, Bachar and Pomorski, Denis and Peng, Jing},
doi = {10.1016/j.trc.2015.12.003},
file = {:home/nathan/Documents/Mendeley Desktop/Dawood et al/Transportation Research Part C Emerging Technologies/Dawood et al.{\_}2016{\_}Virtual 3D city model as a priori information source for vehicle localization system.pdf:pdf},
issn = {0968090X},
journal = {Transportation Research Part C: Emerging Technologies},
keywords = {3D-GIS,Data fusion,GPS,Image processing,Intelligent vehicle,Localization},
pages = {1--22},
publisher = {Elsevier Ltd},
title = {{Virtual 3D city model as a priori information source for vehicle localization system}},
url = {http://dx.doi.org/10.1016/j.trc.2015.12.003},
volume = {63},
year = {2016}
}
@inproceedings{Brubaker2013,
abstract = {In this paper we propose an affordable solution to self-localization, which utilizes visual odometry and road maps as the only inputs. To this end, we present a probabilistic model as well as an efficient approximate inference algorithm, which is able to utilize distributed computation to meet the real-time requirements of autonomous systems. Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads.},
author = {Brubaker, Marcus A. and Geiger, Andreas and Urtasun, Raquel},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2013.393},
file = {:home/nathan/Documents/Mendeley Desktop/Brubaker, Geiger, Urtasun/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Brubaker, Geiger, Urtasun{\_}2013{\_}Lost! leveraging the crowd for probabilistic visual self-localization.pdf:pdf},
isbn = {1063-6919 VO -},
issn = {10636919},
keywords = {localization,mixture model,visual odometry},
pages = {3057--3064},
title = {{Lost! leveraging the crowd for probabilistic visual self-localization}},
year = {2013}
}
@article{Hou2017a,
author = {Hou, Yi and Zhang, Hong and Zhou, Shilin},
file = {:home/nathan/Documents/Mendeley Desktop/Hou, Zhang, Zhou/Journal of Intelligent Robots and Systems/Hou, Zhang, Zhou{\_}2017{\_}Evaluation of Object Proposals and ConvNet Features for Landmark-based Visual Place Recognition.pdf:pdf},
journal = {Journal of Intelligent Robots and Systems},
keywords = {ConvNet feature,Convolutional neural networks,Object proposal,Place description,Place recognition,convnet feature,convolutional neural networks,object,place description,place recognition,proposal},
publisher = {Journal of Intelligent {\&} Robotic Systems},
title = {{Evaluation of Object Proposals and ConvNet Features for Landmark-based Visual Place Recognition}},
year = {2017}
}
@inproceedings{Griffith2016,
author = {Griffith, Shane and Pradalier, C{\'{e}}dric},
booktitle = {British Machine Vision Conference (BMVC)},
file = {:home/nathan/Documents/Mendeley Desktop/Griffith, Pradalier/British Machine Vision Conference (BMVC)/Griffith, Pradalier{\_}2016{\_}Reprojection Flow for Image Registration Across Seasons.pdf:pdf},
pages = {1--12},
title = {{Reprojection Flow for Image Registration Across Seasons}},
year = {2016}
}
@inproceedings{Lin2015,
author = {Lin, Tsung-Yi and Cui, Yin and Belongie, Serge and Hays, James},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Lin et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Lin et al.{\_}2015{\_}Learning Deep Representations for Ground-to-Aerial Geolocalization.pdf:pdf},
isbn = {9781467369640},
number = {JUNE},
pages = {5007--5015},
title = {{Learning Deep Representations for Ground-to-Aerial Geolocalization}},
year = {2015}
}
@inproceedings{Chen2014,
abstract = {Recently Convolutional Neural Networks (CNNs) have been shown to achieve state-of-the-art performance on various classification tasks. In this paper, we present for the first time a place recognition technique based on CNN models, by combining the powerful features learnt by CNNs with a spatial and sequential filter. Applying the system to a 70 km benchmark place recognition dataset we achieve a 75{\%} increase in recall at 100{\%} precision, significantly outperforming all previous state of the art techniques. We also conduct a comprehensive performance comparison of the utility of features from all 21 layers for place recognition, both for the benchmark dataset and for a second dataset with more significant viewpoint changes.},
archivePrefix = {arXiv},
arxivId = {1411.1509},
author = {Chen, Zetao and Lam, Obadiah and Jacobson, Adam and Milford, Michael J.},
booktitle = {Proceedings of the Australasian Conference on Robotics and Automation (ACRA)},
eprint = {1411.1509},
file = {:home/nathan/Documents/Mendeley Desktop/Chen et al/Proceedings of the Australasian Conference on Robotics and Automation (ACRA)/Chen et al.{\_}2014{\_}Convolutional Neural Network-based Place Recognition.pdf:pdf},
pages = {8},
title = {{Convolutional Neural Network-based Place Recognition}},
year = {2014}
}
@article{Pan2010,
author = {Pan, Sinno Jialin and Yang, Qiang},
doi = {10.1109/TKDE.2009.191},
file = {:home/nathan/Documents/Mendeley Desktop/Pan, Yang/IEEE Transactions on Knowledge and Data Engineering/Pan, Yang{\_}2010{\_}A Survey on Transfer Learning.pdf:pdf},
isbn = {1041-4347 VO - 22},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
number = {10},
pages = {1--15},
title = {{A Survey on Transfer Learning}},
volume = {1},
year = {2010}
}
@inproceedings{Saha2017,
author = {Saha, Soham and Varma, Girish and Jawahar, C.V.},
file = {:home/nathan/Documents/Mendeley Desktop/Saha, Varma, Jawahar/Unknown/Saha, Varma, Jawahar{\_}2017{\_}Compressing Deep Neural Networks for Recognizing Places.pdf:pdf},
title = {{Compressing Deep Neural Networks for Recognizing Places}},
year = {2017}
}
@article{Ji2015,
abstract = {Georeferencing image sequences is critical for mobile mapping systems. Traditional methods such as bundle adjustment need adequate and well-distributed ground control points (GCP) when accurate GPS data are not available in complex urban scenes. For applications of large areas, automatic extraction of GCPs by matching vehicle-born image sequences with geo-referenced ortho-images will be a better choice than intensive GCP collection with field surveying. However, such image matching generated GCP's are highly noisy, especially in complex urban street environments due to shadows, occlusions and moving objects in the ortho images. This study presents a probabilistic solution that integrates matching and localization under one framework. First, a probabilistic and global localization model is formulated based on the Bayes' rules and Markov chain. Unlike many conventional methods, our model can accommodate non-Gaussian observation. In the next step, a particle filtering method is applied to determine this model under highly noisy GCP's. Owing to the multiple hypotheses tracking represented by diverse particles, the method can balance the strength of geometric and radiometric constraints, i.e., drifted motion models and noisy GCP's, and guarantee an approximately optimal trajectory. Carried out tests are with thousands of mobile panoramic images and aerial ortho-images. Comparing with the conventional extended Kalman filtering and a global registration method, the proposed approach can succeed even under more than 80{\%} gross errors in GCP's and reach a good accuracy equivalent to the traditional bundle adjustment with dense and precise control.},
annote = {R{\'{e}}sum{\'{e}}

Pipeline pour retrouver le point de vue d'une image {\`{a}} la repr{\'{e}}sentation non photographique d'un batiment {\`{a}} partir d'un mod{\`{e}}le 3D du batiment ou d'une ville.
1) Extraction de features discriminantes {\`{a}} partir du mod{\`{e}}le 3D
2) Matching d'une repr{\'{e}}sentation non photographique avec les features extraites et recalage de l'image

Ce que je n'ai pas compris

Section 4.2 - 4.5 : comment selectionner les bonnes features ? Comment initialiser les features ? Comment calculer la fonction de coup ? Apprentissage supervis{\'{e}} ou non-supervis{\'{e}} ?

Ce qui est int{\'{e}}ressant

Deux axes :
1) recherche global
2) recallage

Le descripteur utilis{\'{e}} (HOG) et la m{\'{e}}thode pour choisir un set de classieurs discriminants
Matching tr{\`{e}}s challenging (donn{\'{e}}es tr{\`{e}}s bruit{\'{e}})

Critiques

- temps de calcul long
- donn{\'{e}}es que l'on cherche {\`{a}} recaller qui ne sont pas des photos (du coup {\c{c}}a doit marcher encore mieux mais peut {\^{e}}tre overkill ?)
- il faut un mod{\`{e}}le 3D
- 50{\%} de bonne classification
- donn{\'{e}}es RGB seulement pour la cible

A approfondir

- LDA
- dataset Hauagge and Snavely
- les 3 types d'allignements {\'{e}}voqu{\'{e}} dans l'introduction
- HOG based linear classifier
- ICP like fine alignement

Computational load
Offline: 2.000 elements/h
Online: 22 minutes for matching the query + 25s for final pose retrieval

Scalability
Scale: Place
Scalability potential: -},
author = {Ji, Shunping and Shi, Yun and Shan, Jie and Shao, Xiaowei and Shi, Zhongchao and Yuan, Xiuxiao and Yang, Peng and Wu, Wenbin and Tang, Huajun and Shibasaki, Ryosuke},
doi = {10.1016/j.isprsjprs.2015.03.005},
file = {:home/nathan/Documents/Mendeley Desktop/Ji et al/ISPRS Journal of Photogrammetry and Remote Sensing/Ji et al.{\_}2015{\_}Particle filtering methods for georeferencing panoramic image sequence in complex urban scenes.pdf:pdf},
isbn = {0924-2716},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Gross errors,Image matching,Kalman filtering,Mobile mapping,Monte Carlo,Particle filtering},
pages = {1--12},
publisher = {International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
title = {{Particle filtering methods for georeferencing panoramic image sequence in complex urban scenes}},
url = {http://dx.doi.org/10.1016/j.isprsjprs.2015.03.005},
volume = {105},
year = {2015}
}
@article{Wang2017,
abstract = {Nearest neighbor search is a problem of finding the data points from the database such that the distances from them to the query point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this paper, we present a comprehensive survey of the learning to hash algorithms, categorize them according to the manners of preserving the similarities into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, as well as quantization, and discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different though quantization, as we show, can be derived from preserving the pairwise similarities. In addition, we present the evaluation protocols, and the general performance analysis, and point out that the quantization algorithms perform superiorly in terms of search accuracy, search time cost, and space cost. Finally, we introduce a few emerging topics.},
archivePrefix = {arXiv},
arxivId = {1606.00185},
author = {Wang, Jingdong and Zhang, Ting and Song, Jingkuan and Sebe, Nicu and Shen, Heng Tao},
doi = {10.1109/TPAMI.2017.2699960},
eprint = {1606.00185},
file = {:home/nathan/Documents/Mendeley Desktop/Wang et al/IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)/Wang et al.{\_}2017{\_}A Survey on Learning to Hash.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
number = {9},
title = {{A Survey on Learning to Hash}},
url = {http://arxiv.org/abs/1606.00185},
volume = {13},
year = {2017}
}
@article{Shrivastava2011,
abstract = {The flow around an arrangement of two cylinders in tandem exhibits a remarkably complex behaviour that is of interest for many engineering problems, such as environmental flows or structural design. In the present paper, a Large Eddy Simulation using a staggered Cartesian grid has been performed for the flow around two cylinders in tandem of diameter D = 20 mm and height H = 50 mm submerged in an open channel with height h = 60 mm . The two axes have a streamwise spacing of 2D. The Reynolds number is 1500, based on the cylinder diameter and the free-stream velocity u . The results obtained show that no vortex shedding occurs in the gap between the two cylinders where the separated shear layers produced by the upstream cylinder reattach on the surface of the downstream one. The flow separates on the top of the first cylinder with the presence of two spiral nodes known as owl-face configuration. On top of the downstream cylinder, the flow is attached. A complex mean flow develops in the gap and also behind the second cylinder. Comparisons with PIV measurements reveal good general agreement, but there are differences concerning some details of the flow in the gap between the cylinders.},
annote = {From Duplicate 1 (Data-driven visual similarity for cross-domain image matching - Shrivastava, Abhinav; Malisiewicz, Tomasz; Gupta, Abhinav; Efros, Alexei A.)

R{\'{e}}sum{\'{e}}

Matching d'images similaire avec repr{\'{e}}sentation h{\'{e}}t{\'{e}}rog{\'{e}}ne (esquisse, peinture, photo). M{\'{e}}thode bas{\'{e}}e sur l'entrainement de SVM (hard-negative mining).

Ce que je n'ai pas compris

- La "hard-negative mining approch"
- Le proc{\'{e}}d{\'{e}} d{\'{e}}taill{\'{e}} de matching (on compare toutes l'image sous HoG ou juste des vignettes ? Et comment on choisit le meilleur match apres ?)

Ce qui est int{\'{e}}ressant

- L'aspect "data driven uniqunesse" pour renforcer les features qui sont vraiment disciminantes
- La robustesse au changement d'illumination et au type de donn{\'{e}}es en entr{\'{e}}es (donc diff{\'{e}}rents types de cam{\'{e}}ras)

Critiques

- Temps de calcul trop long
- Match seulement les images avec le m{\^{e}}me point de vue 
- Extrait des "notions" g{\'{e}}n{\'{e}}rales des images (maisnon, pont, etc.) donc pas forc{\'{e}}ment bien pour la locatlisation pr{\'{e}}cise
- Les r{\'{e}}sultats sont moins bien que la plus part des autres m{\'{e}}thodes ({\c{c}}a fait tout mais en moins bien). La pertience des choix des autres m{\'{e}}thodes pour la comparaison reste {\`{a}} prouver.

A approfondir

- Content based image retrieval
- Hard negative mining approach
- Faire en sorte de rendre robuste l'approche m{\^{e}}me avec des points de vue diff{\'{e}}rente (changer l'approche de pr{\'{e}}-descripition des images)
- Descripteur D-SIFT, GIST

Computational load
Offline: -
Online: 3 min 200 node cluster

Scalability
Scale: -
Scalability potential: -

From Duplicate 2 (Data-driven visual similarity for cross-domain image matching - Shrivastava, Abhinav; Malisiewicz, Tomasz; Gupta, Abhinav; Efros, Alexei A.)

R{\{}{\'{e}}{\}}sum{\{}{\'{e}}{\}}

Matching d'images similaire avec repr{\{}{\'{e}}{\}}sentation h{\{}{\'{e}}{\}}t{\{}{\'{e}}{\}}rog{\{}{\'{e}}{\}}ne (esquisse, peinture, photo). M{\{}{\'{e}}{\}}thode bas{\{}{\'{e}}{\}}e sur l'entrainement de SVM (hard-negative mining).

Ce que je n'ai pas compris

- La "hard-negative mining approch"
- Le proc{\{}{\'{e}}{\}}d{\{}{\'{e}}{\}} d{\{}{\'{e}}{\}}taill{\{}{\'{e}}{\}} de matching (on compare toutes l'image sous HoG ou juste des vignettes ? Et comment on choisit le meilleur match apres ?)

Ce qui est int{\{}{\'{e}}{\}}ressant

- L'aspect "data driven uniqunesse" pour renforcer les features qui sont vraiment disciminantes
- La robustesse au changement d'illumination et au type de donn{\{}{\'{e}}{\}}es en entr{\{}{\'{e}}{\}}es (donc diff{\{}{\'{e}}{\}}rents types de cam{\{}{\'{e}}{\}}ras)

Critiques

- Temps de calcul trop long
- Match seulement les images avec le m{\{}{\^{e}}{\}}me point de vue 
- Extrait des "notions" g{\{}{\'{e}}{\}}n{\{}{\'{e}}{\}}rales des images (maisnon, pont, etc.) donc pas forc{\{}{\'{e}}{\}}ment bien pour la locatlisation pr{\{}{\'{e}}{\}}cise
- Les r{\{}{\'{e}}{\}}sultats sont moins bien que la plus part des autres m{\{}{\'{e}}{\}}thodes ({\{}{\c{c}}{\}}a fait tout mais en moins bien). La pertience des choix des autres m{\{}{\'{e}}{\}}thodes pour la comparaison reste {\{}{\`{a}}{\}} prouver.

A approfondir

- Content based image retrieval
- Hard negative mining approach
- Faire en sorte de rendre robuste l'approche m{\{}{\^{e}}{\}}me avec des points de vue diff{\{}{\'{e}}{\}}rente (changer l'approche de pr{\{}{\'{e}}{\}}-descripition des images)
- Descripteur D-SIFT, GIST},
author = {Shrivastava, Abhinav and Malisiewicz, Tomasz and Gupta, Abhinav and Efros, Alexei A.},
doi = {10.1145/2070781.2024188},
file = {:home/nathan/Documents/Mendeley Desktop/Shrivastava et al/ACM Transactions on Graphics (ToG)/Shrivastava et al.{\_}2011{\_}Data-driven visual similarity for cross-domain image matching.pdf:pdf},
isbn = {9781450308076},
issn = {07300301},
journal = {ACM Transactions on Graphics (ToG)},
keywords = {Challenging matching,Data-driven,Global features,HoG,Linear classifier,Long terme place recognition,Machine learning,Multiple visual domaines,RGB request,SVM,Visual place recognition,image matching,image re-,paintings,re-photography,saliency,sketches,trieval,visual memex,visual similarity},
mendeley-tags = {Challenging matching,Data-driven,Global features,HoG,Linear classifier,Long terme place recognition,Machine learning,Multiple visual domaines,RGB request,SVM,Visual place recognition},
number = {6},
pages = {1},
title = {{Data-driven visual similarity for cross-domain image matching}},
volume = {30},
year = {2011}
}
@inproceedings{Costante2013,
abstract = {As researchers are striving for developing robotic systems able to move into the `the wild', the interest towards novel learning paradigms for domain adaptation has increased. In the specific application of semantic place recognition from cameras, supervised learning algorithms are typically adopted. However, once learning has been performed, if the robot is moved to another location, the acquired knowledge may be not useful, as the novel scenario can be very different from the old one. The obvious solution would be to retrain the model updating the robot internal representation of the environment. Unfortunately this procedure involves a very time consuming data-labeling effort at the human side. To avoid these issues, in this paper we propose a novel transfer learning approach for place categorization from visual cues. With our method the robot is able to decide automatically if and how much its internal knowledge is useful in the novel scenario. Differently from previous approaches, we consider the situation where the old and the novel scenario may differ significantly (not only the visual room appearance changes but also different room categories are present). Importantly, our approach does not require labeling from a human operator. We also propose a strategy for improving the performance of the proposed method by fusing two complementary visual cues. Our extensive experimental evaluation demonstrates the advantages of our approach on several sequences from publicly available datasets.},
author = {Costante, Gabriele and Ciarfuglia, Thomas A. and Valigi, Paolo and Ricci, Elisa},
booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2013.6696653},
file = {:home/nathan/Documents/Mendeley Desktop/Costante et al/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Costante et al.{\_}2013{\_}A transfer learning approach for multi-cue semantic place recognition.pdf:pdf},
isbn = {9781467363587},
title = {{A transfer learning approach for multi-cue semantic place recognition}},
year = {2013}
}
@inproceedings{Zhang2011,
abstract = {Image based localization is an important problem with many applications. The basic idea is to match a user generated query image against a database of geo-tagged images with known 6 degrees of freedom poses. Once this retrieval problem is solved, it is possible to recover the pose of the query image. A challenging problem in image retrieval is performance degradation as the size of the image database grows. In this paper we describe an approach to large scale image retrieval for user localization in urban environment by taking advantage of coarse position estimates available, e.g. via cell tower triangulation, on many mobile devices today. The basic idea is to partition the large image database for a large region into a number of overlapping cells each with its own prebuilt search and retrieval structure. We demonstrate retrieval results over a {\~{}}12,000 image database covering a 1 km2 area of downtown Berkeley.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode permettant de retrouver la position d'une image dans une base de donn{\'{e}}e g{\'{e}}olocalis{\'{e}} en utilisant une information de position grossi{\`{e}}re. Les auteurs ont divis{\'{e}} une zone urbaine en plusieurs sous location qui se superpose partiellement, et on entrain{\'{e}} un vacubulary tree sur chaque sous domaine. Lorsqu'ils recoivent une query, ils la passent {\`{a}} tous les sous domaines adjacents {\`{a}} l'origine de la query et choississent les matching les plus pertiant. Ils ont {\'{e}}galement entrain{\'{e}} un classifieur bayesien tr{\`{e}}s simple pour le donner un indice de confiance de leur matching.

Ce qui est int{\'{e}}ressant
- L'approche divis{\'{e}} pour r{\'{e}}gner

Critiques
- On dispose d'une position a priori

Computational load
Offline: -
Online: -

Scalability
Scale: City
Scalability potential: -},
author = {Zhang, Jerry and Hallquist, Aaron and Liang, Eric and Zakhor, Avideh},
booktitle = {Proceedings of the IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2011.6116517},
file = {:home/nathan/Documents/Mendeley Desktop/Zhang et al/Proceedings of the IEEE International Conference on Image Processing (ICIP)/Zhang et al.{\_}2011{\_}Location-based image retrieval for urban environments.pdf:pdf},
isbn = {9781457713033},
issn = {15224880},
keywords = {augmented reality,image matching,image retrieval,tagged images,visual landmark recognition},
number = {150},
pages = {3677--3680},
title = {{Location-based image retrieval for urban environments}},
volume = {50},
year = {2011}
}
@inproceedings{Sizikova2016,
annote = {R{\'{e}}sum{\'{e}}
Pr{\'{e}}sentation d'un syst{\`{e}}me de place recognition {\`{a}} l'aide de deux CNN (un founissant un descripteur des cannaux RGB et l'autre du canal de profondeur). La particularit{\'{e}} du syst{\`{e}}me est que le CNN descripteur des images de profondeur est entrain{\'{e}} sur des donn{\'{e}}es sythetiques. Les auteurs utilisent un joint descriptor combinant les informations couleurs et profondeur dont les param{\`{e}}tres sont optimis{\'{e}}s par un MM-Type estimator. Application aux loopclosures.

Ce que je n'ai pas compris
- MM-type estimator
- Leur m{\'{e}}thode d'{\'{e}}valuation

Ce qui est int{\'{e}}ressant
- Utilisation de donn{\'{e}}es de profondeur
- Entrainement sur donn{\'{e}}es synthetiques

Critiques
- Pas de donn{\'{e}}es de vitesse de calcul
- Que du indoor

Computational load
Offline: -
Online: fast ?

Scalability
Scale: Room
Scalability potential: Maybe},
author = {Sizikova, Elena and Singh, Vivek K. and Georgescu, Bogdan and Halber, Maciej and Ma, Kai and Chen, Terrence},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision Workshop (ECCVW)},
file = {:home/nathan/Documents/Mendeley Desktop/Sizikova et al/Proceedings of the IEEE European Conference on Computer Vision Workshop (ECCVW)/Sizikova et al.{\_}2016{\_}Enhancing Place Recognition using Joint Intensity - Depth Analysis and Synthetic Data.pdf:pdf},
isbn = {978-3-319-49408-1},
pages = {1--8},
title = {{Enhancing Place Recognition using Joint Intensity - Depth Analysis and Synthetic Data}},
year = {2016}
}
@inproceedings{Liu2016,
author = {Liu, Zhaoliang and Duan, Ling-Yu and Chen, Jie and Huang, Tiejun},
booktitle = {Proceedings of the IEEE International Conference on Image Processing (ICIP)},
file = {:home/nathan/Documents/Mendeley Desktop/Liu et al/Proceedings of the IEEE International Conference on Image Processing (ICIP)/Liu et al.{\_}2016{\_}Depth-Based Local Feature Selection for Mobile Visual Search.pdf:pdf},
title = {{Depth-Based Local Feature Selection for Mobile Visual Search}},
year = {2016}
}
@inproceedings{Li2016,
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode d'estimation de pose (6DOF) {\`{a}} partir de correspondances entre un mod{\`{e}}le 3D et une image de RGBD (3D-3D). Utilisation de trois types de correspondance entre les images pour renforcer le rejet d'outliers : correspondance 2D-3D (BRISK), 3D-3D et N-N (normal au point). Egalement une introduction d'une m{\'{e}}thode de calcul de la pose {\`{a}} proprement dit en utilisant une m{\'{e}}thode de r{\'{e}}solution par moindre carr{\'{e}}. Am{\'{e}}lioration des choix des inliers et rapidit{\'{e}} de la m{\'{e}}thode de calcul de pose. 

Ce que je n'ai pas compris
- Le choix des paires/ la prioretisation de certain appariement par rapport {\`{a}} d'autres
- D{\'{e}}taille de la m{\'{e}}thode de regression de pose

Ce qui est int{\'{e}}ressant
- Utilise des relation heterogene -{\textgreater} bien adapt{\'{e}} a notre probl{\'{e}}matique vu qu'on ne connait pas la nature de la requete

Critiques
- Pas de vitesse de calcul explicitement d{\'{e}}crites

A approfondir
- Les diff{\'{e}}rentes m{\'{e}}thode de reconstruction de pose {\`{a}} partir de diff{\'{e}}rents type d'appariements

Computational load
Offline: -
Online: not given

Scalability
Scale: -
Scalability potential: -},
author = {Li, Shuda and Calway, Andrew},
booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2016.7487678},
file = {:home/nathan/Documents/Mendeley Desktop/Li, Calway/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Li, Calway{\_}2016{\_}Absolute pose estimation using multiple forms of correspondences from RGB-D frames.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
pages = {4756--4761},
title = {{Absolute pose estimation using multiple forms of correspondences from RGB-D frames}},
volume = {2016-June},
year = {2016}
}
@article{Piasco2017,
author = {Piasco, Nathan and Sidib{\'{e}}, D{\'{e}}sir{\'{e}} and Demonceaux, C{\'{e}}dric and Gouet-Brunet, Val{\'{e}}rie},
doi = {10.1016/j.patcog.2017.09.013},
file = {:home/nathan/Documents/Mendeley Desktop/Piasco et al/Pattern Recognition/Piasco et al.{\_}2018{\_}A survey on Visual-Based Localization On the benefit of heterogeneous data.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
month = {feb},
pages = {90--109},
title = {{A survey on Visual-Based Localization: On the benefit of heterogeneous data}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320317303448},
volume = {74},
year = {2018}
}
@inproceedings{Brachmann2017,
abstract = {RANSAC is an important algorithm in robust optimization and a central building block for many computer vision applications. In recent years, traditionally hand-crafted pipelines have been replaced by deep learning pipelines, which can be trained in an end-to-end fashion. However, RANSAC has so far not been used as part of such deep learning pipelines, because its hypothesis selection procedure is non-differentiable. In this work, we present two different ways to overcome this limitation. The most promising approach is inspired by reinforcement learning, namely to replace the deterministic hypothesis selection by a probabilistic selection for which we can derive the expected loss w.r.t. to all learnable parameters. We call this approach DSAC, the differentiable counterpart of RANSAC. We apply DSAC to the problem of camera localization, where deep learning has so far failed to improve on traditional approaches. We demonstrate that by directly minimizing the expected loss of the output camera poses, robustly estimated by RANSAC, we achieve an increase in accuracy. In the future, any deep learning pipeline can use DSAC as a robust optimization component.},
archivePrefix = {arXiv},
arxivId = {1611.05705},
author = {Brachmann, Eric and Krull, Alexander and Nowozin, Sebastian and Shotton, Jamie and Michel, Frank and Gumhold, Stefan and Rother, Carsten},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.267},
eprint = {1611.05705},
file = {:home/nathan/Documents/Mendeley Desktop/Brachmann et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Brachmann et al.{\_}2017{\_}DSAC - Differentiable RANSAC for Camera Localization.pdf:pdf},
title = {{DSAC - Differentiable RANSAC for Camera Localization}},
year = {2017}
}
@article{Oliveira2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.08775v1},
author = {Oliveira, Gabriel L. and Radwan, Noha and Burgard, Wolfram and Brox, Thomas},
eprint = {arXiv:1706.08775v1},
file = {:home/nathan/Documents/Mendeley Desktop/Oliveira et al/arXiv preprint/Oliveira et al.{\_}2017{\_}Topometric Localization with Deep Learning.pdf:pdf},
journal = {arXiv preprint},
title = {{Topometric Localization with Deep Learning}},
year = {2017}
}
@inproceedings{Zhou2014,
abstract = {Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.},
author = {Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
booktitle = {Annual Conference on Neural Information Processing Systems (NIPS)},
file = {:home/nathan/Documents/Mendeley Desktop/Zhou et al/Annual Conference on Neural Information Processing Systems (NIPS)/Zhou et al.{\_}2014{\_}Learning Deep Features for Scene Recognition using Places Database.pdf:pdf},
issn = {10495258},
pages = {487--495},
title = {{Learning Deep Features for Scene Recognition using Places Database}},
url = {http://papers.nips.cc/paper/5349-learning-deep-features-for-scene-recognition-using-places-database.pdf},
year = {2014}
}
@inproceedings{Bui2017,
author = {Bui, Mai and Albarqouni, Shadi and Schrapp, Michael and Navab, Nassir and Ilic, Slobodan},
booktitle = {Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)},
doi = {10.1109/WACV.2017.120},
file = {:home/nathan/Documents/Mendeley Desktop/Bui et al/Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)/Bui et al.{\_}2017{\_}X-ray PoseNet 6 DoF Pose Estimation for Mobile X-ray Devices.pdf:pdf},
isbn = {9781509048229},
title = {{X-ray PoseNet: 6 DoF Pose Estimation for Mobile X-ray Devices}},
year = {2017}
}
@article{Cieslewski2017a,
abstract = {—State-of-the-art systems that do place recognition in a group of n robots either rely on a centralized solution, where each robot's map is sent to a central server, or a decentralized solution, where the map is either sent to all other robots, or robots within a communication range. Both approaches have their drawbacks: centralized systems rely on a central entity which handles all the computational load and cannot be deployed in large, remote areas; decentralized systems either exchange n times more data or preclude matches between robots that visit the same place at different times while never being close enough to communicate directly. We propose a novel decentralized approach which requires a similar amount of data exchange as a centralized system, without precluding any matches. The core idea is that the candidate selection in visual bag-of-words can be distributed by pre-assigning words of the vocabulary to different robots. The result of this candidate selection is then used to choose a single robot to which the full query is sent. We validate our approach on real data and discuss its merit in different network models. To the best of our knowledge, this is the first work to use a distributed inverted index in multi-robot place recognition.},
author = {Cieslewski, Titus and Scaramuzza, Davide},
doi = {10.1109/LRA.2017.2650153},
file = {:home/nathan/Documents/Mendeley Desktop/Cieslewski, Scaramuzza/IEEE Robotics and Automation Letters (RAL)/Cieslewski, Scaramuzza{\_}2017{\_}Efficient Decentralized Visual Place Recognition Using a Distributed Inverted Index.pdf:pdf},
issn = {2377-3766},
journal = {IEEE Robotics and Automation Letters (RAL)},
number = {2},
pages = {1--8},
title = {{Efficient Decentralized Visual Place Recognition Using a Distributed Inverted Index}},
volume = {2},
year = {2017}
}
@inproceedings{Garg2017,
annote = {R{\'{e}}sum{\'{e}}
Aml{\'{e}}lioration de SeqSLAM en ajoutant des infos s{\'{e}}mantiques pour cr{\'{e}}er des segments coh{\'{e}}rant (ou l'on compare ensuite l'image source par SAD). La s{\'{e}}mantisation se fait avec un CNN VGG16 entrain{\'{e}} sur le dataset Places365 (365 mots clefs s{\'{e}}mantiques). Le d{\'{e}}coupage en s{\'{e}}gment se fait grace {\`{a}} un HMM qui estime les proba de transition entre les diff{\'{e}}rents {\'{e}}tats. Am{\'{e}}lioration des r{\'{e}}sultats, {\c{c}}a aide pas mal pour d{\'{e}}terminer les sc{\`{e}}nes indoor/outdoor de nuit.

Lu en diagonal de la mort},
archivePrefix = {arXiv},
arxivId = {1706.07144},
author = {Garg, Sourav and Jacobson, Adam and Kumar, Swagat and Milford, Michael J.},
booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
eprint = {1706.07144},
file = {:home/nathan/Documents/Mendeley Desktop/Garg et al/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Garg et al.{\_}2017{\_}Improving Condition and Environment-Invariant Place Recognition with Semantic Place Categorization.pdf:pdf},
title = {{Improving Condition and Environment-Invariant Place Recognition with Semantic Place Categorization}},
year = {2017}
}
@inproceedings{Tolias2011,
address = {Barcelona, Spain},
author = {Tolias, Giorgos and Avrithis, Yannis},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
title = {{Speeded-up, Relaxed Spatial Matching}},
year = {2011}
}
@article{Shaham2018,
author = {Shaham, Uri and Lederman, Roy R.},
doi = {10.1016/j.patcog.2017.09.015},
file = {:home/nathan/Documents/Mendeley Desktop/Shaham, Lederman/Pattern Recognition/Shaham, Lederman{\_}2018{\_}Learning by coincidence Siamese networks and common variable learning.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Similarity learning,Representation learning,Unsupe},
pages = {52--63},
publisher = {Elsevier Ltd},
title = {{Learning by coincidence: Siamese networks and common variable learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320317303588},
volume = {74},
year = {2018}
}
@inproceedings{Campbell2017,
author = {Campbell, Dylan and Petersson, Lars and Kneip, Laurent and Li, Hongdong},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
file = {:home/nathan/Documents/Mendeley Desktop/Campbell et al/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Campbell et al.{\_}2017{\_}Globally-Optimal Inlier Set Maximisation for Simultaneous Camera Pose and Feature Correspondence.pdf:pdf},
title = {{Globally-Optimal Inlier Set Maximisation for Simultaneous Camera Pose and Feature Correspondence}},
year = {2017}
}
@article{Williams2007,
abstract = {Monocular SLAM has the potential to turn inexpensive cameras into powerful pose sensors for applications such as robotics and augmented reality. However, current implementations lack the robustness required to be useful outside laboratory conditions: blur, sudden motion and occlusion all cause tracking to fail and corrupt the map. Here we present a system which automatically detects and recovers from tracking failure while preserving map integrity. By extending recent advances in keypoint recognition the system can quickly resume tracking - i.e. within a single frame time of 33 ms - using any of the features previously stored in the map. Extensive tests show that the system can reliably generate maps for long sequences even in the presence of frequent tracking failure.},
author = {Williams, Brian and Klein, Georg and Reid, Ian},
doi = {10.1109/ICCV.2007.4409115},
file = {:home/nathan/Documents/Mendeley Desktop/Williams, Klein, Reid/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Williams, Klein, Reid{\_}2007{\_}Real-time SLAM relocalisation.pdf:pdf},
isbn = {978-1-4244-1631-8},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
title = {{Real-time SLAM relocalisation}},
year = {2007}
}
@inproceedings{Perronnin2006,
author = {Perronnin, Florent and Dance, Christopher},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Perronnin, Dance/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Perronnin, Dance{\_}2006{\_}Fisher Kernels on Visual Vocabularies for Image Categorizaton.pdf:pdf},
isbn = {1424411807},
title = {{Fisher Kernels on Visual Vocabularies for Image Categorizaton}},
year = {2006}
}
@inproceedings{Hou2015,
abstract = {Deep convolutional neural networks (CNN) have recently been shown in many computer vision and pattern recog- nition applications to outperform by a significant margin state- of-the-art solutions that use traditional hand-crafted features. However, this impressive performance is yet to be fully exploited in robotics. In this paper, we focus one specific problem that can benefit from the recent development of the CNN technology, i.e., we focus on using a pre-trained CNN model as a method of generating an image representation appropriate for visual loop closure detection in SLAM (simultaneous localization and mapping). We perform a comprehensive evaluation of the outputs at the intermediate layers of a CNN as image descriptors, in comparison with state-of-the-art image descriptors, in terms of their ability to match images for detecting loop closures. The main conclusions of our study include: (a) CNN-based image representations perform comparably to state-of-the-art hand- crafted competitors in environments without significant lighting change, (b) they outperform state-of-the-art competitors when lighting changes significantly, and (c) they are also significantly faster to extract than the state-of-the-art hand-crafted features even on a conventional CPU and are two orders of magnitude faster on an entry-level GPU.},
archivePrefix = {arXiv},
arxivId = {1504.05241},
author = {Hou, Yi and Zhang, Hong and Zhou, Shilin},
booktitle = {Proceedings of the IEEE International Conference on Information and Automation},
doi = {10.1109/icinfa.2015.7279659},
eprint = {1504.05241},
file = {:home/nathan/Documents/Mendeley Desktop/Hou, Zhang, Zhou/Proceedings of the IEEE International Conference on Information and Automation/Hou, Zhang, Zhou{\_}2015{\_}Convolutional Neural Network-Based Image Representation for Visual Loop Closure Detection.pdf:pdf},
isbn = {9781467391047},
number = {August},
pages = {2238--2245},
title = {{Convolutional Neural Network-Based Image Representation for Visual Loop Closure Detection}},
url = {http://arxiv.org/abs/1504.05241},
year = {2015}
}
@inproceedings{Jimenez2017,
abstract = {Image retrieval in realistic scenarios targets large dynamic datasets of unlabeled images. In these cases, training or fine-tuning a model every time new images are added to the database is neither efficient nor scalable. Convolutional neural networks trained for image classification over large datasets have been proven effective feature extractors for image retrieval. The most successful approaches are based on encoding the activations of convolutional layers, as they convey the image spatial information. In this paper, we go beyond this spatial information and propose a local-aware encoding of convolutional features based on semantic information predicted in the target image. To this end, we obtain the most discriminative regions of an image using Class Activation Maps (CAMs). CAMs are based on the knowledge contained in the network and therefore, our approach, has the additional advantage of not requiring external information. In addition, we use CAMs to generate object proposals during an unsupervised re-ranking stage after a first fast search. Our experiments on two public available datasets for instance retrieval, Oxford5k and Paris6k, demonstrate the competitiveness of our approach outperforming the current state-of-the-art when using off-the-shelf models trained on ImageNet. The source code and model used in this paper are publicly available at http://imatge-upc.github.io/retrieval-2017-cam/.},
archivePrefix = {arXiv},
arxivId = {1707.02581},
author = {Jimenez, Albert and Alvarez, Jose M. and Giro-i-Nieto, Xavier},
booktitle = {British Machine Vision Conference (BMVC)},
eprint = {1707.02581},
file = {:home/nathan/Documents/Mendeley Desktop/Jimenez, Alvarez, Giro-i-Nieto/British Machine Vision Conference (BMVC)/Jimenez, Alvarez, Giro-i-Nieto{\_}2017{\_}Class-Weighted Convolutional Features for Visual Instance Search.pdf:pdf},
title = {{Class-Weighted Convolutional Features for Visual Instance Search}},
url = {http://arxiv.org/abs/1707.02581},
year = {2017}
}
@inproceedings{Philbin2007,
abstract = {In this paper, we present a large-scale object retrieval system. The user supplies a query object by selecting a region of a query image, and the system returns a ranked list of images that contain the same object, retrieved from a large corpus. We demonstrate the scalability and performance of our system on a dataset of over 1 million images crawled from the photo-sharing site, Flickr [3], using Oxford landmarks as queries. Building an image-feature vocabulary is a major time and performance bottleneck, due to the size of our dataset. To address this problem we compare different scalable methods for building a vocabulary and introduce a novel quantization method based on randomized trees which we show outperforms the current state-of-the-art on an extensive ground-truth. Our experiments show that the quantization has a major effect on retrieval quality. To further improve query performance, we add an efficient spatial verification stage to re-rank the results returned from our bag-of-words model and show that this consistently improves search quality, though by less of a margin when the visual vocabulary is large. We view this work as a promising step towards much larger, "web-scale " image corpora.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode d'object retrieval test{\'{e}} sur un dataset de type urbain. Introduction d'un nouveau dataset (Oxford 5k), d'une nouvelle m{\'{e}}thode de clustering du dictionnaire de visual word, et d'une m{\'{e}}thode de spatial verification ajout{\'{e}}e pour un reranking des candidates. Utilisation de hessian detector avec SIFT + viusal vocabulary weighted by tf-idf + spatial reranking inside LO-RANSAC. Leur syst{\`{e}}me de clustering s'approche de celui pr{\'{e}}sent{\'{e}} par Nister2006 (vocabulary tree) mais en utilisant des random tree qui limite les effets de quantization tout en acc{\'{e}}lerant le process de clusterisation. Cela permet de cr{\'{e}}{\'{e}}er des dico plus gros (1M visual word) qui marchent mieux. Le reranking s'appuie sur la verification d'une transformation possible entre la query et l'image retrouv{\'{e}} avec l'hypothese que les images sont orient{\'{e}} vers le haut (hypoth{\`{e}}se probable dans le cas d'image prises par des touristes). 

Ce que je n'ai pas compris
- Comment ils cr{\'{e}}ent leur random tree pour le clustering

Ce qui est int{\'{e}}ressant
- Permet de cr{\'{e}}er des dicos plus gros qui fonctionne mieux
- La verification spatial de l'arrangement des features

Critiques
- Mehtod pour de la recherche d'objet, c-a-d que les images sont cropp{\'{e}} sur le point d'interet, ce qui rend la verification saptiale plus facile

A approfondir
- LO-RANSAC

Computational load
Offline: Info dans le papier
Online: idem

Scalability
Scale: City
Scalability potential: probable vu la m{\'{e}}thode de clustering},
author = {Philbin, James and Chum, Ondřej and Isard, Michael and Sivic, Josef and Zisserman, Andrew},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2007.383172},
file = {:home/nathan/Documents/Mendeley Desktop/Philbin et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Philbin et al.{\_}2007{\_}Object retrieval with large vocabularies and fast spatial matching.pdf:pdf},
isbn = {1424411807},
issn = {10636919},
title = {{Object retrieval with large vocabularies and fast spatial matching}},
year = {2007}
}
@article{Fischler1981,
author = {Fischler, Martin A. and Bolles, Robert C.},
journal = {Communications of the ACM},
number = {6},
pages = {381--395},
publisher = {ACM},
title = {{Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography}},
volume = {24},
year = {1981}
}
@inproceedings{Kendall2017,
abstract = {Deep learning has shown to be effective for robust and real-time monocular image relocalisation. In particular, PoseNet is a deep convolutional neural network which learns to regress the 6-DOF camera pose from a single image. It learns to localize using high level features and is robust to difficult lighting, motion blur and unknown camera intrinsics, where point based SIFT registration fails. However, it was trained using a naive loss function, with hyper-parameters which require expensive tuning. In this paper, we give the problem a more fundamental theoretical treatment. We explore a number of novel loss functions for learning camera pose which are based on geometry and scene reprojection error. Additionally we show how to automatically learn an optimal weighting to simultaneously regress position and orientation. By leveraging geometry, we demonstrate that our technique significantly improves PoseNet's performance across datasets ranging from indoor rooms to a small city.},
annote = {R{\'{e}}sum{\'{e}}
Am{\'{e}}lioration de PoseNet pour gagner en pr{\'{e}}cision. Les auteurs pensent que le probl{\`{e}}me de pr{\'{e}}cision est du {\`{a}} la nature de la loss function, trop simple, et comportant un hyperparam{\`{e}}tre compliqu{\'{e}} {\`{a}} r{\'{e}}gler (et qui d{\'{e}}pend des datasets). Ils introduisent donc une loss fonction sans hyperparametre (la balance entre le terme de position et d'orientation se fait automatiquement pendant l'apprentissage). Ils essayent {\'{e}}galement un autre type de loss fonction en imitant la fonction de cout originellement employ{\'{e}} en multiview computer vision, {\`{a}} savoir la reprojection error. Pour {\c{c}}a ils prennent des pixels dans l'image et minimise cette grandeur dans l'apprentissage du r{\'{e}}seau.

Ce que je n'ai pas compris
- Ils parlent de l'importance de la repr{\'{e}}sentation de la rotation (d'o{\`{u}} le choix du quaternion) mais ils font finalement qu'une diff{\'{e}}rence euclidienne pour minimise l'erreur ?
- J'ai pas bien compris comment marche la loss function avec la reporjection dans le plan image (ils prennent plus l'image en entier ? Ou c'est fait en parall{\`{e}}le ?

Ce qui est int{\'{e}}ressant
- Am{\'{e}}lioration des r{\'{e}}sultats


Critiques
- Temps d'apprentissage (avec Titan Black)
- Je sais pas je le sens pas trop ce papier (d{\'{e}}j{\`{a}} ils mettent la m{\'{e}}diane pour comparer)


Computational load
Offline: 4h - 1j / scene
Online: 5ms

Scalability

Scale: small city (test sur dubrovnik)
Scalability potential: ?},
archivePrefix = {arXiv},
arxivId = {1704.00390},
author = {Kendall, Alex and Cipolla, Roberto},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {1704.00390},
file = {:home/nathan/Documents/Mendeley Desktop/Kendall, Cipolla/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Kendall, Cipolla{\_}2017{\_}Geometric loss functions for camera pose regression with deep learning.pdf:pdf},
title = {{Geometric loss functions for camera pose regression with deep learning}},
url = {http://arxiv.org/abs/1704.00390},
year = {2017}
}
@article{Celik2017,
abstract = {Content Based Image Retrieval (CBIR) has been widely studied in the last two decades. Unlike text based image retrieval techniques, visual properties of images are used to obtain high level semantic information in CBIR. There is a gap between low level features and high level semantic information. This is called semantic gap and it is the most important problem in CBIR. The visual properties were extracted from low level features such as color, shape, texture and spatial information in early days. Local Feature Descriptors (LFDs) are more successful to increase performance of CBIR system. Then, a semantic bridge is built with high level semantic information. Sparse Representations (SRs) have become popular to achieve this aim in the last years. In this study, CBIR models that use LFDs and SRs in literature are investigated in detail. The SRs and LFD extraction algorithms are tested and compared within a CBIR framework for different scenarios. Scale Invariant Feature Transform (SIFT), Speeded-Up Robust Features (SURF), Histograms of Oriented Gradients (HoG), Local Binary Pattern (LBP) and Local Ternary Pattern (LTP) are used to extract LFDs from images. Random Features, K-Means and K-Singular Value Decomposition (K-SVD) algorithms are used for dictionary learning and Orthogonal Matching Pursuit (OMP), Homotopy, Lasso, Elastic Net, Parallel Coordinate Descent (PCD) and Separable Surrogate Function (SSF) are used for coefficient learning. Finally, three methods recently proposed in literature (Online Dictionary Learning (ODL), Locality-constrained Linear Coding (LLC) and Feature-based Sparse Representation (FBSR)) are also tested and compared with our framework results. All test results are presented and discussed. As a conclusion, the most successful approach in our framework is to use LLC for Coil20 data set and FBSR for Corel1000 data set. We obtain 89{\%} and 58{\%} Mean Average Precision (MAP) for Coil20 and Corel1000, respectively.},
author = {Celik, Ceyhun and Bilge, Hasan Sakir},
doi = {10.1016/j.patcog.2017.03.006},
file = {:home/nathan/Documents/Mendeley Desktop/Celik, Bilge/Pattern Recognition/Celik, Bilge{\_}2017{\_}Content based image retrieval with sparse representations and local feature descriptors A comparative study.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Coefficient learning,Content based image retrieval,Dictionary learning,Local feature descriptor,Sparse representation},
pages = {1--13},
publisher = {Elsevier Ltd},
title = {{Content based image retrieval with sparse representations and local feature descriptors: A comparative study}},
url = {http://dx.doi.org/10.1016/j.patcog.2017.03.006},
volume = {68},
year = {2017}
}
@inproceedings{Babenko2015,
abstract = {encoding conv features},
annote = {Comparaison de la m{\'{e}}thode d'aggregation max et sum (SPoC). Cela consiste simplement {\`{a}} faire la somme de valeurs d'une deep map feature. Donc si on a M map de taille w*h, on se retrouve avec un descripteur de taille M (au lieu de M*w*h)},
archivePrefix = {arXiv},
arxivId = {arXiv:1510.07493v1},
author = {Babenko, Artem and Lempitsky, Victor},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.150},
eprint = {arXiv:1510.07493v1},
file = {:home/nathan/Documents/Mendeley Desktop/Babenko, Lempitsky/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Babenko, Lempitsky{\_}2015{\_}Aggregating local deep features for image retrieval.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
pages = {1269--1277},
pmid = {10548103},
title = {{Aggregating local deep features for image retrieval}},
volume = {11-18-Dece},
year = {2015}
}
@inproceedings{Shi2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1603.09436},
author = {Shi, Haoyue and Chen, Jia and Hauptmann, Alexander G.},
booktitle = {Proceedings of the ACM International Conference on Multimedia Retrieval (ICMR)},
doi = {10.1145/nnnnnnn.nnnnnnn},
eprint = {arXiv:1603.09436},
file = {:home/nathan/Documents/Mendeley Desktop/Shi, Chen, Hauptmann/Proceedings of the ACM International Conference on Multimedia Retrieval (ICMR)/Shi, Chen, Hauptmann{\_}2017{\_}Joint Saliency Estimation and Matching using Image Regions for Geo-Localization of Online Video.pdf:pdf},
isbn = {9781450335492},
issn = {15232867},
keywords = {-  Computer systems organization  -{\textgreater}  Neural netwo,-  Information systems  -{\textgreater}  Multimedia and multimo,Image search,Video search,region matching,region saliency,video geo-localization},
pages = {1--16},
title = {{Joint Saliency Estimation and Matching using Image Regions for Geo-Localization of Online Video}},
url = {http://www.di.ens.fr/{~}cezarad/popl16.pdf},
year = {2017}
}
@inproceedings{Sicre2017,
abstract = {Part-based image classification aims at representing categories by small sets of learned discriminative parts, upon which an image representation is built. Considered as a promising avenue a decade ago, this direction has been neglected since the advent of deep neural networks. In this context, this paper brings two contributions: first, it shows that despite the recent success of end-to-end holistic models, explicit part learning can boosts classification performance. Second, this work proceeds one step further than recent part-based models (PBM), focusing on how to learn parts without using any labeled data. Instead of learning a set of parts per class, as generally done in the PBM literature, the proposed approach both constructs a partition of a given set of images into visually similar groups, and subsequently learn a set of discriminative parts per group in a fully unsupervised fashion. This strategy opens the door to the use of PBM in new applications for which the notion of image categories is irrelevant, such as instance-based image retrieval, for example. We experimentally show that our learned parts can help building efficient image representations, for classification as well as for indexing tasks, resulting in performance superior to holistic state-of-the art Deep Convolutional Neural Networks (DCNN) encoding.},
archivePrefix = {arXiv},
arxivId = {1704.03755},
author = {Sicre, Ronan and Avrithis, Yannis and Kijak, Ewa and Jurie, Frederic},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {1704.03755},
file = {:home/nathan/Documents/Mendeley Desktop/Sicre et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Sicre et al.{\_}2017{\_}Unsupervised part learning for visual recognition.pdf:pdf},
title = {{Unsupervised part learning for visual recognition}},
url = {http://arxiv.org/abs/1704.03755},
year = {2017}
}
@article{Glocker2015,
abstract = {Recovery from tracking failure is essential in any simultaneous localization and tracking system. In this context, we explore an efficient keyframe-based relocalization method based on frame encoding using randomized ferns. The method enables automatic discovery of keyframes through online harvesting in tracking mode, and fast retrieval of pose candidates in the case when tracking is lost. Frame encoding is achieved by applying simple binary feature tests which are stored in the nodes of an ensemble of randomized ferns. The concatenation of small block codes generated by each fern yields a global compact representation of camera frames. Based on those representations we define the frame dissimilarity as the block-wise hamming distance (BlockHD). Dissimilarities between an incoming query frame and a large set of keyframes can be efficiently evaluated by simply traversing the nodes of the ferns and counting image co-occurrences in corresponding code tables. In tracking mode, those dissimilarities decide whether a frame/pose pair is considered as a novel keyframe. For tracking recovery, poses of the most similar keyframes are retrieved and used for reinitialization of the tracking algorithm. The integration of our relocalization method into a hand-held KinectFusion system allows seamless continuation of mapping even when tracking is frequently lost.},
author = {Glocker, Ben and Shotton, Jamie and Criminisi, Antonio and Izadi, Shahram},
doi = {10.1109/TVCG.2014.2360403},
file = {:home/nathan/Documents/Mendeley Desktop/Glocker et al/IEEE Transactions on Visualization and Computer Graphics (ToVCG)/Glocker et al.{\_}2015{\_}Real-time RGB-D camera relocalization via randomized ferns for keyframe encoding.pdf:pdf},
isbn = {1077-2626},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics (ToVCG)},
keywords = {camera relocalization,dense tracking and mapping,marker-free augmented reality,tracking recovery},
number = {5},
pages = {571--583},
title = {{Real-time RGB-D camera relocalization via randomized ferns for keyframe encoding}},
volume = {21},
year = {2015}
}
@article{Gordo2017,
abstract = {While deep learning has become a key ingredient in the top performing methods for many computer vision tasks, it has failed so far to bring similar improvements to instance-level image retrieval. In this article, we argue that reasons for the underwhelming results of deep methods on image retrieval are threefold: i) noisy training data, ii) inappropriate deep architecture, and iii) suboptimal training procedure. We address all three issues. First, we leverage a large-scale but noisy landmark dataset and develop an automatic cleaning method that produces a suitable training set for deep retrieval. Second, we build on the recent R-MAC descriptor, show that it can be interpreted as a deep and differentiable architecture, and present improvements to enhance it. Last, we train this network with a siamese architecture that combines three streams with a triplet loss. At the end of the training process, the proposed architecture produces a global image representation in a single forward pass that is well suited for image retrieval. Extensive experiments show that our approach significantly outperforms previous retrieval approaches, including state-of-the-art methods based on costly local descriptor indexing and spatial verification. On Oxford 5k, Paris 6k and Holidays, we respectively report 94.7, 96.6, and 94.8 mean average precision. Our representations can also be heavily compressed using product quantization with little loss in accuracy. For additional material, please see www.xrce.xerox.com/Deep-Image-Retrieval.},
archivePrefix = {arXiv},
arxivId = {1610.07940},
author = {Gordo, Albert and Almaz{\'{a}}n, Jon and Revaud, Jerome and Larlus, Diane},
doi = {10.1007/s11263-017-1016-8},
eprint = {1610.07940},
file = {:home/nathan/Documents/Mendeley Desktop/Gordo et al/International Journal of Computer Vision (IJCV)/Gordo et al.{\_}2017{\_}End-to-End Learning of Deep Visual Representations for Image Retrieval.pdf:pdf},
isbn = {9783319464664},
issn = {15731405},
journal = {International Journal of Computer Vision (IJCV)},
keywords = {Deep learning,Instance-level retrieval,Visual representation,Visual search},
number = {2},
pages = {237--254},
pmid = {4520227},
title = {{End-to-End Learning of Deep Visual Representations for Image Retrieval}},
volume = {124},
year = {2017}
}
@article{Song2016,
abstract = {The 6-degrees of freedom (DOF) image localization, which aims to calculate the spatial position and rotation of a camera, is a challenging problem formost location-based services. In existing approaches, this problem is often tackled by finding the matches between 2D image points and 3D structure points so as to derive the location information via direct linear transformation algorithm. However, as these 2D-to-3D-based approaches need to reconstruct the 3D structure points of the scene, they may not be flexible enough to employ massive and increasing geo-tagged data. To this end, this paper presents a novel approach for 6-DOF image localization by fusing candidate poses relative to reference images. In this approach, we propose to localize an input image according to the position and rotation information of multiple geo-tagged images retrieved from a reference dataset. From the reference images, an efficient relative pose estimation algorithm is proposed to derive a set of candidate poses for the input image. Each candidate pose encodes the relative rotation and direction of the input image with respect to a specific reference image. Finally, these candidate poses can be fused together by minimizing a well- defined geometry error so that the 6-DOF location of the input image is effectively derived. Experimental results show that our method can obtain satisfactory localization accuracy. In addition, the proposed relative pose estimation algorithm ismuchfaster than existing work.},
annote = {From Duplicate 1 (6-DOF Image Localization From Massive Geo-Tagged Reference Images - Song, Yafei; Chen, Xiaowu; Wang, Xiaogang; Zhang, Yu; Li, Jia)

R{\{}{\'{e}}{\}}sum{\{}{\'{e}}{\}}
Pipeline permettant de localiser (6 DOF) une image {\{}{\`{a}}{\}} partir d'une base de donn{\{}{\'{e}}{\}}e dense localis{\{}{\'{e}}{\}}e. On peut diviser la m{\{}{\'{e}}{\}}thode en 2 parties: premi{\{}{\`{e}}{\}}rement on retrouve les k plus proches voisins d'une image en requ{\{}{\^{e}}{\}}te par image retrival (SIFT + VBoW). On estime ensuite la pose relative de la query {\{}{\`{a}}{\}} chacun des voisins (estimation de la matrice OSR, qui encapsule la matrice fondamentale F et la d{\{}{\'{e}}{\}}formation de la lentille lambda, puis estimation de la matrice essentielle E et de la focal f et d{\{}{\'{e}}{\}}compositon de E pour r{\{}{\'{e}}{\}}cup{\{}{\'{e}}{\}}rer t et R) et on recoupe les poses relatives des deux plus robustes estimations pour retrouver la pose absolue.

Ce qui est int{\{}{\'{e}}{\}}ressant
- La m{\{}{\'{e}}{\}}thode est plus pr{\{}{\'{e}}{\}}cise que celle bas{\{}{\'{e}}{\}} corresspondance 2D/3D
- M{\{}{\'{e}}{\}}thode d'estimation de l'OSR plus robuste que au bruit l'{\{}{\'{e}}{\}}tat de l'art (mais necessite 3 correspondances de plus)

Critiques
- Necessite plusieurs vue (dans la database) pour une m{\{}{\^{e}}{\}}me query
- Pourquoi ne pas utiliser directement de la g{\{}{\'{e}}{\}}om{\{}{\'{e}}{\}}trie multivues ?
- Temps de calcul online un peu long (30s)

From Duplicate 2 (6-DOF Image Localization From Massive Geo-Tagged Reference Images - Song, Yafei; Chen, Xiaowu; Wang, Xiaogang; Zhang, Yu; Li, Jia)

R{\'{e}}sum{\'{e}}
Pipeline permettant de localiser (6 DOF) une image {\`{a}} partir d'une base de donn{\'{e}}e dense localis{\'{e}}e. On peut diviser la m{\'{e}}thode en 2 parties: premi{\`{e}}rement on retrouve les k plus proches voisins d'une image en requ{\^{e}}te par image retrival (SIFT + VBoW). On estime ensuite la pose relative de la query {\`{a}} chacun des voisins (estimation de la matrice OSR, qui encapsule la matrice fondamentale F et la d{\'{e}}formation de la lentille lambda, puis estimation de la matrice essentielle E et de la focal f et d{\'{e}}compositon de E pour r{\'{e}}cup{\'{e}}rer t et R) et on recoupe les poses relatives des deux plus robustes estimations pour retrouver la pose absolue.

Ce qui est int{\'{e}}ressant
- La m{\'{e}}thode est plus pr{\'{e}}cise que celle bas{\'{e}} corresspondance 2D/3D
- M{\'{e}}thode d'estimation de l'OSR plus robuste que au bruit l'{\'{e}}tat de l'art (mais necessite 3 correspondances de plus)

Critiques
- Necessite plusieurs vue (dans la database) pour une m{\^{e}}me query
- Pourquoi ne pas utiliser directement de la g{\'{e}}om{\'{e}}trie multivues ?
- Temps de calcul online un peu long (30s)

Computational load
Offline: 139h
Online: 30s

Scalability
Scale: Multiple places
Scalability potential: Depending on the retrivial approach},
author = {Song, Yafei and Chen, Xiaowu and Wang, Xiaogang and Zhang, Yu and Li, Jia},
file = {:home/nathan/Documents/Mendeley Desktop/Song et al/IEEE Transactions on Multimedia (ToM)/Song et al.{\_}2016{\_}6-DOF Image Localization From Massive Geo-Tagged Reference Images.pdf:pdf},
journal = {IEEE Transactions on Multimedia (ToM)},
keywords = {5 points algorithm,Outdoor,SVD,Visual place recognition,Visual words},
mendeley-tags = {5 points algorithm,Outdoor,SVD,Visual place recognition,Visual words},
number = {8},
pages = {1542--1554},
title = {{6-DOF Image Localization From Massive Geo-Tagged Reference Images}},
volume = {18},
year = {2016}
}
@article{Brejcha2017,
author = {Brejcha, Jan and {\v{C}}ad{\'{i}}k, Martin},
doi = {10.1007/s10044-017-0611-1},
file = {:home/nathan/Documents/Mendeley Desktop/Brejcha, {\v{C}}ad{\'{i}}k/Pattern Analysis and Applications/Brejcha, {\v{C}}ad{\'{i}}k{\_}2017{\_}State-of-the-art in visual geo-localization.pdf:pdf},
issn = {1433-7541},
journal = {Pattern Analysis and Applications},
keywords = {localization {\'{a}} natural environments,visual geo-localization {\'{a}} city-scale,{\'{a}} image geo-location {\'{a}}},
title = {{State-of-the-art in visual geo-localization}},
url = {http://link.springer.com/10.1007/s10044-017-0611-1},
year = {2017}
}
@article{Brachmann2017a,
abstract = {Popular research areas like autonomous driving and augmented reality have renewed the interest in image-based camera localization. In this work, we address the task of predicting the 6D camera pose from a single RGB image in a given 3D environment. With the advent of neural networks, previous works have either learned the entire camera localization process, or multiple components of a camera localization pipeline. Our key contribution is to demonstrate and explain that learning a single component of this pipeline is sufficient. This component is a fully convolutional neural network for densely regressing so-called scene coordinates, defining the correspondence between the input image and the 3D scene space. The neural network is prepended to a new end-to-end trainable pipeline. Our system is efficient, highly accurate, robust in training, and exhibits outstanding generalization capabilities. It exceeds state-of-the-art consistently on indoor and outdoor datasets. Interestingly, our approach surpasses existing techniques even without utilizing a 3D model of the scene during training, since the network is able to discover 3D scene geometry automatically, solely from single-view constraints.},
archivePrefix = {arXiv},
arxivId = {1711.10228},
author = {Brachmann, Eric and Rother, Carsten},
eprint = {1711.10228},
file = {:home/nathan/Documents/Mendeley Desktop/Brachmann, Rother/Unknown/Brachmann, Rother{\_}2017{\_}Learning Less is More - 6D Camera Localization via 3D Surface Regression.pdf:pdf},
title = {{Learning Less is More - 6D Camera Localization via 3D Surface Regression}},
url = {http://arxiv.org/abs/1711.10228},
year = {2017}
}
@article{Tian2017,
abstract = {In this paper, we address the problem of cross-view image geo-localization. Specifically, we aim to estimate the GPS location of a query street view image by finding the matching images in a reference database of geo-tagged bird's eye view images, or vice versa. To this end, we present a new framework for cross-view image geo-localization by taking advantage of the tremendous success of deep convolutional neural networks (CNNs) in image classification and object detection. First, we employ the Faster R-CNN to detect buildings in the query and reference images. Next, for each building in the query image, we retrieve the {\$}k{\$} nearest neighbors from the reference buildings using a Siamese network trained on both positive matching image pairs and negative pairs. To find the correct NN for each query building, we develop an efficient multiple nearest neighbors matching method based on dominant sets. We evaluate the proposed framework on a new dataset that consists of pairs of street view and bird's eye view images. Experimental results show that the proposed method achieves better geo-localization accuracy than other approaches and is able to generalize to images at unseen locations.},
archivePrefix = {arXiv},
arxivId = {1703.07815},
author = {Tian, Yicong and Chen, Chen and Shah, Mubarak},
eprint = {1703.07815},
file = {:home/nathan/Documents/Mendeley Desktop/Tian, Chen, Shah/arXiv preprint/Tian, Chen, Shah{\_}2017{\_}Cross-View Image Matching for Geo-localization in Urban Environments.pdf:pdf},
journal = {arXiv preprint},
title = {{Cross-View Image Matching for Geo-localization in Urban Environments}},
url = {http://arxiv.org/abs/1703.07815},
year = {2017}
}
@article{McManus2013,
abstract = {In an effort to facilitate lighting-invariant exploration, this paper presents an appearance-based approach using 3D scanning laser-rangefinders for two core visual navigation techniques: visual odometry (VO) and visual teach and repeat (VT{\&}R). The key to our method is to convert raw laser intensity data into greyscale camera-like images, in order to apply sparse, appearance-based techniques traditionally used with camera imagery. The novel concept of an image stack is introduced, which is an array of azimuth, elevation, range, and intensity images that are used to generate keypoint measurements and measurement uncertainties. Using this technique, we present the following four experiments. In the first experiment, we explore the stability of a representative keypoint detection/description algorithm on camera and laser intensity images collected over a 24 h period outside. In the second and third experiments, we validate our VO algorithm using real data collected outdoors with two different 3D scanning laser-rangefinders. Lastly, our fourth experiment presents promising preliminary VT{\&}R localization results, where the teaching phase was done during the day and the repeating phase was done at night. These experiments show that it possible to overcome lighting sensitivity encountered with cameras, yet continue to exploit the heritage of the appearance-based visual odometry pipeline. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {McManus, Colin and Furgale, Paul and Barfoot, Timothy D.},
doi = {10.1016/j.robot.2013.04.008},
file = {:home/nathan/Documents/Mendeley Desktop/McManus, Furgale, Barfoot/Robotics and Autonomous Systems (RAS)/McManus, Furgale, Barfoot{\_}2013{\_}Towards lighting-invariant visual navigation An appearance-based approach using scanning laser-rangefinde.pdf:pdf},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems (RAS)},
keywords = {Appearance-based lidar,Lighting-invariant visual odometry,Visual teach and repeat},
number = {8},
pages = {836--852},
publisher = {Elsevier B.V.},
title = {{Towards lighting-invariant visual navigation: An appearance-based approach using scanning laser-rangefinders}},
url = {http://dx.doi.org/10.1016/j.robot.2013.04.008},
volume = {61},
year = {2013}
}
@article{Xu2017,
abstract = {Several recent works have shown that part-based image representation provides state-of-the-art performance for fine-grained categorization. Moreover, it has also been shown that image global representation generated by aggregating deep convolutional features provides excellent performance for image retrieval. In this paper we propose a novel aggregation method, which utilizes the information of retrieval object parts. The proposed part-based weighting aggregation (PWA) method utilizes the normalized feature maps as part detectors to weight and aggregate the convolutional features. The part detectors which are selected by the unsupervised method highlight the discriminative parts of objects and effectively suppress the noise of background. We experiment on five public standard datasets for image retrieval. Our unsupervised PWA method outperforms the state-of-the-art approaches based on pre-trained networks and achieves comparable accuracy with the fine-tuned methods. It is worth noting that our unsupervised method is very suitable and effective for the situation where the annotated training dataset is difficult to collect.},
archivePrefix = {arXiv},
arxivId = {1705.01247},
author = {Xu, Jian and Shi, Cunzhao and Qi, Chengzuo and Wang, Chunheng and Xiao, Baihua},
eprint = {1705.01247},
file = {:home/nathan/Documents/Mendeley Desktop/Xu et al/arXiv preprint/Xu et al.{\_}2017{\_}Part-based Weighting Aggregation of Deep Convolutional Features for Image Retrieval.pdf:pdf},
journal = {arXiv preprint},
title = {{Part-based Weighting Aggregation of Deep Convolutional Features for Image Retrieval}},
url = {http://arxiv.org/abs/1705.01247},
year = {2017}
}
@inproceedings{Sattler2015,
abstract = {{\textcopyright} 2015 IEEE.Structure-based localization is the task of finding the absolute pose of a given query image w.r.t. a pre-computed 3D model. While this is almost trivial at small scale, special care must be taken as the size of the 3D model grows, because straight-forward descriptor matching becomes ineffective due to the large memory footprint of the model, as well as the strictness of the ratio test in 3D. Recently, several authors have tried to overcome these problems, either by a smart compression of the 3D model or by clever sampling strategies for geometric verification. Here we explore an orthogonal strategy, which uses all the 3D points and standard sampling, but performs feature matching implicitly, by quantization into a fine vocabulary. We show that although this matching is ambiguous and gives rise to 3D hyperpoints when matching each 2D query feature in isolation, a simple voting strategy, which enforces the fact that the selected 3D points shall be co-visible, can reliably find a locally unique 2D-3D point assignment. Experiments on two large-scale datasets demonstrate that our method achieves state-of-the-art performance, while the memory footprint is greatly reduced, since only visual word labels but no 3D point descriptors need to be stored.},
annote = {R{\'{e}}sum{\'{e}}


Ce que je n'ai pas compris


Ce qui est int{\'{e}}ressant


Critiques


A approfondir


Computational load
Offline:
Online:

Scalability
Scale:
Scalability potential:},
author = {Sattler, Torsten and Havlena, Michal and Radenovi{\'{c}}, Filip and Schindler, Konrad and Pollefeys, Marc},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.243},
file = {:home/nathan/Documents/Mendeley Desktop/Sattler et al/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Sattler et al.{\_}2015{\_}Hyperpoints and fine vocabularies for large-scale location recognition.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
pages = {2102--2106},
title = {{Hyperpoints and fine vocabularies for large-scale location recognition}},
volume = {11-18-Dece},
year = {2015}
}
@article{Greene2017,
abstract = {Figure 1: Fast Lightweight Mesh Estimation: FLaME generates 3D mesh reconstructions from monocular images in real-time onboard computationally constrained platforms. The key to the approach is a graph-based variational optimization framework that allows for the mesh to be efficiently smoothed and refined. The top row of images (from left to right) show the meshes computed onboard a small autonomous robot flying at 3.5 meters-per-second as it avoids a tree. The bottom row shows the current frame (left), the collision-free plan in pink (middle), and the dense depthmap generated from the mesh (right) for each timestep along the approach. Abstract We propose a lightweight method for dense online monocular depth estimation capable of reconstructing 3D meshes on computationally constrained platforms. Our main contribution is to pose the reconstruction problem as a non-local variational optimization over a time-varying De-launay graph of the scene geometry, which allows for an efficient, keyframeless approach to depth estimation. The graph can be tuned to favor reconstruction quality or speed and is continuously smoothed and augmented as the camera explores the scene. Unlike keyframe-based approaches, the optimized surface is always available at the current pose, which is necessary for low-latency obstacle avoidance. FLaME (Fast Lightweight Mesh Estimation) can gener-ate mesh reconstructions at upwards of 230 Hz using less than one Intel i7 CPU core, which enables operation on size, weight, and power-constrained platforms. We present results from both benchmark datasets and experiments run-ning FLaME in-the-loop onboard a small flying quadrotor.},
author = {Greene, W Nicholas and Roy, Nicholas},
file = {:home/nathan/Documents/Mendeley Desktop/Greene, Roy/IEEE International Conference on Computer Vision (ICCV)/Greene, Roy{\_}2017{\_}FLaME Fast Lightweight Mesh Estimation using Variational Smoothing on Delaunay Graphs.pdf:pdf},
journal = {IEEE International Conference on Computer Vision (ICCV)},
pages = {4686--4694},
title = {{FLaME: Fast Lightweight Mesh Estimation using Variational Smoothing on Delaunay Graphs}},
url = {http://groups.csail.mit.edu/rrg/papers/greene{\_}iccv17.pdf},
year = {2017}
}
@inproceedings{Elbaz2017,
abstract = {We present an algorithm for registration between a large-scale point cloud and a close-proximity scanned point cloud, providing a localization solution that is fully in- dependent of prior information about the initial positions of the two point cloud coordinate systems. The algo- rithm, denoted LORAX, selects super-points—local subsets of points—and describes the geometric structure of each with a low-dimensional descriptor. These descriptors are then used to infer potential matching regions for an effi- cient coarse registration process, followed by a fine-tuning stage. The set of super-points is selected by covering the point clouds with overlapping spheres, and then filtering out those of low-quality or nonsalient regions. The descriptors are computed using state-of-the-art unsupervised machine learning, utilizing the technology of deep neural network based auto-encoders. This novel framework provides a strong alternative to the common practice of using manually designed key-point descriptors for coarse point cloud registration. Utilizing super-points instead of key-points allows the available geo- metrical data to be better exploited to find the correct trans- formation. Encoding local 3D geometric structures using a deep neural network auto-encoder instead of traditional descriptors continues the trend seen in other computer vi- sion applications and indeed leads to superior results. The algorithm is tested on challenging point cloud registration datasets, and its advantages over previous approaches as well as its robustness to density changes, noise and missing data are shown.},
author = {Elbaz, Gil and Avraham, Tamar and Elbaz, Gil},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Elbaz, Avraham, Elbaz/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Elbaz, Avraham, Elbaz{\_}2017{\_}3D Point Cloud Registration for Localization using a Deep Neural Network Auto-Encoder.pdf:pdf},
number = {July},
pages = {4631--4640},
title = {{3D Point Cloud Registration for Localization using a Deep Neural Network Auto-Encoder}},
year = {2017}
}
@inproceedings{Azzi2016,
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de Image Based Localization (IBL) utilisant le descripteur GIST pour pr{\'{e}}-selectionner des images potentiellement matchables (suite/conclusion des travaux de la these de master de Azzi ?). Pr{\'{e}}-calcul des descrpiteurs GIST des images de la database puis comparaison avec le descripteur de la query. Selection d'un sous-ensemble de la database par thresholding suivant la GIST-distance, puis matching des SIFT avec les 3D-SIFT (par FLANN). Verficication du nombre d'inliers apr{\`{e}}s le matching et le RANSAC, si pas assez {\'{e}}lev{\'{e}} diminution du threshold GIST.

Ce qui est int{\'{e}}ressant
- Augmentation de la precision/methode classiques
- "Temps de calcul"

Critiques
- Temps de calcul sans l'extraction des features
- Aucune information sur comment ils reconstruisent la pose

Computational load
Offline: ?
Online: {\textless} 0.1 s (without feature extraction)

Scalability
Scale: Multiple places
Scalability potential: Real},
author = {Azzi, Charbel and Asmar, Daniel and Fakih, Adel and Zelek, John},
booktitle = {British Machine Vision Conference (BMVC)},
file = {:home/nathan/Documents/Mendeley Desktop/Azzi et al/British Machine Vision Conference (BMVC)/Azzi et al.{\_}2016{\_}Filtering 3D Keypoints Using GIST For Accurate Image-Based Localization.pdf:pdf},
number = {2},
pages = {1--12},
title = {{Filtering 3D Keypoints Using GIST For Accurate Image-Based Localization}},
year = {2016}
}
@inproceedings{Cieslewski2017,
abstract = {—State-of-the-art systems that do place recognition in a group of n robots either rely on a centralized solution, where each robot's map is sent to a central server, or a decentralized solution, where the map is either sent to all other robots, or robots within a communication range. Both approaches have their drawbacks: centralized systems rely on a central entity which handles all the computational load and cannot be deployed in large, remote areas; decentralized systems either exchange n times more data or preclude matches between robots that visit the same place at different times while never being close enough to communicate directly. We propose a novel decentralized approach which requires a similar amount of data exchange as a centralized system, without precluding any matches. The core idea is that the candidate selection in visual bag-of-words can be distributed by pre-assigning words of the vocabulary to different robots. The result of this candidate selection is then used to choose a single robot to which the full query is sent. We validate our approach on real data and discuss its merit in different network models. To the best of our knowledge, this is the first work to use a distributed inverted index in multi-robot place recognition.},
archivePrefix = {arXiv},
arxivId = {1705.10739},
author = {Cieslewski, Titus and Scaramuzza, Davide},
booktitle = {Proceedings of the IEEE International Conference of Robotics and Automation Workshop (ICRAW)},
doi = {10.1109/LRA.2017.2650153},
eprint = {1705.10739},
file = {:home/nathan/Documents/Mendeley Desktop/Cieslewski, Scaramuzza/Proceedings of the IEEE International Conference of Robotics and Automation Workshop (ICRAW)/Cieslewski, Scaramuzza{\_}2017{\_}Efficient Decentralized Visual Place Recognition From Full-Image Descriptors.pdf:pdf},
issn = {2377-3766},
number = {2},
pages = {1--8},
title = {{Efficient Decentralized Visual Place Recognition From Full-Image Descriptors}},
volume = {2},
year = {2017}
}
@inproceedings{Arth2009,
abstract = {We present a fast and memory efficient method for localizing a mobile user's 6DOF pose from a single camera image. Our approach registers a view with respect to a sparse 3D point reconstruction. The 3D point dataset is partitioned into pieces based on visibility constraints and occlusion culling, making it scalable and efficient to handle. Starting with a coarse guess, our system only considers features that can be seen from the user's position. Our method is resource efficient, usually requiring only a few megabytes of memory, thereby making it feasible to run on low-end devices such as mobile phones. At the same time it is fast enough to give instant results on this device class.},
author = {Arth, Clemens and Wagner, Daniel and Klopschitz, Manfred and Irschara, Arnold and Schmalstieg, Dieter},
booktitle = {Proceedings of the IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
doi = {10.1109/ISMAR.2009.5336494},
file = {:home/nathan/Documents/Mendeley Desktop/Arth et al/Proceedings of the IEEE International Symposium on Mixed and Augmented Reality (ISMAR)/Arth et al.{\_}2009{\_}Wide area localization on mobile phones.pdf:pdf},
isbn = {978-1-4244-5390-0},
keywords = {3D point dataset,3D/stereo scene analysis I.4.8 [Image Processing A,Cameras,Computer Vision C.5.3 [Computer System Implementat,Computer vision,Data acquisition,I.2.10 [Artificial Intelligence]: Vision and Scene,I.5.4 [Pattern Recognition]: Applications,Image analysis,Image generation,Image segmentation,Layout,Mobile computing,Mobile handsets,Portable devices (e.g.,Tracking,camera image,image reconstruction,image registration,laptops,mobile phone,mobile radio,mobile user 6DOF pose localization,object detection,occlusion culling,personal digital assistants),pose estimation,sparse 3D point reconstruction,wide area localization},
pages = {73--82},
title = {{Wide area localization on mobile phones}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5336494},
year = {2009}
}
@inproceedings{Wang2015,
abstract = {In this paper we propose a novel approach to localization in very large indoor spaces (i.e., 200+ store shopping malls) that takes a single image and a floor plan of the environment as input. We formulate the localization problem as inference in a Markov random field, which jointly reasons about text detection (localizing shop's names in the image with precise bounding boxes), shop facade segmentation, as well as camera's rotation and translation within the entire shopping mall. The power of our approach is that it does not use any prior information about appearance and instead exploits text detections corresponding to the shop names. This makes our method applicable to a variety of domains and robust to store appearance variation across countries, seasons, and illumination conditions. We demonstrate the performance of our approach in a new dataset we collected of two very large shopping malls, and show the power of holistic reasoning.},
author = {Wang, Shenlong and Fidler, Sanja and Urtasun, Raquel},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.309},
file = {:home/nathan/Documents/Mendeley Desktop/Wang, Fidler, Urtasun/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Wang, Fidler, Urtasun{\_}2015{\_}Lost shopping! monocular localization in large indoor spaces.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
pages = {2695--2703},
title = {{Lost shopping! monocular localization in large indoor spaces}},
volume = {11-18-Dece},
year = {2015}
}
@inproceedings{Fernandez-Moral2013,
abstract = {This paper presents a new method for recognizing places in indoor environments based on the extraction of planar regions from range data provided by a hand-held RGB-D sensor. We propose to build a plane-based map (PbMap) consisting of a set of 3D planar patches described by simple geometric features (normal vector, centroid, area, etc.). This world representation is organized as a graph where the nodes represent the planar patches and the edges connect planes that are close by. This map structure permits to efficiently select subgraphs representing the local neighborhood of observed planes, that will be compared with other subgraphs corresponding to local neighborhoods of planes acquired previously. To find a candidate match between two subgraphs we employ an interpretation tree that permits working with partially observed and missing planes. The candidates from the interpretation tree are further checked out by a rigid registration test, which also gives us the relative pose between the matched places. The experimental results indicate that the proposed approach is an efficient way to solve this problem, working satisfactorily even when there are substantial changes in the scene (lifelong maps).},
author = {Fernandez-Moral, Edurado and Mayol-Cuevas, Walterio and Arevalo, V. and Gonzalez-Jimenez, J.},
booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2013.6630951},
file = {:home/nathan/Documents/Mendeley Desktop/Fernandez-Moral et al/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Fernandez-Moral et al.{\_}2013{\_}Fast place recognition with plane-based maps.pdf:pdf},
isbn = {9781467356411},
issn = {10504729},
pages = {2719--2724},
title = {{Fast place recognition with plane-based maps}},
year = {2013}
}
@article{Morago2015,
author = {Morago, Brittany and Bui, Giang and Duan, Ye},
journal = {IEEE Transactions on Image Processing (ToIP)},
number = {11},
pages = {4474--4487},
publisher = {IEEE},
title = {{An Ensemble Approach to Image Matching Using Contextual Features}},
volume = {24},
year = {2015}
}
@inproceedings{Gee2012,
abstract = {With the advent of real-time dense scene reconstruction from handheld cameras, one key aspect to enable robust operation is the ability to relocalise in a previously mapped environment or after loss of measurement. Tasks such as operating on a workspace, where moving objects and occlusions are likely, require a recovery competence in or- der to be useful. For RGBD cameras, this must also include the ability to relocalise in areas with reduced visual texture. This paper describes a method for relocalisation of a freely moving RGBD camera in small workspaces. The approach combines both 2D image and 3D depth information to estimate the full 6D camera pose. The method uses a general regression over a set of synthetic views distributed throughout an informed es- timate of possible camera viewpoints. The resulting relocalisation is accurate and works faster than framerate and the system's performance is demonstrated through a compari- son against visual and geometric feature matching relocalisation techniques on sequences with moving objects and minimal texture.},
annote = {R{\'{e}}sum{\'{e}}
Papier permettant de relocaliser une cam{\'{e}}ra dans un environement 3D connu en utilisant une cam{\'{e}}ra RGBD. Les auteurs font seulement une g{\'{e}}n{\'{e}}ration de vue sythetique sur le mod{\`{e}}le et au query time compare rapidement (pixel {\`{a}} pixel sur la valeur de niveau de gris et la profondeur) les images sythetiques et la query. La comparaison montre que ca marche mieux que du SIFT-like dans le cas de d'endroits non-textur{\'{e}}s.

Ce que je n'ai pas compris
- Pas lu en d{\'{e}}tail

Ce qui est int{\'{e}}ressant
- Marche sur du non-textur{\'{e}}
- Plus rapide {\`{a}} comparer que du SIFT ({\`{a}} cause de l'extraction)

Critiques
- Tr{\`{e}}s orient{\'{e}} robotique (cf. etat de l'art)

A approfondir
- VPFH (descripteurs de points 3D)

Computational load
Offline: 30ms/view sythese
Online: 60ms/1000 views comparison

Scalability
Scale: room
Scalability potential: No},
author = {Gee, Andrew P. and Mayol-Cuevas, Walterio},
booktitle = {British Machine Vision Conference (BMVC)},
doi = {10.5244/C.26.113},
file = {:home/nathan/Documents/Mendeley Desktop/Gee, Mayol-Cuevas/British Machine Vision Conference (BMVC)/Gee, Mayol-Cuevas{\_}2012{\_}6D Relocalisation for RGBD Cameras Using Synthetic View Regression.pdf:pdf},
isbn = {1-901725-46-4},
pages = {1--11},
title = {{6D Relocalisation for RGBD Cameras Using Synthetic View Regression}},
url = {http://www.bmva.org/bmvc/2012/BMVC/paper113/paper113.pdf},
year = {2012}
}
@inproceedings{Toft2017,
author = {Toft, Carl and Olsson, Carl and Kahl, Fredrik},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision Workshop (ICCVW)},
file = {:home/nathan/Documents/Mendeley Desktop/Toft, Olsson, Kahl/Proceedings of the IEEE International Conference on Computer Vision Workshop (ICCVW)/Toft, Olsson, Kahl{\_}2017{\_}Long-term 3D Localization and Pose from Semantic Labellings.pdf:pdf},
pages = {650--659},
title = {{Long-term 3D Localization and Pose from Semantic Labellings}},
year = {2017}
}
@article{Arroyo2016,
abstract = {— The extreme variability in the appearance of a place across the four seasons of the year is one of the most challenging problems in life-long visual topological localization for mobile robotic systems and intelligent vehicles. Tradi-tional solutions to this problem are based on the descrip-tion of images using hand-crafted features, which have been shown to offer moderate invariance against seasonal changes. In this paper, we present a new proposal focused on auto-matically learned descriptors, which are processed by means of a technique recently popularized in the computer vi-sion community: Convolutional Neural Networks (CNNs). The novelty of our approach relies on fusing the image information from multiple convolutional layers at several levels and granularities. In addition, we compress the redundant data of CNN features into a tractable number of bits for efficient and robust place recognition. The final descriptor is reduced by applying simple compression and binarization techniques for fast matching using the Hamming distance. An exhaustive experimental evaluation confirms the improved performance of our proposal (CNN-VTL) with respect to state-of-the-art meth-ods over varied long-term datasets recorded across seasons.},
author = {Arroyo, Roberto and Alcantarilla, Pablo F. and Bergasa, Luis M. and Romera, Eduardo},
doi = {10.1109/IROS.2016.7759685},
file = {:home/nathan/Documents/Mendeley Desktop/Arroyo et al/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Arroyo et al.{\_}2016{\_}Fusion and Binarization of CNN Features for Robust Topological Localization across Seasons.pdf:pdf},
isbn = {9781509037612},
issn = {21530866},
journal = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
pages = {4656--4663},
title = {{Fusion and Binarization of CNN Features for Robust Topological Localization across Seasons}},
year = {2016}
}
@inproceedings{Cupec,
author = {Cupec, Robert and Nyarko, Emmanuel Karlo and Filko, Damir and Markasovi{\'{c}}, Luka},
booktitle = {Proceedings of the International Conference {\&} Workshop on Mechatronics in Practice and Education (MECHEDU).},
file = {:home/nathan/Documents/Mendeley Desktop/Cupec et al/Proceedings of the International Conference {\&} Workshop on Mechatronics in Practice and Education (MECHEDU)/Cupec et al.{\_}2015{\_}Recognition of Objects and Places in 3D Point Clouds for Robotic Applications.pdf:pdf},
keywords = {3d sensors,computer,recognition},
title = {{Recognition of Objects and Places in 3D Point Clouds for Robotic Applications}},
year = {2015}
}
@article{Pepperell2016,
abstract = {Vision-based place recognition is becoming an increasingly viable component of navigation systems for autonomous robots and personal aids. However, attaining robustness to variations in environmental conditions—such as time of day, weather and season—and camera viewpoint remains a major challenge. Featureless, sequence-based place recognition techniques have demonstrated promise, but often rely on long image sequences, manually-tuned parameters and exhaustive sequence match searching through multiple locations and image scales. In this paper, we address these deficiencies by implementing a condition-invariant, sequence-based place recognition algorithm suitable for networked environments, such as city streets, and routes with lateral platform shift, such as multiple-lane roads. We achieve this capability by augmenting the traditional 1D image database with a directed graph to describe the branching of contiguous sections of imagery at intersections. A particle filter is then used to efficiently explore these paths, as well as various lateral positions synthesized by rescaling imagery. Our proposed approach eliminates manual tuning of sequence length parameters, improves localization on branched routes, improves overall place recognition accuracy and coverage, and reduces computational requirements. We evaluated the new method against the original SeqSLAM and SMART algorithms on two day–night, road-based datasets and a summer–winter train dataset, where it attained superior precision-recall performance and coverage in all environments. Together, these contributions represent a significant step towards the provision of a robust, near parameter-free condition- and viewpoint-invariant visual place recognition capability for vehicles and robots.},
author = {Pepperell, Edward and Corke, Peter and Milford, Michael J.},
doi = {10.1177/0278364915618766},
file = {:home/nathan/Documents/Mendeley Desktop/Pepperell, Corke, Milford/The International Journal of Robotics Research (IJRR)/Pepperell, Corke, Milford{\_}2016{\_}Routed roads Probabilistic vision-based place recognition for changing conditions, split streets and vari.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research (IJRR)},
keywords = {localization,place recognition,robotic vision},
number = {9},
pages = {1057--1179},
title = {{Routed roads: Probabilistic vision-based place recognition for changing conditions, split streets and varied viewpoints}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364915618766},
volume = {35},
year = {2016}
}
@article{Johansson2002,
abstract = {Abstract We describe an automatic system for pose - estimation from a single image in a city scene . Each building has a model consisting of a number of parallel planes associ- ated with it. The homographies for the best match of the planes to the image is estimated ...},
author = {Johansson, Bj{\"{o}}rn and Cipolla, Roberto},
file = {:home/nathan/Documents/Mendeley Desktop/Johansson, Cipolla/Proceedings of the International Conference on Signal Processing, Pattern Recognition and Applications/Johansson, Cipolla{\_}2002{\_}A system for automatic pose-estimation from a single image in a city scene.pdf:pdf},
journal = {Proceedings of the International Conference on Signal Processing, Pattern Recognition and Applications},
pages = {1--6},
title = {{A system for automatic pose-estimation from a single image in a city scene}},
year = {2002}
}
@article{Zhu2017a,
author = {Zhu, Yilin and Jing, Naihuan and Chen, Wenquan},
file = {:home/nathan/Documents/Mendeley Desktop/Zhu, Jing, Chen/Unknown/Zhu, Jing, Chen{\_}2017{\_}A Novel Image Retrieval Model Based on Semantic Information and Probability Rough Set Analysis Method.pdf:pdf},
isbn = {9781509067930},
keywords = {-image retrieval,between pixels of the,color,contourlet transform,etc,have been widely,image,image descriptors like texture,pca,retrievals,rough set,semantic information,the relationship},
pages = {23--27},
title = {{A Novel Image Retrieval Model Based on Semantic Information and Probability Rough Set Analysis Method}},
year = {2017}
}
@inproceedings{torralba2003context,
author = {Torralba, Antonio and Murphy, Kevin P. and Freeman, William T. and Rubin, Mark A.},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
pages = {273--280},
title = {{Context-based vision system for place and object recognition.}},
volume = {3},
year = {2003}
}
@inproceedings{Wang2013frif,
author = {Wang, Zhenhua and Fan, Bin and Wu, Fuchao},
booktitle = {British Machine Vision Conference (BMVC)},
title = {{FRIF: Fast Robust Invariant Feature.}},
year = {2013}
}
@inproceedings{Vo2016,
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode pour de la localisation cross-view (retrouver la position d'une image street-view en fonction d'une image a{\'{e}}rienne) utilisant des CNN. Comparaison de deux types de CNN : le premier pour la classification (qui dit si une paire d'image est bonne ou non) -{\textgreater} comparaison d'un reseau simple (un CNN ou les deux images sont concat{\'{e}}n{\'{e}}es) et un reseau siamois hybride (un reseau par image puis une couche de fully connected pour la classif), le deuxi{\`{e}}me pour le ranking -{\textgreater} reseau siamois (CNN produisant des descripteurs et entrain{\'{e}}s en m{\^{e}}me temps MAIS pas avec les poids partag{\'{e}}s car pas le m{\^{e}}me domaine d'image) et reseau triple. Ils introduisent une m{\'{e}}trique de comparaison des descrpteurs CNN exponentiel (distance based logistic DBL). Les auteurs s'interessent {\'{e}}galement {\`{a}} l'invariance en rotation des paires d'images en entrainant plusieurs types de r{\'{e}}seaux invariant {\`{a}} diff{\'{e}}rents degr{\'{e}}es de rotations (en introduisant une rotation volontaire dans l'apprentissage) et en introduisant une auxilary loss (branch{\'{e}}e en parall{\`{e}}le sur le reseau) pour regresser la rotation. Ils font enfin une comparaison sur la classification (est ce que mes deux images sont bien des paires ?) et sur le ranking (retrouver la bonne image parmi les K)

Ce qui est int{\'{e}}ressant
- Auxilary loss
- DLB
- Travail sur l'invariance en rotation

Critiques
- Pas de temps de calcul
- {\~{}}50 pour recall @top 1{\%}

Computational load
Offline: ?
Online: ?

Scalability
Scale: City
Scalability potential: -},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Vo, Nam N. and Hays, James},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-319-46448-0},
eprint = {1311.2901},
file = {:home/nathan/Documents/Mendeley Desktop/Vo, Hays/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Vo, Hays{\_}2016{\_}Localizing and Orienting Street Views Using Overhead Imagery.pdf:pdf},
isbn = {978-3-319-46447-3},
issn = {0302-9743},
keywords = {deep learning,image geolocalization,image matching,siamese network,triplet network},
pages = {494--509},
title = {{Localizing and Orienting Street Views Using Overhead Imagery}},
url = {http://link.springer.com/10.1007/978-3-319-46448-0},
volume = {9905},
year = {2016}
}
@article{Liu2015,
abstract = {We consider the problem of depth estimation from a single monocular image in this work. It is a challenging task as no reliable depth cues are available, e.g., stereo correspondences, motions, etc. Previous efforts have been focusing on exploiting geometric priors or additional sources of information, with all using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) are setting new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimations can be naturally formulated into a continuous conditional random field (CRF) learning problem. Therefore, we in this paper present a deep convolutional neural field model for estimating depths from a single image, aiming to jointly explore the capacity of deep CNN and continuous CRF. Specifically, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework. The proposed method can be used for depth estimations of general scenes with no geometric priors nor any extra information injected. In our case, the integral of the partition function can be analytically calculated, thus we can exactly solve the log-likelihood optimization. Moreover, solving the MAP problem for predicting depths of a new image is highly efficient as closed-form solutions exist. We experimentally demonstrate that the proposed method outperforms state-of-the-art depth estimation methods on both indoor and outdoor scene datasets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.6387v2},
author = {Liu, Fayao and Shen, Chunhua and Lin, Guosheng},
doi = {10.1109/CVPR.2015.7299152},
eprint = {arXiv:1411.6387v2},
file = {:home/nathan/Documents/Mendeley Desktop/Liu, Shen, Lin/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Liu, Shen, Lin{\_}2015{\_}Deep Convolutional Neural Fields for Depth Estimation from a Single Image ∗.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {1--13},
title = {{Deep Convolutional Neural Fields for Depth Estimation from a Single Image ∗}},
year = {2015}
}
@inproceedings{Tzeng2013,
abstract = {We propose a system for user-aided visual localization of desert imagery without the use of any metadata such as GPS readings, camera focal length, or field-of-view. The system makes use only of publicly available digital elevation models (DEMs) to rapidly and accurately locate photographs in non-urban environments such as deserts. Our system generates synthetic skyline views from a DEM and extracts stable concavity-based features from these skylines to form a database. To localize queries, a user manually traces the skyline on an input photograph. The skyline is automatically refined based on this estimate, and the same concavity-based features are extracted. We then apply a variety of geometrically constrained matching techniques to efficiently and accurately match the query skyline to a database skyline, thereby localizing the query image. We evaluate our system using a test set of 44 ground-truthed images over a 10, 000 km2 region of interest in a desert and show that in many cases, queries can be localized with precision as fine as 100 m2.},
author = {Tzeng, Eric and Zhai, Andrew and Clements, Matthew and Townshend, Raphael and Zakhor, Avideh},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2013.42},
file = {:home/nathan/Documents/Mendeley Desktop/Tzeng et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)/Tzeng et al.{\_}2013{\_}User-driven geolocation of untagged desert imagery using digital elevation models.pdf:pdf},
isbn = {9780769549903},
issn = {21607508},
pages = {237--244},
title = {{User-driven geolocation of untagged desert imagery using digital elevation models}},
year = {2013}
}
@article{Bampis2017,
author = {Bampis, Loukas and Amanatiadis, Angelos and Gasteratos, Antonios},
file = {:home/nathan/Documents/Mendeley Desktop/Bampis, Amanatiadis, Gasteratos/Unknown/Bampis, Amanatiadis, Gasteratos{\_}2017{\_}High Order Visual Words for Structure-Aware and Viewpoint-Invariant Loop Closure Detection.pdf:pdf},
isbn = {9781538626818},
pages = {4268--4275},
title = {{High Order Visual Words for Structure-Aware and Viewpoint-Invariant Loop Closure Detection}},
year = {2017}
}
@article{Garcia-Fidalgo2015,
abstract = {Topological maps model the environment as a graph, where nodes are distinctive places of the environment and edges indicate topological relationships between them. They represent an interesting alternative to the classic metric maps, due to their simplicity and storage needs, what has made topological mapping and localization an active research area. The different solutions that have been proposed during years have been designed around several kinds of sensors. However, in the last decades, vision approaches have emerged because of the technology improvements and the amount of useful information that a camera can provide. In this paper, we review the main solutions presented in the last fifteen years, and classify them in accordance to the kind of image descriptor employed. Advantages and disadvantages of each approach are thoroughly reviewed and discussed.},
author = {Garcia-Fidalgo, Emilio and Ortiz, Alberto},
doi = {10.1016/j.robot.2014.11.009},
file = {:home/nathan/Documents/Mendeley Desktop/Garcia-Fidalgo, Ortiz/Robotics and Autonomous Systems (RAS)/Garcia-Fidalgo, Ortiz{\_}2015{\_}Vision-based topological mapping and localization methods A survey.pdf:pdf},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems (RAS)},
keywords = {Bag of Words,Image descriptors,Localization,Loop closure,Topological mapping,Visual SLAM},
pages = {1--20},
title = {{Vision-based topological mapping and localization methods: A survey}},
volume = {64},
year = {2015}
}
@article{Wang2014,
abstract = {Similarity search (nearest neighbor search) is a problem of pursuing the data items whose distances to a query item are the smallest from a large database. Various methods have been developed to address this problem, and recently a lot of efforts have been devoted to approximate search. In this paper, we present a survey on one of the main solutions, hashing, which has been widely studied since the pioneering work locality sensitive hashing. We divide the hashing algorithms two main categories: locality sensitive hashing, which designs hash functions without exploring the data distribution and learning to hash, which learns hash functions according the data distribution, and review them from various aspects, including hash function design and distance measure and search scheme in the hash coding space.},
archivePrefix = {arXiv},
arxivId = {1408.2927},
author = {Wang, Jingdong and Shen, Heng Tao and Song, Jingkuan and Ji, Jianqiu},
doi = {10.1561/2200000016},
eprint = {1408.2927},
file = {:home/nathan/Documents/Mendeley Desktop/Wang et al/arXiv preprint/Wang et al.{\_}2014{\_}Hashing for Similarity Search A Survey.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {arXiv preprint},
pages = {1--29},
title = {{Hashing for Similarity Search: A Survey}},
url = {http://arxiv.org/abs/1408.2927},
year = {2014}
}
@inproceedings{Godard2017,
abstract = {Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage. We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Exploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth.},
archivePrefix = {arXiv},
arxivId = {1609.03677},
author = {Godard, Cl{\'{e}}ment and {Mac Aodha}, Oisin and Brostow, Gabriel J.},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.699},
eprint = {1609.03677},
file = {:home/nathan/Documents/Mendeley Desktop/Godard, Mac Aodha, Brostow/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Godard, Mac Aodha, Brostow{\_}2017{\_}Unsupervised Monocular Depth Estimation with Left-Right Consistency.pdf:pdf},
title = {{Unsupervised Monocular Depth Estimation with Left-Right Consistency}},
url = {http://arxiv.org/abs/1609.03677},
year = {2017}
}
@article{Pedronette2016,
abstract = {Several re-ranking algorithms have been proposed recently. Some effective approaches are based on complex graph-based diffusion processes, which usually are time consuming and therefore inappropriate for real-world large scale shape collections. In this paper, we introduce a novel graph-based approach for iterative distance learning in shape retrieval tasks. The proposed method is based on the combination of graphs defined in terms of multiple ranked lists. The efficiency of the method is guaranteed by the use of only top positions of ranked lists in the definition of graphs that encode reciprocal references. Effectiveness analysis performed in three widely used shape datasets demonstrate that the proposed graph-based ranked-list model yields significant gains (up to +55.52{\%}) when compared with the use of shape descriptors in isolation. Furthermore, the proposed method also yields comparable or superior effectiveness scores when compared with several state-of-the-art approaches.},
author = {Pedronette, Daniel Carlos Guimar{\~{a}}es and Almeida, Jurandy and Torres, Ricardo da S.},
doi = {10.1016/j.patrec.2016.05.021},
file = {:home/nathan/Documents/Mendeley Desktop/Pedronette, Almeida, Torres/Pattern Recognition Letters/Pedronette, Almeida, Torres{\_}2016{\_}A graph-based ranked-list model for unsupervised distance learning on shape retrieval.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Graph-based approaches,Ranking methods,Shape retrieval},
pages = {357--367},
title = {{A graph-based ranked-list model for unsupervised distance learning on shape retrieval}},
volume = {83},
year = {2016}
}
@article{Ruder2017,
archivePrefix = {arXiv},
arxivId = {1706.05098},
author = {Ruder, Sebastian},
eprint = {1706.05098},
file = {:home/nathan/Documents/Mendeley Desktop/Ruder/arXiv preprint/Ruder{\_}2017{\_}An Overview of Multi-Task Learning in Deep Neural Networks.pdf:pdf},
journal = {arXiv preprint},
number = {May},
title = {{An Overview of Multi-Task Learning in Deep Neural Networks}},
year = {2017}
}
@inproceedings{Torii2013,
abstract = {Repeated structures such as building facades, fences or road markings $\backslash$noften represent a significant challenge for place recognition. Repeated $\backslash$nstructures are notoriously hard for establishing correspondences using $\backslash$nmulti-view geometry. Even more importantly, they violate the feature $\backslash$nindependence assumed in the bag-of-visual-words representation which often leads $\backslash$nto over-counting evidence and significant degradation of retrieval performance. $\backslash$nIn this work we show that repeated structures are not a nuisance but, when $\backslash$nappropriately represented, they form an important distinguishing feature for $\backslash$nmany places. We describe a representation of repeated structures suitable for $\backslash$nscalable retrieval. It is based on robust detection of repeated image structures $\backslash$nand a simple modification of weights in the bag-of-visual-word model. Place $\backslash$nrecognition results are shown on datasets of street-level imagery from $\backslash$nPittsburgh and San Francisco demonstrating significant gains in recognition $\backslash$nperformance compared to the standard bag-of-visual-words baseline and more $\backslash$nrecently proposed burstiness weighting.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de visual place recognition qui int{\'{e}}gre une gestion des structures r{\'{e}}p{\'{e}}titives dans les images. Les auteurs sont partis du sch{\'{e}}ma classique d'image retrieval en utilisant un dictionnaire de BoVW et en modifiant le poid des VW d{\'{e}}tect{\'{e}}s en fonction de leur redondance dans l'image originale. Cette modification se fait en deux {\'{e}}tapes, dans un premier temps les auteurs proc{\`{e}}dent {\`{a}} un regroupement des features similaires dans l'image en fonction de leur descripteurs, de leur {\'{e}}chelle et de leur proximit{\'{e}} spatiale. Ils se servent ensuite du graph de descripteurs obtenu pour donner un poids au VW de l'image qui permet de d{\'{e}}crire dans sa globalit{\'{e}} la structure r{\'{e}}p{\'{e}}t{\'{e}} sans lui donner une trop forte influence dans le vecteur descripteur (am{\'{e}}lioration du soft assignment). 

Ce que je n'ai pas compris
- Les d{\'{e}}tails sur l'assignement des poids aux VW

Ce qui est int{\'{e}}ressant
- Cout calculatoire relativement faible (juste une {\'{e}}tape de regroupement des features similaires peut {\^{e}}tre couteuse) pour bonne am{\'{e}}lioration

Critiques
- Pas de temps de calcul
- Utilisable seulemnt dans un contexte BOVW

Computational load
Offline: {\textgreater} BOVW
Online: a peu pr{\`{e}}s comme BOVW

Scalability
Scale: City
Scalability potential: Quantization issues},
author = {Torii, Akihiko and Sivic, Josef and Okutomi, Masatoshi and Pajdla, Tomas},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/TPAMI.2015.2409868},
file = {:home/nathan/Documents/Mendeley Desktop/Torii et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Torii et al.{\_}2013{\_}Visual Place Recognition with Repetitive Structures.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {01628828},
keywords = {Place recognition,bag of visual words,geometric verification,image retrieval},
number = {11},
pages = {2346--2359},
pmid = {26440272},
title = {{Visual Place Recognition with Repetitive Structures}},
volume = {37},
year = {2013}
}
@article{Wei2017,
abstract = {Recently, convolutional neural network (CNN) visual features have demonstrated their powerful ability as a universal representation for various recognition tasks. In this paper, cross-modal retrieval with CNN visual features is implemented with several classic methods. Specifically, off-the-shelf CNN visual features are extracted from the CNN model, which is pretrained on ImageNet with more than one million images from 1000 object categories, as a generic image representation to tackle cross-modal retrieval. To further enhance the representational ability of CNN visual features, based on the pretrained CNN model on ImageNet, a fine-tuning step is performed by using the open source Caffe CNN library for each target data set. Besides, we propose a deep semantic matching method to address the cross-modal retrieval problem with respect to samples which are annotated with one or multiple labels. Extensive experiments on five popular publicly available data sets well demonstrate the superiority of CNN visual features for cross-modal retrieval.},
author = {Wei, Yunchao and Zhao, Yao and Lu, Canyi and Wei, Shikui and Liu, Luoqi and Zhu, Zhenfeng and Yan, Shuicheng},
doi = {10.1109/TCYB.2016.2519449},
file = {:home/nathan/Documents/Mendeley Desktop/Wei et al/IEEE Transactions on Cybernetics/Wei et al.{\_}2017{\_}Cross-Modal Retrieval With CNN Visual Features A New Baseline.pdf:pdf},
issn = {21682267},
journal = {IEEE Transactions on Cybernetics},
keywords = {Convolutional neural network (CNN) visual features,cross-media,cross-modal,deep learning,multimodal},
number = {2},
pages = {449--460},
pmid = {27046859},
title = {{Cross-Modal Retrieval With CNN Visual Features: A New Baseline}},
volume = {47},
year = {2017}
}
@inproceedings{Lee2016,
annote = {So...
2 CNN special for Facerecognition, one trained on RGB and the other on Depth (with many many data augmentation and prepocessing -{\textgreater} mesh etc...)
Fusion is done by SVM kernel, with a weighting scheme according to normal deviation to dataset training data as confidence term ?},
author = {Lee, Yuancheng and Chen, Jiancong and Tseng, Ching-Wei and Lai, Shang-Hong},
booktitle = {British Machine Vision Conference (BMVC)},
file = {:home/nathan/Documents/Mendeley Desktop/Lee et al/British Machine Vision Conference (BMVC)/Lee et al.{\_}2016{\_}Accurate and robust face recognition from RGB-D images with a deep learning approach.pdf:pdf},
pages = {1--14},
title = {{Accurate and robust face recognition from RGB-D images with a deep learning approach}},
year = {2016}
}
@article{Rusu2009,
abstract = {In our recent work [1], [2], we proposed Point Feature Histograms (PFH) as robust multi-dimensional features which describe the local geometry around a point p for 3D point cloud datasets. In this paper, we modify their mathematical expressions and perform a rigorous analysis on their robustness and complexity for the problem of 3D registration for overlapping point cloud views. More concretely, we present several optimizations that reduce their computation times drastically by either caching previously computed values or by revising their theoretical formulations. The latter results in a new type of local features, called Fast Point Feature Histograms (FPFH), which retain most of the discriminative power of the PFH. Moreover, we propose an algorithm for the online computation of FPFH features for realtime applications. To validate our results we demonstrate their efficiency for 3D registration and propose a new sample consensus based method for bringing two datasets into the convergence basin of a local non-linear optimizer: SAC-IA (SAmple Consensus Initial Alignment).},
author = {Rusu, Radu Bogdan and Blodow, Nico and Beetz, Michael},
doi = {10.1109/ROBOT.2009.5152473},
file = {:home/nathan/Documents/Mendeley Desktop/Rusu, Blodow, Beetz/Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)/Rusu, Blodow, Beetz{\_}2009{\_}Fast Point Feature Histograms (FPFH) for 3D registration.pdf:pdf},
isbn = {978-1-4244-2788-8},
issn = {1050-4729},
journal = {Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)},
pages = {3212--3217},
title = {{Fast Point Feature Histograms (FPFH) for 3D registration}},
year = {2009}
}
@inproceedings{Armagan2017,
author = {Armagan, Anil and Hirzer, Martin and Roth, Peter M. and Lepetit, Vincent},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Armagan et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Armagan et al.{\_}2017{\_}Learning to Align Semantic Segmentation and 2.5D Maps for Geolocalization.pdf:pdf},
title = {{Learning to Align Semantic Segmentation and 2.5D Maps for Geolocalization}},
year = {2017}
}
@inproceedings{Russell2011,
abstract = {This paper addresses the problem of automatically aligning historical architectural paintings with 3D models obtained using multi-view stereo technology from modern photographs. This is a challenging task because of the variations in appearance, geometry, color and texture due to environmental changes over time, the non-photorealistic nature of architectural paintings, and differences in the viewpoints used by the painters and photographers. Our contribution is two-fold: (i) we combine the gist descriptor [23] with the view-synthesis/retrieval of Irschara et al. [14] to obtain a coarse alignment of the painting to the 3D model by view-sensitive retrieval; (ii) we develop an ICP-like viewpoint refinement procedure, where 3D surface orientation discontinuities (folds and creases) and view-dependent occlusion boundaries are rendered from the automatically obtained and noisy 3D model in a view-dependent manner and matched to contours extracted from the paintings. We demonstrate the alignment of XIXth Century architectural watercolors of the Casa di Championnet in Pompeii with a 3D model constructed from modern photographs using the PMVS public-domain multi-view stereo software.},
annote = {From Duplicate 1 (Automatic alignment of paintings and photographs depicting a 3D scene - Russell, Bryan C.; Sivic, Josef; Ponce, Jean; Dessales, H{\'{e}}l{\`{e}}ne)

R{\'{e}}sum{\'{e}}
M{\'{e}}thode permettant d'alligner pr{\'{e}}cis{\'{e}}ment une painture ou un dessin {\`{a}} un mod{\`{e}}le 3D de la sc{\`{e}}ne qu'il repr{\'{e}}sente. Pipeline se d{\'{e}}composant comme suit : obtention d'un modele 3D par PMCS (SfM-like), g{\'{e}}n{\'{e}}ration de vues dans le mod{\`{e}}le, comparaison de la query au travers du GIST descriptor (NN retriveal en minimisant L2), et fine alignement en utilisant les coutours et en les faisant matcher avec un algorithme de type ICP.

Ce que je n'ai pas compris
- la partie sur le "shape contexte descritor"

Ce qui est int{\'{e}}ressant
- M{\'{e}}thod tr{\`{e}}s pr{\'{e}}cise
- Permet de mettre en relation des images qui comportent de forts changements d'apparence
- La synth{\`{e}}se automatique de vues

Critiques
- Taille de la sc{\`{e}}ne ou rechercher l'image requete est tr{\`{e}}s petite
- Pas de mention sur les temps de calculs

A approfondir
- Le descipteur GIST (vs dense SIFT par exemple)
- Synthese de vue
- la m{\'{e}}thode de recallage avec "extraction de contours"

Computational load
Offline: Pas d'info, creation de vue + description par GIST
Online: Pas d'info, ou recup{\'{e}}re les NN en comparant des GIST et on extrait les contours avant de les fiter par ICP-like

Scalability
Scale: Place
Scalability potential: Maybe too computionaly intensive

From Duplicate 2 (Automatic alignment of paintings and photographs depicting a 3D scene - Russell, Bryan C.; Sivic, Josef; Ponce, Jean; Dessales, H{\'{e}}l{\`{e}}ne)

R{\{}{\'{e}}{\}}sum{\{}{\'{e}}{\}}

M{\{}{\'{e}}{\}}thode permettant d'alligner pr{\{}{\'{e}}{\}}cis{\{}{\'{e}}{\}}ment une painture ou un dessin {\{}{\`{a}}{\}} un mod{\{}{\`{e}}{\}}le 3D de la sc{\{}{\`{e}}{\}}ne qu'il repr{\{}{\'{e}}{\}}sente.

Ce que je n'ai pas compris

- la partie sur le "shape contexte descritor"

Ce qui est int{\{}{\'{e}}{\}}ressant

- M{\{}{\'{e}}{\}}thod tr{\{}{\`{e}}{\}}s pr{\{}{\'{e}}{\}}cise
- Permet de mettre en relation des images qui comportent de forts changements d'apparence
- La synth{\{}{\`{e}}{\}}se automatique de vues

Critiques

- Taille de la sc{\{}{\`{e}}{\}}ne ou rechercher l'image requete est tr{\{}{\`{e}}{\}}s petite
- Pas de mention sur les temps de calculs

A approfondir

- Le descipteur GIST (vs dense SIFT par exemple)
- Synthese de vue
- la m{\{}{\'{e}}{\}}thode de recallage avec "extraction de contours"},
author = {Russell, Bryan C. and Sivic, Josef and Ponce, Jean and Dessales, H{\'{e}}l{\`{e}}ne},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision Workshop (ICCVW)},
doi = {10.1109/ICCVW.2011.6130291},
file = {:home/nathan/Documents/Mendeley Desktop/Russell et al/Proceedings of the IEEE International Conference on Computer Vision Workshop (ICCVW)/Russell et al.{\_}2011{\_}Automatic alignment of paintings and photographs depicting a 3D scene.pdf:pdf},
isbn = {9781467300629},
keywords = {Alignement,Challenging matching,GIST,Global features,Long terme place recognition,Multiple visual domaines,Outdoor,RGB request,View Synthesis,Visual place recognition},
mendeley-tags = {Alignement,Challenging matching,GIST,Global features,Long terme place recognition,Multiple visual domaines,Outdoor,RGB request,View Synthesis,Visual place recognition},
title = {{Automatic alignment of paintings and photographs depicting a 3D scene}},
year = {2011}
}
@inproceedings{Author2017,
archivePrefix = {arXiv},
arxivId = {1708.02898},
author = {Douze, Matthijs and J{\'{e}}gou, Herv{\'{e}} and Johnson, Jeff},
booktitle = {Proceedings of the ACM International Conference on Multimedia Workshop (MMW)},
eprint = {1708.02898},
file = {:home/nathan/Documents/Mendeley Desktop/Douze, J{\'{e}}gou, Johnson/Proceedings of the ACM International Conference on Multimedia Workshop (MMW)/Douze, J{\'{e}}gou, Johnson{\_}2017{\_}An evaluation of large-scale methods for image instance and class discovery.pdf:pdf},
title = {{An evaluation of large-scale methods for image instance and class discovery}},
year = {2017}
}
@article{Ball2017,
abstract = {In recent years, deep learning (DL), a re-branding of neural networks (NNs), has risen to the top in numerous areas, namely computer vision (CV), speech recognition, natural language processing, etc. Whereas remote sensing (RS) possesses a number of unique challenges, primarily related to sensors and applications, inevitably RS draws from many of the same theories as CV; e.g., statistics, fusion, and machine learning, to name a few. This means that the RS community should be aware of, if not at the leading edge of, of advancements like DL. Herein, we provide the most comprehensive survey of state-of-the-art RS DL research. We also review recent new developments in the DL field that can be used in DL for RS. Namely, we focus on theories, tools and challenges for the RS community. Specifically, we focus on unsolved challenges and opportunities as it relates to (i) inadequate data sets, (ii) human-understandable solutions for modelling physical phenomena, (iii) Big Data, (iv) non-traditional heterogeneous data sources, (v) DL architectures and learning algorithms for spectral, spatial and temporal data, (vi) transfer learning, (vii) an improved theoretical understanding of DL systems, (viii) high barriers to entry, and (ix) training and optimizing the DL.},
archivePrefix = {arXiv},
arxivId = {1709.00308},
author = {Ball, John E. and Anderson, Derek T. and Chan, Chee Seng},
doi = {10.1117/1.JRS.11.042609},
eprint = {1709.00308},
file = {:home/nathan/Documents/Mendeley Desktop/Ball, Anderson, Chan/Unknown/Ball, Anderson, Chan{\_}2017{\_}A Comprehensive Survey of Deep Learning in Remote Sensing Theories, Tools and Challenges for the Community.pdf:pdf},
keywords = {ball,big data,computer vision,deep learning,ece,edu,hyperspectral,jeball,john e,msstate,multispectral,remote sensing},
number = {vii},
pages = {1--64},
title = {{A Comprehensive Survey of Deep Learning in Remote Sensing: Theories, Tools and Challenges for the Community}},
url = {http://arxiv.org/abs/1709.00308},
year = {2017}
}
@inproceedings{Pani2015Robust,
author = {Paudel, Danda Pani and Habed, Adlane and Demonceaux, C{\'{e}}dric and Vasseur, Pascal},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
pages = {2048--2056},
title = {{Robust and Optimal Sum-of-Squares-Based Point-to-Plane Registration of Image Sets and Structured Scenes}},
year = {2015}
}
@article{Walch2016,
author = {Walch, Florian},
file = {:home/nathan/Documents/Mendeley Desktop/Walch/arXiv preprint/Walch{\_}2016{\_}Deep Learning for Image-Based Localization.pdf:pdf},
journal = {arXiv preprint},
title = {{Deep Learning for Image-Based Localization}},
year = {2016}
}
@inproceedings{Matei2013,
abstract = {We present a novel method for matching ground-based query images to a georeferenced LIDAR 3D dataset acquired from an airborne platform in urban environments. We are addressing two main technical challenges: (i) different modalities between the query and the reference data (electro-optical vs. LIDAR) that impose unique challenges to the matching problem; (ii) very different viewing directions from which the query, respectively the LIDAR data were acquired. We make two main technical contributions in this paper. First, we present a method for automatically extracting features from LIDAR data that largely remain invariant to the projection in a 2D image and thus allow robust matching across modalities and change in viewpoint. Second, we describe a matching technique that finds the best 3D pose that relates the query input image to a rendered image of the 3D models. We present results of matching images to high-resolution LIDAR data covering five square kilometers over a city that demonstrate the power of the matching method proposed.},
author = {Matei, Bogdan C. and {Vander Valk}, Nick and Zhu, Zhiwei and Cheng, Hui and Sawhney, Harpreet S.},
booktitle = {Proceedings of IEEE Workshop on Applications of Computer Vision (WACV)},
doi = {10.1109/WACV.2013.6475048},
file = {:home/nathan/Documents/Mendeley Desktop/Matei et al/Proceedings of IEEE Workshop on Applications of Computer Vision (WACV)/Matei et al.{\_}2013{\_}Image to LIDAR matching for geotagging in urban environments.pdf:pdf},
isbn = {9781467350532},
issn = {21583978},
pages = {413--420},
title = {{Image to LIDAR matching for geotagging in urban environments}},
year = {2013}
}
@inproceedings{Kim2015,
abstract = {We address the problem of recognizing a place depicted in a query image by using a large database of geo-tagged images at a city-scale. In particular, we discover features that are useful for recognizing a place in a data-driven man- ner, and use this knowledge to predict useful features in a query image prior to the geo-localization process. This al- lows us to achieve better performance while reducing the number of features. Also, for both learning to predict fea- tures and retrieving geo-tagged images from the database, we propose per-bundle vector of locally aggregated de- scriptors (PBVLAD), where each maximally stable region is described by a vector of locally aggregated descriptors (VLAD) on multiple scale-invariant features detected within the region. Experimental results show the proposed ap- proach achieves a significant improvement over other base- line methods.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de localisation bas{\'{e}} vision orient{\'{e}}e image-retrieval avec apprentissage de features discriminantes. Les auteurs utilisent comme descripteurs MSER + SIFT (bundle descriptor) concat{\'{e}} dans avec une repr{\'{e}}sentation VLAD (mini dico de 128 mot) avec r{\'{e}}duction de dimension par PCA (128-{\textgreater}16 components). Ils entrainent des SVM permettant de discriminer ces descriteurs pour la tache de IBL. Pour cr{\'{e}}er les bases d'apprentissages pour les SVM les auteurs utilisent une m{\'{e}}thode non-supervi{\'{e}} (comme pour les CNN) en associant {\`{a}} des images de tests (collect{\'{e}} sur flikr) des ppv (en s'aidant des g{\'{e}}o-tags) et en d{\'{e}}finissant des trues matches et des false matches en comparant la distance des descripteurs. Ils mettent en place ensuite une phase de clustering pour entrainer plusieurs SVM (avec hard negative mining). Au moment du match, ils extraient leur features, prennent seulment les plus discriminantes gr{\^{a}}ce {\`{a}} leur SVM puis calcul l'image qui a le plus gros score de matching.

Ce que je n'ai pas compris
- Pourquoi ils comparent les descipteurs par produit scalaire

Ce qui est int{\'{e}}ressant
- L'apprentissage pour la tache d'image base-localization
- Les r{\'{e}}sultats
- Papier bien {\'{e}}crit, {\'{e}}tat de l'art intessant donnant des bons examples d'applications

Critiques
- Pas de temps de calcul, et surement tr{\`{e}}s long vu le nombre de descripteurs extraits

Computational load
Offline: - (surement tr{\`{e}}s long)
Online: -

Scalability
Scale: City
Scalability potential:},
author = {Kim, Hyo Jin and Dunn, Enrique and Frahm, Jan-Michael},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.139},
file = {:home/nathan/Documents/Mendeley Desktop/Kim, Dunn, Frahm/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Kim, Dunn, Frahm{\_}2015{\_}Predicting good features for image geo-localization using per-bundle VLAD.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
pages = {1170--1178},
title = {{Predicting good features for image geo-localization using per-bundle VLAD}},
volume = {11-18-Dece},
year = {2015}
}
@phdthesis{Azzi2015,
author = {Azzi, Charbel},
file = {:home/nathan/Documents/Mendeley Desktop/Azzi/Unknown/Azzi{\_}2015{\_}Efficient Image-Based Localization Using Context.pdf:pdf},
title = {{Efficient Image-Based Localization Using Context}},
year = {2015}
}
@inproceedings{Snavely2006,
author = {Snavely, Noah and Seitz, Steven M and Szeliski, Richard},
booktitle = {ACM Transactions on Graphics (TOG)},
number = {3},
organization = {ACM},
pages = {835--846},
title = {{Photo tourism: exploring photo collections in 3D}},
volume = {25},
year = {2006}
}
@article{Jegou2011,
abstract = {This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {J{\'{e}}gou, Herv{\'{e}} and Douze, Matthijs and Schmid, Cordelia},
doi = {10.1109/TPAMI.2010.57},
eprint = {arXiv:1011.1669v3},
file = {:home/nathan/Documents/Mendeley Desktop/J{\'{e}}gou, Douze, Schmid/IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)/J{\'{e}}gou, Douze, Schmid{\_}2011{\_}Product Quantization for Nearest Neighbor Search Herve.pdf:pdf},
isbn = {9781450300551},
issn = {1939-3539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Cluster Analysis,Computer-Assisted,Computer-Assisted: methods,Image Interpretation,Image Processing,Information Storage and Retrieval,Models,Pattern Recognition,Statistical},
number = {1},
pages = {117--128},
pmid = {21088323},
title = {{Product Quantization for Nearest Neighbor Search Herve}},
url = {http://eprints.pascal-network.org/archive/00008315/},
volume = {33},
year = {2011}
}
@inproceedings{Iscen2017,
abstract = {Location recognition is commonly treated as visual instance re-trieval on " street view " imagery. dataset items and queries are panoramic views, i.e. groups of images taken at a single location. work introduces a novel panorama-to-panorama matching process, either by aggregating features of individual images in a group or by explicitly constructing a larger panorama. In either case, multiple views are used as queries. We reach near perfect location recognition on a standard benchmark with only four query views.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de VBL qui part du principe que l'on a en entr{\'{e}} du syst{\`{e}}me un panorma et des panoramas dans la base de donn{\'{e}}es. Comparaison de diff{\'{e}}rentes m{\'{e}}thodes de mise en correspondance de panoramas (toujours avec le descripteur NetVLAD). La premi{\`{e}}re m{\'{e}}thode consiste {\`{a}} extraire des features sur diff{\'{e}}rentes images associ{\'{e}}es {\`{a}} un m{\^{e}}me panorama puis dans un second temp {\`{a}} les agr{\'{e}}ger entre elles avec un memory vector. La seconde consiste d'extraire le descripteur directement {\`{a}} partir du panorma. La seconde m{\'{e}}thode semble meilleur. Les auteurs partes ensuite du principes qu'ils n'ont pas en entr{\'{e}} le panorma complet, mais seulement n images du panorama, et ils montrent qu'avec peu d'images on a des bons r{\'{e}}sultats.

Ce que je n'ai pas compris
- La construction exacte du memory vector

Ce qui est int{\'{e}}ressant
- Les r{\'{e}}sultats tr{\`{e}}s bon
- Compression du dataset (une features par panorama)
- La comparaison des diff{\'{e}}rentes m{\'{e}}thodes, im/im, pan/im, etc.
- Leur m{\'{e}}thodes d'aggregation marche seulement si on a deux vecteurs, apr{\`{e}}s c'est moins bien que direct le CNN

Critiques
- Pas de r{\'{e}}elle nouveaut{\'{e}} et assez lourd sur la contraite d'entr{\'{e}}
- On aurait aim{\'{e}} voir les r{\'{e}}sultats si les images du pano {\'{e}}t{\'{e}} cons{\'{e}}cutives

Computational load
Offline: overhead au moment de l'aggregation des panos
Online: pareil

Scalability
Scale: Diminution de la taille de la database ++},
author = {Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Furon, Teddy and Chum, Ondřej},
booktitle = {Proceedings of the ACM International Conference on Multimedia Retrieval (ICMR)},
doi = {10.1145/3078971.3079033},
file = {:home/nathan/Documents/Mendeley Desktop/Iscen et al/Proceedings of the ACM International Conference on Multimedia Retrieval (ICMR)/Iscen et al.{\_}2017{\_}Panorama to panorama matching for location recognition.pdf:pdf},
isbn = {9781450347013},
keywords = {KEYWORDS image retrieval,location recognition},
title = {{Panorama to panorama matching for location recognition}},
year = {2017}
}
@article{Lindsten2010,
abstract = {A UAV navigation system relying on GPS is vulnerable to signal failure, making a drift free backup system necessary. We introduce a vision based geo-referencing system that uses pre-existing maps to reduce the long term drift. The system classifies an image according to its environmental content and thereafter matches it to an environmentally classified map over the operational area. This map matching provides a measurement of the absolute location of the UAV, that can easily be incorporated into a sensor fusion framework. Experiments show that the geo-referencing system reduces the long term drift in UAV navigation, enhancing the ability of the UAV to navigate accurately over large areas without the use of GPS.},
author = {Lindsten, Fredrik and Callmer, Jonas and Ohlsson, Henrik and T{\"{o}}rnqvist, David and Sch{\"{o}}n, Thomas B. and Gustafsson, Fredrik},
doi = {10.1109/ROBOT.2010.5509424},
file = {:home/nathan/Documents/Mendeley Desktop/Lindsten et al/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Lindsten et al.{\_}2010{\_}Geo-referencing for UAV navigation using environmental classification.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
keywords = {Geo-referencing,UAV,classification,david t,fredrik lindsten,geo-referencing for uav navigation,henrik ohlsson,jonas callmer,localization,using environmental classification},
pages = {1420--1425},
title = {{Geo-referencing for UAV navigation using environmental classification}},
year = {2010}
}
@article{Li2017b,
author = {Li, Wen and Chen, Lin and Xu, Dong and {Van Gool}, Luc},
doi = {10.1109/TPAMI.2017.2734890},
file = {:home/nathan/Documents/Mendeley Desktop/Li et al/IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)/Li et al.{\_}2017{\_}Visual Recognition in RGB Images and Videos by Learning from RGB-D Data.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
number = {c},
pages = {1--1},
title = {{Visual Recognition in RGB Images and Videos by Learning from RGB-D Data}},
url = {http://ieeexplore.ieee.org/document/8000401/},
volume = {8828},
year = {2017}
}
@article{Gupta2016,
abstract = {{\textless}p{\textgreater}The primary method for geo-localization is based on GPS which has issues of localization accuracy, power consumption, and unavailability. This paper proposes a novel approach to geo-localization in a GPS-denied environment for a mobile platform. Our approach has two principal components: public domain transport network data available in GIS databases or OpenStreetMap; and a trajectory of a mobile platform. This trajectory is estimated using visual odometry and 3D view geometry. The transport map information is abstracted as a graph data structure, where various types of roads are modelled as graph edges and typically intersections are modelled as graph nodes. A search for the trajectory in real time in the graph yields the geo-location of the mobile platform. Our approach uses a simple visual sensor and it has a low memory and computational footprint. In this paper, we demonstrate our method for trajectory estimation and provide examples of geolocalization using public-domain map data. With the rapid proliferation of visual sensors as part of automated driving technology and continuous growth in public domain map data, our approach has the potential to completely augment, or even supplant, GPS based navigation since it functions in all environments.{\textless}/p{\textgreater}},
author = {Gupta, Ashish and Chang, Huan and Yilmaz, Alper},
doi = {10.5194/isprsannals-III-3-263-2016},
file = {:home/nathan/Documents/Mendeley Desktop/Gupta, Chang, Yilmaz/ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences/Gupta, Chang, Yilmaz{\_}2016{\_}Gps-Denied Geo-Localisation Using Visual Odometry.pdf:pdf},
issn = {2194-9050},
journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {3d view geometry,computer vision,geo-localisation,gis,gps-denied,openstreetmap},
number = {July},
pages = {263--270},
title = {{Gps-Denied Geo-Localisation Using Visual Odometry}},
url = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/III-3/263/2016/isprs-annals-III-3-263-2016.pdf},
volume = {III-3},
year = {2016}
}
@inproceedings{Zamir2010,
abstract = {Abstract. Finding an image's exact GPS location is a challenging com- puter vision problem that has many real-world applications. In this pa- per, we address the problem of nding the GPS location of images with an accuracy which is comparable to hand-held GPS devices.We leverage a structured data set of about 100,000 images build from Google Maps Street View as the reference images. We propose a localization method in which the SIFT descriptors of the detected SIFT interest points in the reference images are indexed using a tree. In order to localize a query im- age, the tree is queried using the detected SIFT descriptors in the query image. A novel GPS-tag-based pruning method removes the less reliable descriptors. Then, a smoothing step with an associated voting scheme is utilized; this allows each query descriptor to vote for the location its nearest neighbor belongs to, in order to accurately localize the query image. A parameter called Con dence of Localization which is based on the Kurtosis of the distribution of votes is de ned to determine how re- liable the localization of a particular image is. In addition, we propose a novel approach to localize groups of images accurately in a hierarchical manner. First, each image is localized individually; then, the rest of the images in the group are matched against images in the neighboring area of the found rst match. The nal location is determined based on the Con dence of Localization parameter. The proposed image group local- ization method can deal with very unclear queries which are not capable of being geolocated individually.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de localisation d'image {\`{a}} partir d'une base de donn{\'{e}}es (google street view). Les principaux apports sont l'utilisation d'un vocabulary tree pour faire le matching des descripteurs SIFT, d'une m{\'{e}}thode de filtrage bas{\'{e}} sur l'information de la localisation des plus proches voisins et une m{\'{e}}thode de lissage pour affiner les r{\'{e}}sultats. Les auteurs ont aussi mod{\'{e}}liser la fonction de comparaison par une distribution de Kurtosis afin d'obtenir un crit{\`{e}}re qualitatif du matching (pour faire du rejet, de la classification au de la localisation de groupes d'images g{\'{e}}olocalis{\'{e}}s dans un m{\^{e}}me p{\'{e}}rim{\`{e}}tre). Ils appliquent {\'{e}}galement leurs avanc{\'{e}}s pour faire de la g{\'{e}}olocalisation de groupe d'images.

Ce que je n'ai pas compris
- Comment marche le matching SIFT par vocabulary tree

Ce qui est int{\'{e}}ressant
- Les m{\'{e}}thodes de filtrages des r{\'{e}}sultats de la recherche des NN : {\'{e}}lagage et affinage (un peu comme dans Sattler et al. 2016)
- L'{\'{e}}lagage se basant sur les tags de g{\'{e}}olocalisation des images
- L'indicateur de bon "matching"

Critiques
- R{\'{e}}sultats pas exceptionnels (papier un peu vieux)
- Localisation de groupes d'images pas tr{\`{e}}s r{\'{e}}utilisable dans le cadre de notre probl{\'{e}}matique
- Pas de temps de calcul 

A approfondir
- Kurtosis
- NN Tree

Computational load
Offline: Spawing multiple Vocabulary tree
Online: Retriveal in multiple vocabulary + check consistency of the candidates

Scalability
Scale: City
Scalability potential: Don't trust},
author = {Zamir, Amir Roshan and Shah, Mubarak},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-642-15561-1_19},
file = {:home/nathan/Documents/Mendeley Desktop/Zamir, Shah/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Zamir, Shah{\_}2010{\_}Accurate image localization based on google maps street view.pdf:pdf},
isbn = {364215560X},
issn = {03029743},
keywords = {Local features,Outdoor,RGB request,SIFT,Visual place recognition},
mendeley-tags = {Local features,Outdoor,RGB request,SIFT,Visual place recognition},
number = {PART 4},
pages = {255--268},
title = {{Accurate image localization based on google maps street view}},
volume = {6314 LNCS},
year = {2010}
}
@inproceedings{Seddati2017,
author = {Seddati, Omar and Dupont, St{\'{e}}phane and Mahmoudi, Sa{\"{i}}d and Parian, Mahnaz},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision Workshop (ICCVW)},
file = {:home/nathan/Documents/Mendeley Desktop/Seddati et al/Proceedings of the IEEE International Conference on Computer Vision Workshop (ICCVW)/Seddati et al.{\_}2017{\_}Towards Good Practices for Image Retrieval Based on CNN Features.pdf:pdf},
pages = {1246--1255},
title = {{Towards Good Practices for Image Retrieval Based on CNN Features}},
year = {2017}
}
@article{Mishchuk2017,
abstract = {We introduce a novel loss for learning local feature descriptors that is inspired by the SIFT matching scheme. We show that the proposed loss that relies on the maximization of the distance between the closest positive and closest negative patches can replace more complex regularization methods which have been used in local descriptor learning; it works well for both shallow and deep convolution network architectures. The resulting descriptor is compact -- it has the same dimensionality as SIFT (128), it shows state-of-art performance on matching, patch verification and retrieval benchmarks and it is fast to compute on a GPU.},
archivePrefix = {arXiv},
arxivId = {1705.10872},
author = {Mishchuk, Anastasiya and Mishkin, Dmytro and Radenovi{\'{c}}, Filip and Matas, Jiri},
eprint = {1705.10872},
file = {:home/nathan/Documents/Mendeley Desktop/Mishchuk et al/Unknown/Mishchuk et al.{\_}2017{\_}Working hard to know your neighbor's margins Local descriptor learning loss.pdf:pdf},
pages = {1--11},
title = {{Working hard to know your neighbor's margins: Local descriptor learning loss}},
url = {http://arxiv.org/abs/1705.10872},
year = {2017}
}
@article{Arroyo2017,
author = {Arroyo, Roberto and Alcantarilla, Pablo F. and Bergasa, Luis M. and Romera, Eduardo},
doi = {10.1007/s10514-017-9664-7},
file = {:home/nathan/Documents/Mendeley Desktop/Arroyo et al/Autonomous Robots/Arroyo et al.{\_}2017{\_}Are you ABLE to perform a life-long visual topological localization.pdf:pdf},
issn = {0929-5593},
journal = {Autonomous Robots},
keywords = {Binary descriptors,Image matching,Localization across seasons,Loop closure detection,Visual place recognition,image matching,localization across seasons,loop closure detection,recognition,visual place},
publisher = {Springer US},
title = {{Are you ABLE to perform a life-long visual topological localization?}},
url = {http://link.springer.com/10.1007/s10514-017-9664-7},
year = {2017}
}
@mastersthesis{Walch2016mastersThesis,
author = {Walch, Florian},
school = {Technical University of Munich},
title = {{Deep Learning for Image-Based Localization}},
year = {2016}
}
@article{Ceylan2014,
author = {Ceylan, Duygu and Mitra, Niloy J. and Zheng, Youyi and Pauly, Mark},
journal = {ACM Transactions on Graphics (TOG)},
number = {1},
pages = {2},
publisher = {ACM},
title = {{Coupled structure-from-motion and 3D symmetry detection for urban facades}},
volume = {33},
year = {2014}
}
@inproceedings{Jackson2017,
abstract = {3D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty. Current systems often assume the availability of multiple facial images (sometimes from the same subject) as input, and must address a number of methodological challenges such as establishing dense correspondences across large facial poses, expressions, and non-uniform illumination. In general these methods require complex and inefficient pipelines for model building and fitting. In this work, we propose to address many of these limitations by training a Convolutional Neural Network (CNN) on an appropriate dataset consisting of 2D images and 3D facial models or scans. Our CNN works with just a single 2D facial image, does not require accurate alignment nor establishes dense correspondence between images, works for arbitrary facial poses and expressions, and can be used to reconstruct the whole 3D facial geometry (including the non-visible parts of the face) bypassing the construction (during training) and fitting (during testing) of a 3D Morphable Model. We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image. We also demonstrate how the related task of facial landmark localization can be incorporated into the proposed framework and help improve reconstruction quality, especially for the cases of large poses and facial expressions. Testing code will be made available online, along with pre-trained models http://aaronsplace.co.uk/papers/jackson2017recon},
archivePrefix = {arXiv},
arxivId = {1703.07834},
author = {Jackson, Aaron S. and Bulat, Adrian and Argyriou, Vasileios and Tzimiropoulos, Georgios},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
eprint = {1703.07834},
file = {:home/nathan/Documents/Mendeley Desktop/Jackson et al/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Jackson et al.{\_}2017{\_}Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression.pdf:pdf},
title = {{Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression}},
url = {http://arxiv.org/abs/1703.07834},
year = {2017}
}
@article{Marchand2016,
author = {Marchand, Eric and Uchiyama, Hideaki and Spindler, Fabien},
doi = {10.1109/TVCG.2015.2513408},
file = {:home/nathan/Documents/Mendeley Desktop/Marchand, Uchiyama, Spindler/IEEE Transactions on Visualization and Computer Graphics (ToVCG)/Marchand, Uchiyama, Spindler{\_}2016{\_}Pose Estimation for Augmented Reality a Hands-On Survey.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics (ToVCG)},
number = {12},
pages = {2633--2651},
title = {{Pose Estimation for Augmented Reality : a Hands-On Survey}},
volume = {22},
year = {2016}
}
@article{Kendall2017a,
abstract = {Numerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.},
archivePrefix = {arXiv},
arxivId = {1705.07115},
author = {Kendall, Alex and Gal, Yarin and Cipolla, Roberto},
eprint = {1705.07115},
file = {:home/nathan/Documents/Mendeley Desktop/Kendall, Gal, Cipolla/Unknown/Kendall, Gal, Cipolla{\_}2017{\_}Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics.pdf:pdf},
title = {{Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics}},
url = {http://arxiv.org/abs/1705.07115},
year = {2017}
}
@article{Zamir2014,
abstract = {In this paper, we present a new framework for geo-locating an image utilizing a novel multiple nearest neighbor feature matching method using Generalized Minimum Clique Graphs (GMCP). First, we extract local features (e.g., SIFT) from the query image and retrieve a number of nearest neighbors for each query feature from the reference data set. Next, we apply our GMCP-based feature matching to select a single nearest neighbor for each query feature such that all matches are globally consistent. Our approach to feature matching is based on the proposition that the first nearest neighbors are not necessarily the best choices for finding correspondences in image matching. Therefore, the proposed method considers multiple reference nearest neighbors as potential matches and selects the correct ones by enforcing consistency among their global features (e.g., GIST) using GMCP. In this context, we argue that using a robust distance function for finding the similarity between the global features is essential for the cases where the query matches multiple reference images with dissimilar global features. Towards this end, we propose a robust distance function based on the Gaussian Radial Basis Function (G-RBF). We evaluated the proposed framework on a new data set of 102k street view images; the experiments show it outperforms the state of the art by 10 percent.},
annote = {R{\'{e}}sum{\'{e}}
Suite des travaux de Zamir et al. 2010, m{\'{e}}thode de g{\'{e}}olocalisation d'image gr{\^{a}}ce {\`{a}} un k-mean tree construit {\`{a}} partir de descripteur locaux (SIFT). L'am{\'{e}}lioration majeur est introduit en ne choissisant pas le best NN lors de la phase de retrivial mais en combiant des descipteur globaux (g{\'{e}}o-tag) pour choisir parmis les k meilleurs NNs. Formalisation de la solution en utilisant les Generalized Minimum Clicque Graph (GMCP). 

Ce que je n'ai pas compris
- L'algo pour r{\'{e}}soudre le probl{\`{e}}me de minimisation de cout du GMCP

Ce qui est int{\'{e}}ressant
- Utilisation {\`{a}} la fois de descripteurs locaux {\&} globaux
- Utilisation de l'info de g{\'{e}}olocalisation pour trouver les meilleurs NN.
- Temps de calcul proche du temps r{\'{e}}el

Critiques
- R{\'{e}}sultats pas tr{\`{e}}s bon (c'est peut {\^{e}}tre du au dataset et {\`{a}} la m{\'{e}}trique utilis{\'{e}}e)

A approfondir
- visual phrase (en intro)
- suite des travaux s'il y a

Computational load
Offline: Vocabulary tree creation
Online: Recherche des NN + graph matching ({\textless}1s) 

Scalability
Scale: City
Scalability potential: Why not},
author = {Zamir, Amir Roshan and Shah, Mubarak},
doi = {10.1109/TPAMI.2014.2299799},
file = {:home/nathan/Documents/Mendeley Desktop/Zamir, Shah/IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)/Zamir, Shah{\_}2014{\_}Image geo-localization based on multiplenearest neighbor feature matching using generalized graphs.pdf:pdf},
isbn = {0162-8828 VO - 36},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
keywords = {Generalized Minimum Clique Problem (GMCP),Geo-location,Global features,Local features,Outdoor,SIFT,Visual place recognition,Visual words,feature correspondence,feature matching,generalized graphs,generalized minimum spanning tree (GMST),image localization,multiple nearest neighbor feature matching},
mendeley-tags = {Global features,Local features,Outdoor,SIFT,Visual place recognition,Visual words},
number = {8},
pages = {1546--1558},
title = {{Image geo-localization based on multiplenearest neighbor feature matching using generalized graphs}},
volume = {36},
year = {2014}
}
@inproceedings{Sudholt2017,
author = {Sudholt, Sebastian and Rothacker, Leonard and Fink, Gernot A.},
booktitle = {Proceedings of the International Conference on Document Analysis and Recognition (ICDAR)},
file = {:home/nathan/Documents/Mendeley Desktop/Sudholt, Rothacker, Fink/Proceedings of the International Conference on Document Analysis and Recognition (ICDAR)/Sudholt, Rothacker, Fink{\_}2017{\_}Query-by-Online Word Spotting Revisited Using CNNs for Cross-Domain Retrieval.pdf:pdf},
title = {{Query-by-Online Word Spotting Revisited: Using CNNs for Cross-Domain Retrieval}},
year = {2017}
}
@inproceedings{Li2012,
abstract = {We address the problem of determining where a photo was taken by estimating a full 6-DOF-plus-intrincs camera pose with respect to a large geo-registered 3D point cloud, bringing together research on image localization, landmark recognition, and 3D pose estimation. Our method scales to datasets with hundreds of thousands of images and tens of millions of 3D points through the use of two new techniques: a co-occurrence prior for RANSAC and bidirectional matching of image features with 3D points. We evaluate our method on several large data sets, and show state-of-the-art results on landmark recognition as well as the ability to locate cameras to within meters, requiring only seconds per query.},
annote = {R{\'{e}}sum{\'{e}}
Estimation des 6 DOF d'une image {\`{a}} partir d'une base de donn{\'{e}}es de nuages de points 3D g{\'{e}}olocalis{\'{e}}s (cr{\'{e}}{\'{e}}s par SfM). L'am{\'{e}}lioration se situe essentiellement sur la partie ranking des inliers en introduisant deux m{\'{e}}thodes : co-occurence prior for RANSAC qui consiste {\`{a}} donner plus d'importance aux potentiels inliers qui apparaissent souvent ensemble dans une image, et le bidirectional matching introduit par Li et al. 2010. Test de la m{\'{e}}thode sur des worldwide datasets avec beaucoup de points.
Utilisation des algos 3Pts ou 4Pts + bundle adjustment pour retrouver la pose absolue (pr{\'{e}}cision au m{\`{e}}tre).

Ce que je n'ai pas compris
- Comment on donne concretement des poids aux inliers qui apparaissent souvent ensemble

Ce qui est int{\'{e}}ressant
- La m{\'{e}}thode en escalier qui consiste {\`{a}} utiliser le bidirectional matching seulement si le nombre d'inliers est trop faible
- Le RANSAC am{\'{e}}lior{\'{e}}

Critiques
- On a besoin de plusieurs desctipteurs par points 
3D (SfM)
- Comparaison avec {\'{e}}tat de l'art pas top
- Pas de vrai quantification de la pr{\'{e}}cision
- Pas d'info sur le bundel adjustment utilis{\'{e}} pour le 
pose refinement
- Resultats du retrieval pas forcement tres pertinent vu que les images ont {\'{e}}t{\'{e}} sectionn{\'{e}}es pour faire la SfM

A approfondir
- Les RANSAC am{\'{e}}lior{\'{e}} guided-MLESAC, PROSAC, GroupSAC...

Computational load
Offline: ANN database creation
Online: Few seconds by query

Scalability
Scale: District
Scalability potential: Performance decreasing},
author = {Li, Yunpeng and Snavely, Noah and Huttenlocher, Daniel P. and Fua, Pascal},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-642-33718-5_2},
file = {:home/nathan/Documents/Mendeley Desktop/Li et al/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Li et al.{\_}2012{\_}Worldwide Pose Estimation Using 3D Point Clouds.pdf:pdf},
isbn = {978-3-642-33717-8},
issn = {03029743},
pages = {15--29},
title = {{Worldwide Pose Estimation Using 3D Point Clouds}},
url = {http://link.springer.com/10.1007/978-3-642-33718-5{\%}7B{\_}{\%}7D2 http://link.springer.com/10.1007/978-3-642-33718-5{\_}2},
year = {2012}
}
@article{Passalis2017,
abstract = {In this paper, a neural learning architecture for the well-known Bag-of-Features (BoF) model, called Neural Bag-of-Features, is proposed. The Neural BoF model is formulated in two neural layers: a Radial Basis Function (RBF) layer and an accumulation layer. The ability of the Neural BoF model to improve the classification performance is demonstrated using four datasets, including a large-scale dataset, and five different feature types. The gains are two-fold: the classification accuracy increases and, at the same time, smaller networks can be used, reducing the required training and testing time. Furthermore, the Neural BoF natively supports training and classifying from feature streams. This allows the proposed method to efficiently scale to large datasets. The streaming process can also be used to introduce noise and reduce the over-fitting of the network. Finally, the Neural BoF provides a framework that can model and extend the dictionary learning methodology.},
author = {Passalis, Nikolaos and Tefas, Anastasios},
doi = {10.1016/j.patcog.2016.11.014},
file = {:home/nathan/Documents/Mendeley Desktop/Passalis, Tefas/Pattern Recognition/Passalis, Tefas{\_}2017{\_}Neural Bag-of-Features learning.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Bag-of-Features,Dictionary learning,RBF neural networks},
number = {August 2016},
pages = {277--294},
publisher = {Elsevier},
title = {{Neural Bag-of-Features learning}},
url = {http://dx.doi.org/10.1016/j.patcog.2016.11.014},
volume = {64},
year = {2017}
}
@inproceedings{Pani2015Lmi,
author = {Paudel, Danda Pani and Habed, Adlane and Demonceaux, C{\'{e}}dric and Vasseur, Pascal},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {4494--4502},
title = {{LMI-based 2D-3D registration: From uncalibrated images to Euclidean scene}},
year = {2015}
}
@article{Kim2013,
abstract = {We present a novel method for a feature descriptor called an exact order based descriptor (EOD). The proposed method consists of three steps. First, to resolve ordering ambiguity for pixels of the same intensity, an exact order image is created by changing the discrete intensity into a k-dimensional continuous value. Second, exact order based features are generated globally and locally. Finally, the EOD is constructed by combining the global and local exact order features using the discrete cosine transform. Experimental results show that the proposed method outperforms other state-of-the-art descriptors over a number of images. {\textcopyright} 2013 Elsevier Ltd.},
author = {Kim, Bongjoe and Yoo, Hunjae and Sohn, Kwanghoon},
doi = {10.1016/j.patcog.2013.04.015},
file = {:home/nathan/Documents/Mendeley Desktop/Kim, Yoo, Sohn/Pattern Recognition/Kim, Yoo, Sohn{\_}2013{\_}Exact order based feature descriptor for illumination robust image matching.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Feature descriptor,Global order descriptor,Local order descriptor},
number = {12},
pages = {3268--3278},
publisher = {Elsevier},
title = {{Exact order based feature descriptor for illumination robust image matching}},
url = {http://dx.doi.org/10.1016/j.patcog.2013.04.015},
volume = {46},
year = {2013}
}
@inproceedings{Sun2017,
author = {Sun, Xun and Xie, Yuanfan and Luo, Pei and Wang, Liang},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Sun et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Sun et al.{\_}2017{\_}A Dataset for Benchmarking Image-based Localization.pdf:pdf},
title = {{A Dataset for Benchmarking Image-based Localization}},
year = {2017}
}
@inproceedings{Lenc2016,
abstract = {Local covariant feature detection, namely the problem of extracting viewpoint invariant features from images, has so far largely resisted the application of machine learning techniques. In this paper, we propose the first fully general formulation for learning local covariant feature detectors. We propose to cast detection as a regression problem, enabling the use of powerful regressors such as deep neural networks. We then derive a covariance constraint that can be used to automatically learn which visual structures provide stable anchors for local feature detection. We support these ideas theoretically, proposing a novel analysis of local features in term of geometric transformations, and we show that all common and many uncommon detectors can be derived in this framework. Finally, we present empirical results on a variety of detector types and on standard feature benchmarks, showing the power and flexibility of the framework.},
archivePrefix = {arXiv},
arxivId = {1605.01224},
author = {Lenc, Karel and Vedaldi, Andrea},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision Workshop (ECCVW)},
eprint = {1605.01224},
file = {:home/nathan/Documents/Mendeley Desktop/Lenc, Vedaldi/Proceedings of the IEEE European Conference on Computer Vision Workshop (ECCVW)/Lenc, Vedaldi{\_}2016{\_}Learning Covariant Feature Detectors.pdf:pdf},
title = {{Learning Covariant Feature Detectors}},
url = {http://arxiv.org/abs/1605.01224},
year = {2016}
}
@inproceedings{Yi2016a,
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Yi, Kwang Moo and Trulls, Eduard and Lepetit, Vincent and Fua, Pascal},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-319-46448-0},
eprint = {1311.2901},
file = {:home/nathan/Documents/Mendeley Desktop/Yi et al/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Yi et al.{\_}2016{\_}LIFT Learned Invariant Feature Transform.pdf:pdf},
isbn = {978-3-319-46447-3},
issn = {0302-9743},
keywords = {deep learning,feature descriptors,local features},
pages = {467--483},
pmid = {4520227},
title = {{LIFT: Learned Invariant Feature Transform}},
url = {http://link.springer.com/10.1007/978-3-319-46448-0},
volume = {9905},
year = {2016}
}
@article{Sattler2017a,
abstract = {Visual localization enables autonomous vehicles to navigate in their surroundings and Augmented Reality applications to link virtual to real worlds. In order to be practically relevant, visual localization approaches need to be robust to a wide variety of viewing condition, including day-night changes, as well as weather and seasonal variations. In this paper, we introduce the first benchmark datasets specifically designed for analyzing the impact of such factors on visual localization. Using carefully created ground truth poses for query images taken under a wide variety of conditions, we evaluate the impact of various factors on the quality of 6 degree-of-freedom (6DOF) camera pose estimation through extensive experiments with state-of-the-art localization approaches. Based on our results, we draw conclusions about the difficulty of different conditions and propose promising avenues for future work. We will eventually make our two novel benchmarks publicly available.},
archivePrefix = {arXiv},
arxivId = {1707.09092},
author = {Sattler, Torsten and Maddern, Will and Torii, Akihiko and Sivic, Josef and Pajdla, Tomas and Pollefeys, Marc and Okutomi, Masatoshi},
eprint = {1707.09092},
file = {:home/nathan/Documents/Mendeley Desktop/Sattler et al/arXiv preprint/Sattler et al.{\_}2017{\_}Benchmarking 6DOF Urban Visual Localization in Changing Conditions.pdf:pdf},
journal = {arXiv preprint},
title = {{Benchmarking 6DOF Urban Visual Localization in Changing Conditions}},
url = {http://arxiv.org/abs/1707.09092},
year = {2017}
}
@article{Milford2012,
abstract = { Abstract— Learning and then recognizing a route, whether travelled during the day or at night, in clear or inclement weather, and in summer or winter is a challenging task for state of the art algorithms in computer vision and robotics. In this paper, we present a new approach to visual navigation under changing conditions dubbed SeqSLAM. Instead of calculating the single location most likely given a current image, our approach calculates the best candidate matching location within every local navigation sequence. Localization is then achieved by recognizing coherent sequences of these " local best matches " . This approach removes the need for global matching performance by the vision front-end – instead it must only pick the best match within any short sequence of images. The approach is applicable over environment changes that render traditional feature-based techniques ineffective. Using two car-mounted camera datasets we demonstrate the effectiveness of the algorithm and compare it to one of the most successful feature-based SLAM algorithms, FAB-MAP. The perceptual change in the datasets is extreme; repeated traverses through environments during the day and then in the middle of the night, at times separated by months or years and in opposite seasons, and in clear weather and extremely heavy rain. While the feature-based method fails, the sequence-based algorithm is able to match trajectory segments at 100{\%} precision with recall rates of up to 60{\%}.},
author = {Milford, Michael J. and Wyeth, Gordon F.},
doi = {10.1109/ICRA.2012.6224623},
file = {:home/nathan/Documents/Mendeley Desktop/Milford, Wyeth/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Milford, Wyeth{\_}2012{\_}SeqSLAM Visual route-based navigation for sunny summer days and stormy winter nights.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
pages = {1643--1649},
title = {{SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights}},
year = {2012}
}
@article{Shao2003,
author = {Shao, Hao and Svoboda, Tom{\'{a}}{\v{s}} and {Van Gool}, Luc},
journal = {Computer Vision Lab, Swiss Federal Institute of Technology, Switzerland, Tech. Rep},
pages = {20},
title = {{Zubud-zurich buildings database for image based recognition}},
volume = {260},
year = {2003}
}
@article{Mikolajczyk2004,
author = {Mikolajczyk, Krystian and Schmid, Cordelia},
journal = {International Journal of Computer Vision (IJCV)},
number = {1},
pages = {63--86},
publisher = {Springer},
title = {{Scale {\&} affine invariant interest point detectors}},
volume = {60},
year = {2004}
}
@inproceedings{Sivic2003,
author = {Sivic, Josef and Zisserman, Andrew},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
file = {:home/nathan/Documents/Mendeley Desktop/Sivic, Zisserman/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Sivic, Zisserman{\_}2003{\_}Video Google A Text Retrieval Approach to Object Matching in Videos.pdf:pdf},
isbn = {0769519504},
pages = {1470--1477},
title = {{Video Google: A Text Retrieval Approach to Object Matching in Videos}},
year = {2003}
}
@inproceedings{Yan2016,
abstract = {In the past decade, SIFT is widely used in most vision tasks such as image retrieval. While in recent several years, deep convolutional neural networks (CNN) features achieve the state-of-the-art performance in several tasks such as image classification and object detection. Thus a natural question arises: for the image retrieval task, can CNN features substitute for SIFT? In this paper, we experimentally demonstrate that the two kinds of features are highly complementary. Following this fact, we propose an image representation model, complementary CNN and SIFT (CCS), to fuse CNN and SIFT in a multi-level and complementary way. In particular, it can be used to simultaneously describe scene-level, object-level and point-level contents in images. Extensive experiments are conducted on four image retrieval benchmarks, and the experimental results show that our CCS achieves state-of-the-art retrieval results.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode d'image retriveal combinant {\`{a}} la fois des descripteurs locaux et des CNN. Premi{\`{e}}re analyse qui compare la repr{\'{e}}sentation VLAD (donc extraction de SIFT, puis concat{\'{e}}nation par rapport aux centroides d'un visual dictonnary) avec une pr{\'{e}}sentation CNN (vecteur obtenu par poids d'une couche). Ils introduisent ensuite une repr{\'{e}}sentation de l'image se basant sur 3 types de descripteurs : les deux pr{\'{e}}c{\'{e}}dent (CNN niveau s{\'{e}}mantique, et VLAD) ainsi qu'un niveau interm{\'{e}}diaire (object level) se basant aussi sur les CNN mais en aggregant des edges boxes (ils testent trois types d'aggregations : max, mean {\&} VLAD). Enfin la fusion des trois types de features se fait par concat{\'{e}}nation suivit de PCA , witening {\&} L2 normalisation. 

Ce qui est int{\'{e}}ressant
- Les trois aspects de la repr{\'{e}}sentation : s{\'{e}}mantique, object et point

Critiques
- Les mAP pour les datastes sont pas vraiment {\`{a}} jour, donc pas de state of the art
- Ca doit {\^{e}}tre vachement long (CNN s{\'{e}}mantique ok, mais VLAD bof et edge boxes + CNN + VLAD {\c{c}}a doit aussi {\^{e}}tre chaud).

A approfondir
VLAD pour CNN

Computational load
Offline: no
Online: no

Scalability
Scale: test{\'{e}} sur Oxford {\&} Paris},
author = {Yan, Ke and Wang, Yaowei and Liang, Dawei and Huang, Tiejun and Tian, Yonghong},
booktitle = {Proceedings of the ACM International Conference on Multimedia (MM)},
doi = {10.1145/2964284.2967252},
file = {:home/nathan/Documents/Mendeley Desktop/Yan et al/Proceedings of the ACM International Conference on Multimedia (MM)/Yan et al.{\_}2016{\_}CNN vs. SIFT for Image Retrieval.pdf:pdf},
isbn = {9781450336031},
keywords = {ccs,cnn,complemen-,figure 1,multi-level image representation,proposed com-,sift,tary cnn and sift,the demonstration of our},
pages = {407--411},
title = {{CNN vs. SIFT for Image Retrieval}},
url = {http://dl.acm.org/citation.cfm?doid=2964284.2967252},
year = {2016}
}
@article{Feng2016a,
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode d'image based localization se basant sur une approche directe (2D to 3D). Les auteurs veulent diminuer le temps de calcul de la m{\'{e}}thode en utilisant des descripteurs binaires. lls pr{\'{e}}sentent leur travaux en 3 {\'{e}}tapes. D'abord la diminution du temps de description des descripteurs en remplacant SIFT par BRISK. Neanmoins ils soulignent le fait que ca la m{\'{e}}thode de matching des kppv est moins bonne est devient donc le bottleneck. Ils introduisent alors une m{\'{e}}thode de matching par random forest entrain{\'{e}} sur les sets de descriteurs (chaque points 3D est associ{\'{e}} {\`{a}} plusieurs descripteurs 2D). Enfin ils montrent que l'utilisation d'un detecteur de points d'interets plus rapide peut {\^{e}}tre utilis{\'{e}}, et remplace DoG par FALoG. 

Ce que je n'ai pas compris
- Le matching des descripteurs binaires

Ce qui est int{\'{e}}ressant
- 10x plus rapide et sans perte de precision

A approfondir
- BRISK {\&} FALoG

Computational load
Offline: non mentionn{\'{e}}
Online: 0.3s par image dans la version finale

Scalability
Scale: Places
Scalability potential: ?},
author = {Feng, Youji and Fan, Lixin and Wu, Yihong},
doi = {10.1109/TIP.2015.2500030},
file = {:home/nathan/Documents/Mendeley Desktop/Feng, Fan, Wu/IEEE Transactions on Image Processing (ToIP)/Feng, Fan, Wu{\_}2016{\_}Fast Localization in Large-Scale Environments Using Supervised Indexing of Binary Features.pdf:pdf},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing (ToIP)},
number = {1},
pages = {343--358},
title = {{Fast Localization in Large-Scale Environments Using Supervised Indexing of Binary Features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7327197},
volume = {25},
year = {2016}
}
@inproceedings{Cao2013,
abstract = {Recognizing the location of a query image by matching it to a database is an important problem in computer vision, and one for which the representation of the database is a key issue. We explore new ways for exploiting the structure of a database by representing it as a graph, and show how the rich information embedded in a graph can improve a bag-of-words-based location recognition method. In particular, starting from a graph on a set of images based on visual connectivity, we propose a method for selecting a set of sub graphs and learning a local distance function for each using discriminative techniques. For a query image, each database image is ranked according to these local distance functions in order to place the image in the right part of the graph. In addition, we propose a probabilistic method for increasing the diversity of these ranked database images, again based on the structure of the image graph. We demonstrate that our methods improve performance over standard bag-of-words methods on several existing location recognition datasets. View full abstract},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode d'image retriveal bas{\'{e}} sur un frameworks bag of worlds. Cr{\'{e}}ation en amont d'un graph compos{\'{e}} des images et reli{\'{e}} en fonction de leur similitude, puis compression en un sub-graph ou chaque noeud (ensemble d'image) permet d'entrainer un classifieur lin{\'{e}}aire (SVM) en prenant comme exemples positifs les images (leur vecteur issu de la quantification par BoW) du noeud. Au moment du traitement de la query on la compare aux SVM et on r{\'{e}}cup{\`{e}}re les images les plus problables. On a ensuite un reranking {\`{a}} la fois probabiliste (en maximisant la proba que le deuxi{\`{e}}me candidat soit le bon en sachant que le 1er est mauvais) et en mixant l'approche classique de retriveal par BoW.

Ce que je n'ai pas compris
- Logistic regressor training

Ce qui est int{\'{e}}ressant
- Le clustering par graph
- Le reranking probabiliste valorisant la diversit{\'{e}}

Critiques
- Pas de temps de clacul mais surement long vu qu'ils mixent deux m{\'{e}}thodes

Computational load
Offline: Plus long que du BoW vu que {\c{c}}a utilise de l'apprentissage et une comparaison entre chaque image
Online: Un peu long vu qu'on a {\`{a}} la fois leur m{\'{e}}thode + le BoW classique

Scalability
Scale: Multiple places
Scalability potential: D{\'{e}}pend de la base de donn{\'{e}}e},
author = {Cao, Song and Snavely, Noah},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1007/s11263-014-0774-9},
file = {:home/nathan/Documents/Mendeley Desktop/Cao, Snavely/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Cao, Snavely{\_}2013{\_}Graph-Based Discriminative Learning for Location Recognition.pdf:pdf},
isbn = {1063-6919},
issn = {15731405},
keywords = {Discriminative learning,Image graphs,Location recognition},
number = {2},
pages = {239--254},
title = {{Graph-Based Discriminative Learning for Location Recognition}},
volume = {112},
year = {2013}
}
@inproceedings{Saupe2001,
author = {Saupe, Dietmar and Vrani{\'{c}}, Dejan V.},
booktitle = {Joint Pattern Recognition Symposium},
organization = {Springer},
pages = {392--397},
title = {{3D model retrieval with spherical harmonics and moments}},
year = {2001}
}
@article{Ye2017,
author = {Ye, Yawei and Cieslewski, Titus and Loquercio, Antonio and Scaramuzza, Davide},
file = {:home/nathan/Documents/Mendeley Desktop/Ye et al/British Machine Vision Conference (BMVC)/Ye et al.{\_}2017{\_}Place Recognition in Semi-Dense Maps Geometric and Learning-Based Approaches.pdf:pdf},
journal = {British Machine Vision Conference (BMVC)},
title = {{Place Recognition in Semi-Dense Maps : Geometric and Learning-Based Approaches}},
year = {2017}
}
@article{Ramanana2016,
author = {Ramanana, Hasinarivo and Rahajaniaina, Andriamasinoro and Jessel, Jean-Pierre},
file = {:home/nathan/Documents/Mendeley Desktop/Ramanana, Rahajaniaina, Jessel/Proceedings of the International Conference on Advanced Geographic Information Systems, Applications, and Services (GEOProcessing)/Ramanana, Rahajaniaina, Jessel{\_}2016{\_}Image Based-Localization on Mobile Devices Using Geometric Features of Buildings.pdf:pdf},
isbn = {9781612084695},
journal = {Proceedings of the International Conference on Advanced Geographic Information Systems, Applications, and Services (GEOProcessing)},
keywords = {-image-based,building recognition},
number = {c},
pages = {58--62},
title = {{Image Based-Localization on Mobile Devices Using Geometric Features of Buildings}},
year = {2016}
}
@article{Greff2017,
abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ({\$}\backslashapprox 15{\$} years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
archivePrefix = {arXiv},
arxivId = {1503.04069},
author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
doi = {10.1109/TNNLS.2016.2582924},
eprint = {1503.04069},
file = {:home/nathan/Documents/Mendeley Desktop/Greff et al/IEEE Transactions on Neural Networks and Learning Systems/Greff et al.{\_}2017{\_}LSTM A Search Space Odyssey.pdf:pdf},
isbn = {9788578110796},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Functional ANalysis Of VAriance (fANOVA),long short-term memory (LSTM),random search,recurrent neural networks,sequence learning},
number = {10},
pages = {2222--2232},
pmid = {25246403},
title = {{LSTM: A Search Space Odyssey}},
volume = {28},
year = {2017}
}
@article{Wouw2017,
author = {Wouw, Dennis W. J. M. and Pieck, Martin A. R. and Dubbelman, Gijs and With, Peter H. N.},
file = {:home/nathan/Documents/Mendeley Desktop/Wouw et al/Electronic Imaging/Wouw et al.{\_}2017{\_}Real-time Estimation of the 3D Transformation between Images with Large Viewpoint Differences in Cluttered Environments.pdf:pdf},
journal = {Electronic Imaging},
pages = {109--116},
title = {{Real-time Estimation of the 3D Transformation between Images with Large Viewpoint Differences in Cluttered Environments}},
year = {2017}
}
@inproceedings{Rubio2015,
abstract = {— We propose a robust and efficient method to estimate the pose of a camera with respect to complex 3D textured models of the environment that can potentially contain more than 100, 000 points. To tackle this problem we follow a top down approach where we combine high-level deep network classifiers with low level geometric approaches to come up with a solution that is fast, robust and accurate. Given an input image, we initially use a pre-trained deep network to compute a rough estimation of the camera pose. This initial estimate constrains the number of 3D model points that can be seen from the camera viewpoint. We then establish 3D-to-2D corres-pondences between these potentially visible points of the model and the 2D detected image features. Accurate pose estimation is finally obtained from the 2D-to-3D correspondences using a novel PnP algorithm that rejects outliers without the need to use a RANSAC strategy, and which is between 10 and 100 times faster than other methods that use it. Two real experiments dealing with very large and complex 3D models demonstrate the effectiveness of the approach.},
author = {Rubio, A and Villamizar, M and Ferraz, L and Penata-Sanchez, A and Ramisa, A and Simo-Serra, E and Sanfeliu, A and Moreno-Noguer, F},
booktitle = {Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)},
file = {:home/nathan/Documents/Mendeley Desktop/Rubio et al/Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)/Rubio et al.{\_}2015{\_}Efficient Monocular Pose Estimation for Complex 3D Models.pdf:pdf},
isbn = {9781479969227},
pages = {1397--1402},
title = {{Efficient Monocular Pose Estimation for Complex 3D Models}},
volume = {2},
year = {2015}
}
@inproceedings{Poglitsch2015,
abstract = {We propose an outdoor localization system using a particle filter. In our approach, a textured, geo-registered model of the outdoor environment is used as a reference to estimate the pose of a smartphone. The device position and the orientation obtained from a Global Positioning System (GPS) receiver and an inertial measurement unit (IMU) are used as a first estimation of the true pose. Then, multiple pose hypotheses are randomly distributed about the GPS/IMU measurement and use to produce renderings of the virtual model. With vision-based methods, the rendered images are compared with the image received from the smartphone, and the matching scores are used to update the particle filter. The outcome of our system improves the camera pose estimate in real time without user assistance.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de localisation bas{\'{e}} vision utilsant une image RGB en input, un mod{\`{e}}le 3D textur{\'{e}} (par image reprojection) et une position grossi{\`{e}}re de la pose de la requ{\^{e}}te. Utilise un filtre particulaire qui g{\'{e}}n{\'{e}}re des particules autours du point d'origine grossi{\`{e}}rement localis{\'{e}} et compare le rendu (image) du mod{\`{e}}le 3D g{\'{e}}n{\'{e}}r{\'{e}} aux locations des particules pour estimer la probabilit{\'{e}} d'{\^{e}}tre {\`{a}} une pose position de la particule.

Ce qui est int{\'{e}}ressant
- Approche d'optimisation de position originale
- Bons r{\'{e}}sultats malgr{\`{e}}s les m{\'{e}}thodes de comparaison basiques des images (SSD)

Critiques
- Temps de calcul un peu long
- Petits probl{\`{e}}mes de stabilit{\'{e}} du filtre

Computational load
Offline: -
Online: 30hz after initialization (699ms)

Scalability
Scale: -
Scalability potential: Maybe},
author = {Poglitsch, Christian and Arth, Clemens and Schmalstieg, Dieter and Ventura, Jonathan},
booktitle = {Proceedings of the IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
doi = {10.1109/ISMAR.2015.39},
file = {:home/nathan/Documents/Mendeley Desktop/Poglitsch et al/Proceedings of the IEEE International Symposium on Mixed and Augmented Reality (ISMAR)/Poglitsch et al.{\_}2015{\_}A particle filter approach to outdoor localization using image-based rendering.pdf:pdf},
isbn = {9781467376600},
pages = {132--135},
title = {{A particle filter approach to outdoor localization using image-based rendering}},
year = {2015}
}
@inproceedings{Wu2009,
abstract = {In this paper we describe the problem of visual place categorization (VPC) for mobile robotics, which involves predicting the semantic category of a place from image measurements acquired from an autonomous platform. For example, a robot in an unfamiliar home environment should be able to recognize the functionality of the rooms it visits, such as kitchen, living room, etc. We describe an approach to VPC based on sequential processing of images acquired with a conventional video camera. We identify two key challenges: Dealing with non-characteristic views and integrating restricted-FOV imagery into a holistic prediction. We present a solution to VPC based upon a recently-developed visual feature known as CENTRIST (census transform histogram). We describe a new dataset for VPC which we have recently collected and are making publicly available. We believe this is the first significant, realistic dataset for the VPC problem. It contains the interiors of six different homes with ground truth labels. We use this dataset to validate our solution approach, achieving promising results. View full abstract},
author = {Wu, Jianxin and Christensen, Henrik I. and Rehg, James M.},
booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2009.5354164},
file = {:home/nathan/Documents/Mendeley Desktop/Wu, Christensen, Rehg/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Wu, Christensen, Rehg{\_}2009{\_}Visual place categorization Problem, dataset, and algorithm.pdf:pdf},
isbn = {9781424438044},
issn = {2153-0858},
pages = {4763--4770},
title = {{Visual place categorization: Problem, dataset, and algorithm}},
year = {2009}
}
@article{Lowry2016a,
abstract = {This paper investigates the application of linear learning techniques to the place recognition problem. We present two learning methods, a supervised change prediction technique based on linear regression and an unsupervised change removal technique based on principal component analysis, and investigate how the performance of each is affected by the choice of training data. We show that the change prediction technique presented here succeeds only if it is provided with appropriate and adequate training data, which can be challenging for a mobile robotic system operating in an uncontrolled environment. In contrast, change removal can improve place recognition performance even when trained with as few as 100 samples. This paper shows that change removal can be combined with a number of different image descriptors and can improve performance across a range of different appearance conditions.},
annote = {R{\'{e}}sum{\'{e}}
Toujours bas{\'{e}} plus ou moins sur le framework de SeqSLAM. Les auteurs comparent deux techniques pour matcher des images avec des repr{\'{e}}sentations diff{\'{e}}rentes (season changes). La premi{\`{e}}re m{\'{e}}thode, supervis{\'{e}}, tend {\`{a}} modifier l'image pour qu'elle s'approche des conditions observ{\'{e}} {\`{a}} un temps t (rendre une image "neigeuse"). La deuxi{\`{e}}me, qui necessite moins de supervison, consiste {\`{a}} modifier l'image pour la rendre invariante aux changements. Les deux m{\'{e}}thode se basent sur une technique simple de r{\'{e}}duction de dimension par PCA. La conclusion est que c'est la m{\'{e}}thode moins supervis{\'{e}} qui marche le mieux. Les auteurs font des essaies avec plusieurs descripteurs globaux (apr{\`{e}}s correction de l'image par PCA), dont les layers du CNN ConvNet, tout {\c{c}}a embedded dans le framework SeqSLAM.},
author = {Lowry, Stephanie and Milford, Michael J.},
doi = {10.1109/TRO.2016.2545711},
file = {:home/nathan/Documents/Mendeley Desktop/Lowry, Milford/IEEE Transactions on Robotics (ToR)/Lowry, Milford{\_}2016{\_}Supervised and Unsupervised Linear Learning Techniques for Visual Place Recognition in Changing Environments.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics (ToR)},
keywords = {Changing environments,learning about change,linear regression,principal component analysis (PCA),visual place recognition},
number = {3},
pages = {600--613},
title = {{Supervised and Unsupervised Linear Learning Techniques for Visual Place Recognition in Changing Environments}},
volume = {32},
year = {2016}
}
@article{Garcia2017,
abstract = {Can a neural network learn the concept of visual similarity? In this work, this question is addressed by training a deep learning model for the specific task of measuring the similarity between a pair of pictures in content-based image retrieval datasets. Traditionally, content-based image retrieval systems rely on two fundamental tasks: 1) computing meaningful image representations from pixels and 2) measuring accurate visual similarity between those representations. Whereas in the last few years several methods have been proposed to find high quality image representations including SIFT, VLAD or RMAC, most techniques still depend on standard metrics such as Euclidean distance or cosine similarity for the visual similarity task. However, standard metrics are independent from data and might be missing the nonlinear inner structure of visual representations. In this paper, we propose to learn a non-metric visual similarity function directly from image representations to measure how alike two images are. Experiments on standard image retrieval datasets show that results are boosted when using the proposed method over standard metrics.},
archivePrefix = {arXiv},
arxivId = {1709.01353},
author = {Garcia, Noa and Vogiatzis, George},
eprint = {1709.01353},
file = {:home/nathan/Documents/Mendeley Desktop/Garcia, Vogiatzis/arXiv preprint/Garcia, Vogiatzis{\_}2017{\_}Learning Non-Metric Visual Similarity for Image Retrieval.pdf:pdf},
journal = {arXiv preprint},
title = {{Learning Non-Metric Visual Similarity for Image Retrieval}},
url = {http://arxiv.org/abs/1709.01353},
year = {2017}
}
@article{Atanasov2016,
abstract = {Most approaches to robot localization rely on low-level geometric features such as points, lines, and planes. In this paper, we use object recognition to obtain semantic information from the robot's sensors and consider the task of localizing the robot within a prior map of landmarks, which are annotated with semantic labels. As object recognition algorithms miss detections and produce false alarms, correct data association between the detections and the landmarks on the map is central to the semantic localization problem. Instead of the traditional vector-based representation, we propose a sensor model, which encodes the semantic observations via random finite sets and enables a unified treatment of missed detections, false alarms, and data association. Our second contribution is to reduce the problem of computing the likelihood of a set-valued observation to the problem of computing a matrix permanent. It is this crucial transformation that allows us to solve the semantic localization problem with a polynomial-time approximation to the set-based Bayes filter. Finally, we address the active semantic localization problem, in which the observer's trajectory is planned in order to improve the accuracy and efficiency of the localization process. The performance of our approach is demonstrated in simulation and in real environments using deformable-part-model-based object detectors. Robust global localization from semantic observations is demonstrated for a mobile robot, for the Project Tango phone, and on the KITTI visual odometry dataset. Comparisons are made with the traditional lidar-based geometric Monte Carlo localization.},
author = {Atanasov, Nikolay and Zhu, Menglong and Daniilidis, Kostas and Pappas, George J.},
doi = {10.1177/0278364915596589},
file = {:home/nathan/Documents/Mendeley Desktop/Atanasov et al/The International Journal of Robotics Research (IJRR)/Atanasov et al.{\_}2016{\_}Localization from semantic observations via the matrix permanent.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research (IJRR)},
number = {1-3},
pages = {73--99},
title = {{Localization from semantic observations via the matrix permanent}},
url = {http://journals.sagepub.com/doi/10.1177/0278364915596589},
volume = {35},
year = {2016}
}
@article{Tetreault2017,
author = {Tetreault, Jesse},
file = {:home/nathan/Documents/Mendeley Desktop/Tetreault/Unknown/Tetreault{\_}2017{\_}Deep Multimodal Fusion Networks for Semantic Segmentation.pdf:pdf},
title = {{Deep Multimodal Fusion Networks for Semantic Segmentation}},
year = {2017}
}
@article{Galvez-Lopez2014,
abstract = {We present a real-time object-based SLAM system that leverages the largest object database to date. Our approach comprises two main components: (1) a monocular SLAM algorithm that exploits object rigidity constraints to improve the map and find its real scale, and (2) a novel object recognition algorithm based on bags of binary words, which provides live detections with a database of 500 3D objects. The two components work together and benefit each other: the SLAM algorithm accumulates information from the observations of the objects, anchors object features to especial map landmarks and sets constrains on the optimization. At the same time, objects partially or fully located within the map are used as a prior to guide the recognition algorithm, achieving higher recall. We evaluate our proposal on five real environments showing improvements on the accuracy of the map and efficiency with respect to other state-of-the-art techniques.},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.02398v1},
author = {G{\'{a}}lvez-L{\'{o}}pez, Dorian and Salas, Marta and Tard{\'{o}}s, Juan D. and Montiel, J. M M},
doi = {10.1016/j.robot.2015.08.009},
eprint = {arXiv:1504.02398v1},
file = {:home/nathan/Documents/Mendeley Desktop/G{\'{a}}lvez-L{\'{o}}pez et al/Robotics and Autonomous Systems (RAS)/G{\'{a}}lvez-L{\'{o}}pez et al.{\_}2014{\_}Real-time monocular object SLAM.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems (RAS)},
keywords = {Object recognition,Object slam},
title = {{Real-time monocular object SLAM}},
year = {2014}
}
@article{Csurka2017,
abstract = {The aim of this paper is to give an overview of domain adaptation and transfer learning with a specific view on visual applications. After a general motivation, we first position domain adaptation in the larger transfer learning problem. Second, we try to address and analyze briefly the state-of-the-art methods for different types of scenarios, first describing the historical shallow methods, addressing both the homogeneous and the heterogeneous domain adaptation methods. Third, we discuss the effect of the success of deep convolutional architectures which led to new type of domain adaptation methods that integrate the adaptation within the deep architecture. Fourth, we overview the methods that go beyond image categorization, such as object detection or image segmentation, video analyses or learning visual attributes. Finally, we conclude the paper with a section where we relate domain adaptation to other machine learning solutions.},
archivePrefix = {arXiv},
arxivId = {1702.05374},
author = {Csurka, Gabriela},
eprint = {1702.05374},
file = {:home/nathan/Documents/Mendeley Desktop/Csurka/arXiv preprint/Csurka{\_}2017{\_}Domain Adaptation for Visual Applications A Comprehensive Survey.pdf:pdf},
journal = {arXiv preprint},
pages = {1--46},
title = {{Domain Adaptation for Visual Applications: A Comprehensive Survey}},
url = {http://arxiv.org/abs/1702.05374},
year = {2017}
}
@inproceedings{Babenko2014,
annote = {Meme conclusion que Sunderaulf 2015 (un an plus tot -{\_}-). Utilisation des features directement des couches fully connected (et une couche de convolution) avec r{\'{e}}duction de dimension.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.1777v1},
author = {Babenko, Artem and Slesarev, Anton and Chigorin, Alexandr and Lempitsky, Victor},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-319-10590-1_38},
eprint = {arXiv:1404.1777v1},
file = {:home/nathan/Documents/Mendeley Desktop/Babenko et al/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Babenko et al.{\_}2014{\_}Neural Codes for Image Retrieval.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
keywords = {convolutional neural networks,deep learning,feature extraction,image retrieval,same-object image search},
pages = {584--599},
title = {{Neural Codes for Image Retrieval}},
url = {http://davidstutz.de/wordpress/wp-content/uploads/2015/07/seminar.pdf},
year = {2014}
}
@article{Garg2016,
abstract = {A significant weakness of most current deep Convolutional Neural Networks is the need to train them using vast amounts of manu- ally labelled data. In this work we propose a unsupervised framework to learn a deep convolutional neural network for single view depth predic- tion, without requiring a pre-training stage or annotated ground truth depths. We achieve this by training the network in a manner analogous to an autoencoder. At training time we consider a pair of images, source and target, with small, known camera motion between the two such as a stereo pair. We train the convolutional encoder for the task of predicting the depth map for the source image. To do so, we explicitly generate an inverse warp of the target image using the predicted depth and known inter-view displacement, to reconstruct the source image; the photomet- ric error in the reconstruction is the reconstruction loss for the encoder. The acquisition of this training data is considerably simpler than for equivalent systems, requiring no manual annotation, nor calibration of depth sensor to camera. We show that our network trained on less than half of the KITTI dataset (without any further augmentation) gives com- parable performance to that of the state of art supervised methods for single view depth estimation.},
archivePrefix = {arXiv},
arxivId = {1603.04992},
author = {Garg, Ravi and {Vijay Kumar}, B. G. and Carneiro, Gustavo and Reid, Ian},
doi = {10.1007/978-3-319-46484-8_45},
eprint = {1603.04992},
file = {:home/nathan/Documents/Mendeley Desktop/Garg et al/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Garg et al.{\_}2016{\_}Unsupervised CNN for single view depth estimation Geometry to the rescue.pdf:pdf},
isbn = {9783319464831},
issn = {16113349},
journal = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
pages = {740--756},
pmid = {4520227},
title = {{Unsupervised CNN for single view depth estimation: Geometry to the rescue}},
volume = {9912 LNCS},
year = {2016}
}
@article{Baatz2010,
abstract = {We address the problem of large scale place-of-interest recognition in cell phone images of urban scenarios. Here, we go beyond what has been shown in earlier approaches by exploiting the nowadays often available 3D building information (e.g. from extruded floor plans) and massive street-view like image data for database creation. Exploiting vanishing points in query images and thus fully removing 3D rotation from the recognition problem allows then to simplify the feature invariance to a pure homothetic problem, which we show leaves more discriminative power in feature descriptors than classical SIFT. We rerank visual word based document queries using a fast stratified homothetic verification that is tailored for repetitive patterns like window grids on facades and in most cases boosts the correct document to top positions if it was in the short list. Since we exploit 3D building information, the approach finally outputs the camera pose in real world coordinates ready for augmenting the cell phone image with virtual 3D information. The whole system is demonstrated to outperform traditional approaches on city scale experiments for different sources of street-view like image data and a challenging set of cell phone images.},
author = {Baatz, Georges and K{\"{o}}ser, Kevin and Chen, David M. and Grzeszczuk, Radek and Pollefeys, Marc},
doi = {10.1007/978-3-642-15567-3_20},
file = {:home/nathan/Documents/Mendeley Desktop/Baatz et al/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Baatz et al.{\_}2010{\_}Handling urban location recognition as a 2D homothetic problem.pdf:pdf},
isbn = {3642155669},
issn = {03029743},
journal = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
number = {PART 6},
pages = {266--279},
title = {{Handling urban location recognition as a 2D homothetic problem}},
volume = {6316 LNCS},
year = {2010}
}
@article{Lothe2009,
abstract = {In the past few years, lots of works were achieved on Simultaneous Localization and Mapping (SLAM). It is now possible to follow in real time the trajectory of a moving camera in an unknown environment. However, current SLAM methods are still prone to drift errors, which prevent their use in large-scale applications. In this paper, we propose a solution to reduce those errors a posteriori. Our solution is based on a postprocessing algorithm that exploits additional geometric constraints, relative to the environment, to correct both the reconstructed geometry and the camera trajectory. These geometric constraints are obtained through a coarse 3D modelisation of the environment, similar to those provided by GIS database. First, we propose an original articulated transformation model in order to roughly align the SLAM reconstruction with this 3D model through a non-rigid ICP step. Then, to refine the reconstruction, we introduce a new bundle adjustment cost function that includes, in a single term, the usual 3D point/ID observation consistency constraint as well as the geometric constraints provided by the 3D model. Results on large-scale synthetic and real sequences show that our method successfully improves SLAM reconstructions. Besides, experiments prove that the resulting reconstruction is accurate enough to be directly used for global relocalization applications.},
author = {Lothe, Pierre and Bourgeois, Steve and Dekeyser, Fabien and Royer, Eric and Dhome, Michel},
doi = {10.1109/CVPRW.2009.5206662},
file = {:home/nathan/Documents/Mendeley Desktop/Lothe et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Lothe et al.{\_}2009{\_}Towards geographical referencing of monocular SLAM reconstruction using 3D city models Application to real-time accura.pdf:pdf},
isbn = {9781424439935},
issn = {1063-6919},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2882--2889},
title = {{Towards geographical referencing of monocular SLAM reconstruction using 3D city models: Application to real-time accurate vision-based localization}},
year = {2009}
}
@article{Wong2015,
author = {Wong, David and Deguchi, Daisuke and Ide, Ichiro and Murase, Hiroshi},
doi = {10.1109/ICCVW.2015.22},
file = {:home/nathan/Documents/Mendeley Desktop/Wong et al/Proceedings of the IEEE International Conference on Computer Vision Workshop (ICCVW)/Wong et al.{\_}2015{\_}Position Interpolation Using Feature Point Scale for Decimeter Visual Localization.pdf:pdf},
isbn = {9781467383905},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision Workshop (ICCVW)},
keywords = {Cameras,Databases,Feature extraction,Image matching,Interpolation,Vehicles,Visualization},
pages = {90--97},
title = {{Position Interpolation Using Feature Point Scale for Decimeter Visual Localization}},
volume = {2015-Febru},
year = {2015}
}
@inproceedings{Drost2010,
author = {Drost, Bertram and Ulrich, Markus and Navab, Nassir and Ilic, Slobodan},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
organization = {Ieee},
pages = {998--1005},
title = {{Model globally, match locally: Efficient and robust 3D object recognition}},
year = {2010}
}
@inproceedings{Zhang2006,
abstract = {In this paper we present a prototype system for image based localization in urban environments. Given a database of views of city street scenes tagged by GPS locations, the sys-tem computes the GPS location of a novel query view. We first use a wide-baseline matching technique based on SIFT features to select the closest views in the database. Often due to a large change of viewpoint and presence of repetitive structures, a large percentage of matches ({\textgreater} 50{\%}) are not correct correspondences. The subsequent motion estima-tion between the query view and the reference view, is then handled by a novel and efficient robust estimation technique capable of dealing with large percentage of outliers. This stage is also accompanied by a model selection step among the fundamental matrix and the homography. Once the mo-tion between the closest reference views is estimated, the location of the query view is then obtained by triangulation of translation directions. Approximate solutions for cases when triangulation cannot be obtained reliably are also de-scribed. The presented system is tested on the dataset used in ICCV 2005 Computer Vision Contest and is shown to have higher accuracy than previous reported results.},
author = {Zhang, Wei and Koseck{\'{a}}, Jana},
booktitle = {3D Data Processing, Visualization and Transmission (3DPVT)},
file = {:home/nathan/Documents/Mendeley Desktop/Zhang, Koseck{\'{a}}/3D Data Processing, Visualization and Transmission (3DPVT)/Zhang, Koseck{\'{a}}{\_}2006{\_}Image Based Localization in Urban Environments.pdf:pdf},
title = {{Image Based Localization in Urban Environments}},
year = {2006}
}
@article{Johnson2017,
abstract = {Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy. We propose a design for k-selection that operates at up to 55{\%} of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.},
archivePrefix = {arXiv},
arxivId = {1702.08734},
author = {Johnson, Jeff and Douze, Matthijs and J{\'{e}}gou, Herv{\'{e}}},
eprint = {1702.08734},
file = {:home/nathan/Documents/Mendeley Desktop/Johnson, Douze, J{\'{e}}gou/arXiv preprint/Johnson, Douze, J{\'{e}}gou{\_}2017{\_}Billion-scale similarity search with GPUs.pdf:pdf},
journal = {arXiv preprint},
title = {{Billion-scale similarity search with GPUs}},
url = {http://arxiv.org/abs/1702.08734},
year = {2017}
}
@article{Bao2016,
abstract = {Self-localization is one of the most important part in autonomous driving system. In urban canyon, the multipath and non-line-of-sight effects to GPS receiver decrease the precision of self-localization of the vehicle. More specifically, the lateral error is more serious because of the blockage of the satellites. However, the building on roadside could be the stable reference object for localization. Therefore, this paper proposes to use stereo camera and 3D building map to reduce the lateral error of positioning result. In our proposal, stereo camera is used to detect and reconstruct the building side view. Lateral distance between building and vehicle estimated by stereo camera is compared with 3D building map to rectify the lateral position of vehicle. In addition, this paper employs inertial sensor and GPS receiver to decide the longitudinal position of vehicle. The particle filter is used for the sensor fusion. The experiment is conducted in the center of Tokyo, Japan, which is a typical urban city scene with high density of tall buildings. It demonstrates that the proposed method could achieve sub-meter level accuracy in GPS difficult environments.},
author = {Bao, Jiali and Gu, Yanlei and Hsu, Li Ta and Kamijo, Shunsuke},
doi = {10.1109/IVS.2016.7535499},
file = {:home/nathan/Documents/Mendeley Desktop/Bao et al/Proceedings of the IEEE Intelligent Vehicles Symposium (IV)/Bao et al.{\_}2016{\_}Vehicle self-localization using 3D building map and stereo camera.pdf:pdf},
isbn = {9781509018215},
journal = {Proceedings of the IEEE Intelligent Vehicles Symposium (IV)},
number = {Iv},
pages = {927--932},
title = {{Vehicle self-localization using 3D building map and stereo camera}},
volume = {2016-Augus},
year = {2016}
}
@inproceedings{Krajnik2014,
abstract = {— This paper presents a new approach for topo-logical localisation of service robots in dynamic indoor envi-ronments. In contrast to typical localisation approaches that rely mainly on static parts of the environment, our approach makes explicit use of information about changes by learning and modelling the spatio-temporal dynamics of the environment where the robot is acting. The proposed spatio-temporal world model is able to predict environmental changes in time, allowing the robot to improve its localisation capabilities during long-term operations in populated environments. To investigate the proposed approach, we have enabled a mobile robot to autonomously patrol a populated environment over a period of one week while building the proposed model representation. We demonstrate that the experience learned during one week is applicable for topological localization even after a hiatus of three months by showing that the localization error rate is significantly lower compared to static environment representa-tions.},
annote = {Version courte du ToR 2017},
author = {Krajn{\'{i}}k, Tom{\'{a}}{\v{s}} and Fentanes, Jaime P. and Mozos, Oscar M. and Duckett, Tom and Ekekrantz, Johan and Hanheide, Marc},
booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
file = {:home/nathan/Documents/Mendeley Desktop/Krajn{\'{i}}k et al/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Krajn{\'{i}}k et al.{\_}2014{\_}Long-Term Topological Localisation for Service Robots in Dynamic Environments using Spectral Maps.pdf:pdf},
isbn = {9781479969333},
number = {Iros},
pages = {4537--4542},
title = {{Long-Term Topological Localisation for Service Robots in Dynamic Environments using Spectral Maps}},
year = {2014}
}
@article{Latif2017,
abstract = {Place recognition is an essential component of any Simultaneous Localization and Mapping (SLAM) system. Correct place recognition is a difficult perception task in cases where there is significant appearance change as the same place might look very different in the morning and at night or over different seasons. This work addresses place recognition using a two-step (generative and discriminative) approach. Using a pair of coupled Generative Adversarial Networks (GANs), we show that it is possible to generate the appearance of one domain (such as summer) from another (such as winter) without needing image to image correspondences. We identify these relationships considering sets of images in the two domains without knowing the instance-to-instance correspondence. In the process, we learn meaningful feature spaces, the distances in which can be used for the task of place recognition. Experiments show that learned feature correspond to visual space and can be effectively used for place recognition across seasons.},
archivePrefix = {arXiv},
arxivId = {1709.08810},
author = {Latif, Yasir and Garg, Ravi and Milford, Michael and Reid, Ian},
eprint = {1709.08810},
file = {:home/nathan/Documents/Mendeley Desktop/Latif et al/arXiv preprint/Latif et al.{\_}2017{\_}Addressing Challenging Place Recognition Tasks using Generative Adversarial Networks.pdf:pdf},
journal = {arXiv preprint},
title = {{Addressing Challenging Place Recognition Tasks using Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1709.08810},
year = {2017}
}
@article{Neverova2017,
abstract = {Despite the large improvements in performance attained by using deep learning in computer vision, one can often further improve results with some additional post-processing that exploits the geometric nature of the underlying task. This commonly involves displacing the posterior distribution of a CNN in a way that makes it more appropriate for the task at hand, e.g. better aligned with local image features, or more compact. In this work we integrate this geometric post-processing within a deep architecture, introducing a differentiable and probabilistically sound counterpart to the common geometric voting technique used for evidence accumu-lation in vision. We refer to the resulting neural models as Mass Displacement Networks (MDNs), and apply them to human pose estimation in two distinct setups: (a) landmark localization, where we collapse a distribution to a point, allowing for precise localization of body keypoints and (b) communication across body parts, where we transfer evidence from one part to the other, allowing for a globally con-sistent pose estimate. We evaluate on large-scale pose estimation benchmarks, such as MPII Human Pose and COCO datasets, and report systematic improvements when compared to strong baselines.},
archivePrefix = {arXiv},
arxivId = {1708.03816},
author = {Neverova, Natalia and Kokkinos, Iasonas},
eprint = {1708.03816},
file = {:home/nathan/Documents/Mendeley Desktop/Neverova, Kokkinos/arXiv preprint/Neverova, Kokkinos{\_}2017{\_}Mass Displacement Networks.pdf:pdf},
journal = {arXiv preprint},
pages = {1--12},
title = {{Mass Displacement Networks}},
url = {https://arxiv.org/pdf/1708.03816.pdf},
year = {2017}
}
@inproceedings{Ardeshir2014,
abstract = {Geographical Information System (GIS) databases contain information about many objects, such as traffic signals, road signs, fire hydrants, etc. in urban areas. This wealth of information can be utilized for assisting various computer vision tasks. In this paper, we propose a method for improving object detection using a set of priors acquired from GIS databases. Given a database of object locations from GIS and a query image with metadata, we compute the expected spatial location of the visible objects in the image.We also perform object detection in the query image (e.g., using DPM) and obtain a set of candidate bounding boxes for the objects. Then, we fuse the GIS priors with the potential detections to find the final object bounding boxes. To cope with various inaccuracies and practical complications, such as noisy metadata, occlu- sion, inaccuracies in GIS, and poor candidate detections, we formulate our fusion as a higher-order graph matching problem which we robustly solve using RANSAC. We demonstrate that this approach outperforms well established object detectors, such as DPM, with a large margin. Furthermore, we propose that the GIS objects can be used as cues for discovering the location where an image was taken. Our hypothesis is based on the idea that the objects visible in one image, along with their relative spatial location, provide distinctive cues for the geo-location. In order to estimate the geo-location based on the generic objects, we perform a search on a dense grid of locations over the covered area. We assign a score to each location based on the similarity of its GIS objects and the imperfect object detections in the image.We demonstrate that over a broad urban area of {\textgreater}10 square kilometers, this semantic approach can significantly narrow down the localization search space, and occasionally, even find the correct location.},
author = {Ardeshir, Shervin and Zamir, Amir Roshan and Torroella, Alejandro and Shah, Mubarak},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-319-10599-4_39},
file = {:home/nathan/Documents/Mendeley Desktop/Ardeshir et al/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Ardeshir et al.{\_}2014{\_}GIS-assisted object detection and geospatial localization.pdf:pdf},
isbn = {9783319105987},
issn = {16113349},
number = {PART 6},
pages = {602--617},
title = {{GIS-assisted object detection and geospatial localization}},
volume = {8694 LNCS},
year = {2014}
}
@inproceedings{Cheng2014,
author = {Cheng, Ming-ming and Zhang, Ziming and Lin, Wen-yan and Torr, Philip H. S.},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Cheng et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Cheng et al.{\_}2014{\_}BING Binarized Normed Gradients for Objectness Estimation at 300fps.pdf:pdf},
title = {{BING: Binarized Normed Gradients for Objectness Estimation at 300fps}},
year = {2014}
}
@article{Aubry2014,
abstract = {This article describes a technique that can reliably align arbitrary 2D depictions of an architectural site, including drawings, paintings, and historical photographs, with a 3D model of the site. This is a tremendously difficult task, as the appearance and scene structure in the 2D depictions can be very different from the appearance and geometry of the 3D model, for example, due to the specific rendering style, drawing error, age, lighting, or change of seasons. In addition, we face a hard search problem: the number of possible alignments of the painting to a large 3D model, such as a partial reconstruction of a city, is huge. To address these issues, we develop a newcompact representation of complex 3D scenes. The 3D model of the scene is represented by a small set of discriminative visual elements that are automatically learned from rendered views. Similar to object detection, the set of visual elements, as well as the weights of individual features for each element, are learned in a discriminative fashion. We show that the learned visual elements are reliably matched in 2D depictions of the scene despite large variations in rendering style (e.g., watercolor, sketch, historical photograph) and structural changes (e.g., missing scene parts, large occluders) of the scene. We demonstrate an application of the proposed approach to automatic rephotography to find an approximate viewpoint of historical paintings and photographs with respect to a 3D model of the site. The proposed alignment procedure is validated via a human user study on a new database of paintings and sketches spanning several sites. The results demonstrate that our algorithm produces significantly better alignments than several baseline methods.},
annote = {From Duplicate 1 (Painting-to-3D model alignment via discriminative visual elements - Aubry, Mathieu; Russell, Bryan C.; Sivic, Josef)

R{\'{e}}sum{\'{e}}

Pipeline pour retrouver le point de vue d'une image {\`{a}} la repr{\'{e}}sentation non photographique d'un batiment {\`{a}} partir d'un mod{\`{e}}le 3D du batiment ou d'une ville.
1) Extraction de features discriminantes {\`{a}} partir du mod{\`{e}}le 3D
2) Matching d'une repr{\'{e}}sentation non photographique avec les features extraites et recalage de l'image

Ce que je n'ai pas compris

Section 4.2 - 4.5 : comment selectionner les bonnes features ? Comment initialiser les features ? Comment calculer la fonction de coup ? Apprentissage supervis{\'{e}} ou non-supervis{\'{e}} ?

Ce qui est int{\'{e}}ressant

Deux axes :
1) recherche global
2) recallage

Le descripteur utilis{\'{e}} (HOG) et la m{\'{e}}thode pour choisir un set de classieurs discriminants
Matching tr{\`{e}}s challenging (donn{\'{e}}es tr{\`{e}}s bruit{\'{e}})

Critiques

- temps de calcul long
- donn{\'{e}}es que l'on cherche {\`{a}} recaller qui ne sont pas des photos (du coup {\c{c}}a doit marcher encore mieux mais peut {\^{e}}tre overkill ?)
- il faut un mod{\`{e}}le 3D
- 50{\%} de bonne classification
- donn{\'{e}}es RGB seulement pour la cible

A approfondir

- LDA
- dataset Hauagge and Snavely
- les 3 types d'allignements {\'{e}}voqu{\'{e}} dans l'introduction
- HOG based linear classifier
- ICP like fine alignement

Computational load
Offline: 2.000 elements/h
Online: 22 minutes for matching the query + 25s for final pose retrieval

Scalability
Scale: Place
Scalability potential: -

From Duplicate 2 (Painting-to-3D model alignment via discriminative visual elements - Aubry, Mathieu; Russell, Bryan C.; Sivic, Josef)

R{\{}{\'{e}}{\}}sum{\{}{\'{e}}{\}}

Pipeline pour retrouver le point de vue d'une image {\{}{\`{a}}{\}} la repr{\{}{\'{e}}{\}}sentation non photographique d'un batiment {\{}{\`{a}}{\}} partir d'un mod{\{}{\`{e}}{\}}le 3D du batiment ou d'une ville.
1) Extraction de features discriminantes {\{}{\`{a}}{\}} partir du mod{\{}{\`{e}}{\}}le 3D
2) Matching d'une repr{\{}{\'{e}}{\}}sentation non photographique avec les features extraites et recalage de l'image

Ce que je n'ai pas compris

Section 4.2 - 4.5 : comment selectionner les bonnes features ? Comment initialiser les features ? Comment calculer la fonction de coup ? Apprentissage supervis{\{}{\'{e}}{\}} ou non-supervis{\{}{\'{e}}{\}} ?

Ce qui est int{\{}{\'{e}}{\}}ressant

Deux axes :
1) recherche global
2) recallage

Le descripteur utilis{\{}{\'{e}}{\}} (HOG) et la m{\{}{\'{e}}{\}}thode pour choisir un set de classieurs discriminants
Matching tr{\{}{\`{e}}{\}}s challenging (donn{\{}{\'{e}}{\}}es tr{\{}{\`{e}}{\}}s bruit{\{}{\'{e}}{\}})

Critiques

- temps de calcul long
- donn{\{}{\'{e}}{\}}es que l'on cherche {\{}{\`{a}}{\}} recaller qui ne sont pas des photos (du coup {\{}{\c{c}}{\}}a doit marcher encore mieux mais peut {\{}{\^{e}}{\}}tre overkill ?)
- il faut un mod{\{}{\`{e}}{\}}le 3D
- 50{\{}{\%}{\}} de bonne classification
- donn{\{}{\'{e}}{\}}es RGB seulement pour la cible

A approfondir

- LDA
- dataset Hauagge and Snavely
- les 3 types d'allignements {\{}{\'{e}}{\}}voqu{\{}{\'{e}}{\}} dans l'introduction
- HOG based linear classifier
- ICP like fine alignement},
author = {Aubry, Mathieu and Russell, Bryan C. and Sivic, Josef},
doi = {10.1145/2591009},
file = {:home/nathan/Documents/Mendeley Desktop/Aubry, Russell, Sivic/ACM Transactions on Graphics (ToG)/Aubry, Russell, Sivic{\_}2014{\_}Painting-to-3D model alignment via discriminative visual elements.pdf:pdf},
isbn = {0730-0301},
issn = {07300301},
journal = {ACM Transactions on Graphics (ToG)},
keywords = {3D models,Alignement,Challenging matching,Database creation,Global features,HOG,Linear classifier,Long terme place recognition,Multiple visual domaines,Outdoor,RGB request,View Synthesis,Visual place recognition,a},
mendeley-tags = {3D models,Alignement,Challenging matching,Database creation,Global features,HOG,Linear classifier,Long terme place recognition,Multiple visual domaines,Outdoor,RGB request,View Synthesis,Visual place recognition},
number = {2},
pages = {1--14},
publisher = {Association for Computing Machinery},
title = {{Painting-to-3D model alignment via discriminative visual elements}},
url = {https://hal.inria.fr/hal-00863615v3 http://dl.acm.org/citation.cfm?id=2591009{\%}5Cnhttp://pucrio.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwlV3dS8MwED9ERRTx225-QB98bV2bZG0f5-YYyMQHBfFFkjQRwW3qOv9-75I6P{\_}BBH9NeQrhL7n7hvgBYGreiHzrBqHbJUdTtVON7QBQ54gjGDG-},
volume = {33},
year = {2014}
}
@article{Bansal2012,
abstract = {Matching street-level images to a database of airborne images is hard because of extreme viewpoint and illumination differences. Color/gradient distributions or local descriptors fail to match forcing us to rely on the structure of self-similarity of patterns on facades. We propose to capture this structure with a novel "scale-selective self-similarity" (S 4) descriptor which is computed at each point on the facade at its inherent scale. To achieve this, we introduce a new method for scale selection which enables the extraction and segmentation of facades as well. Matching is done with a Bayesian classification of the street-view query S4 descriptors given all labeled descriptors in the bird's-eye-view database. We show experimental results on retrieval accuracy on a challenging set of publicly available imagery and compare with standard SIFT-based techniques. {\textcopyright} 2012 Springer-Verlag.},
author = {Bansal, Mayank and Daniilidis, Kostas and Sawhney, Harpreet S.},
doi = {10.1007/978-3-642-33863-2_18},
file = {:home/nathan/Documents/Mendeley Desktop/Bansal, Daniilidis, Sawhney/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Bansal, Daniilidis, Sawhney{\_}2012{\_}Ultra-wide baseline facade matching for geo-localization.pdf:pdf},
isbn = {9783642338625},
issn = {03029743},
journal = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
number = {PART 1},
pages = {175--186},
title = {{Ultra-wide baseline facade matching for geo-localization}},
volume = {7583 LNCS},
year = {2012}
}
@inproceedings{Rosen2016,
annote = {R{\'{e}}sum{\'{e}}
Mod{\'{e}}lisation de la persistence des features environmental en fonction du temps pour des applications. Les auteurs mettent en place un filtrage probabiliste permettant de supprimer dans la map que l'on revisite certaine features en fonction de probabilit{\'{e}} {\`{a}} priori de disparition en fonction du temps pass{\'{e}}. 

Ce que je n'ai pas compris
Les proba un peu complexe, les fonctions de hasard etc.

Ce qui est interessant
C'est un systeme qui est feature-independant, c'est {\`{a}} dire que peut imaginer mod{\'{e}}liser n'importe quelle type de features
Code dispo

Critique
Les r{\'{e}}sultats de comparaison sont un peu bizarre, on ne sait pas sur quoi ils testent etc.},
author = {Rosen, David M. and Mason, Julian and Leonard, John J.},
booktitle = {Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)},
file = {:home/nathan/Documents/Mendeley Desktop/Rosen, Mason, Leonard/Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)/Rosen, Mason, Leonard{\_}2016{\_}Towards Lifelong Feature-Based Mapping in Semi-Static Environments.pdf:pdf},
pages = {1--8},
title = {{Towards Lifelong Feature-Based Mapping in Semi-Static Environments}},
url = {papers3://publication/uuid/C0E42BC5-A4C4-44D4-A0AA-ADF39AE4DAD1},
year = {2016}
}
@inproceedings{Achar2011,
abstract = {This paper introduces a vision based localization method for large scale urban environments. The method is based upon Bag-of-Words image retrieval techniques and handles problems that arise in urban environments due to repetitive scene structure and the presence of dynamic objects like vehicles. The localization system was experimentally verified it localization experiments along a 5km long path in an urban environment.},
author = {Achar, Supreeth and Jawahar, C. V. and {Madhava Krishna}, K.},
booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2011.5979925},
file = {:home/nathan/Documents/Mendeley Desktop/Achar, Jawahar, Madhava Krishna/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Achar, Jawahar, Madhava Krishna{\_}2011{\_}Large scale visual localization in urban environments.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
pages = {5642--5648},
title = {{Large scale visual localization in urban environments}},
year = {2011}
}
@article{Krajnik2010,
abstract = {This article describes a simple monocular navigation system for a mobile robot based on the map-and-replay technique. The presented method is robust and easy to implement and does not require sensor calibration or structured environment, and its computational complexity is independent of the environment size. Themethod can navigate a robot while sensing only one landmark at a time, making it more robust than other monocular approaches. The aforementioned properties of themethod allow even low-cost robots to effectively act in large outdoor and indoor environments with natural landmarks only. The basic idea is to utilize a monocular vision to correct only the robots heading, leaving distance measurements to the odometry. The heading correction itself can suppress the odometric error and prevent the overall position error from diverging. The influence of amap-based heading estimation and odometric errors on the overall position uncertainty is examined.Aclaim is stated that for closed polygonal trajectories, the position error of this type of navigation does not diverge. The claim is defended mathematically and experimentally. The method has been experimentally tested in a set of indoor and outdoor experiments, during which the average position errors have been lower than 0.3 m for paths more than 1 kmlong},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Krajn{\'{i}}k, Tom{\'{a}}{\v{s}} and Faigl, Jan and Von{\'{a}}sek, Vojtech and Ko{\v{s}}nar, Karel and Kulich, Miroslav and Preucil, Libor},
doi = {10.1002/rob.20354},
eprint = {10.1.1.91.5767},
file = {:home/nathan/Documents/Mendeley Desktop/Krajn{\'{i}}k et al/Journal of Field Robotics/Krajn{\'{i}}k et al.{\_}2010{\_}Simple yet stable bearing-only navigation.pdf:pdf},
isbn = {9783902661623},
issn = {15564959},
journal = {Journal of Field Robotics},
number = {5},
pages = {511--533},
pmid = {22164016},
title = {{Simple yet stable bearing-only navigation}},
volume = {27},
year = {2010}
}
@article{Wang2017b,
author = {Wang, Jianfeng and Hu, Xiaolin},
file = {:home/nathan/Documents/Mendeley Desktop/Wang, Hu/Unknown/Wang, Hu{\_}2017{\_}Gated Recurrent Convolution Neural Network for OCR.pdf:pdf},
number = {Nips},
title = {{Gated Recurrent Convolution Neural Network for OCR}},
year = {2017}
}
@inproceedings{Sattler2012a,
annote = {R{\'{e}}sum{\'{e}}
Article comparant deux m{\'{e}}thodes d'image-based localization : image-retriveal based method et direct matching method. Les auteurs essayent de comprendre pourquoi les methods image-retriveal based sont moins bonne bien qu'elles semble plus propice pour les grands environnements. Ils soulevent deux raison pour lesquelles ces m{\'{e}}thods sont moins bonne : le grand taux de faux positifs incorpor{\'{e}}s au moment du RANSAC qui perturbent l'estimation de pose et la limitation d'informations r{\'{e}}cup{\'{e}}r{\'{e}}es par le procesus d'image retriveal compar{\'{e}} {\`{a}} une comparaison directe des points 3D. Ils apportent une am{\'{e}}lioration au 1er probl{\`{e}}me en ajoutant au framework de BoW un selective volting au sein du visual world (hamming embedding) pour am{\'{e}}liorer le ranking des images. Pour diminuer les temps de calcul ils comparent les features par leur descripteurs de Hamming embedding (au lieu des SIFT).

Ce que je n'ai pas compris
- Le selective volting

Ce qui est int{\'{e}}ressant
- Hamming embedding
- R{\'{e}}utilisation des matches pour l'image-retriveal method pour faire le matching pour la pose regression
- Intro du papier qui pr{\'{e}}sente pas mal les diff{\'{e}}rentes m{\'{e}}thodes
- Rapide

Critiques
- Pas tr{\`{e}}s clair au niveau des explications et des notations

A approfondir
- Hamming embedding

Computational load
Offline: BoW dictonnary
Online: 40 ms

Scalability
Scale: Mutiple - place
Scalability potential: Yes more than direct matching method},
author = {Sattler, Torsten and Weyand, Tobias and Leibe, Bastian and Kobbelt, Leif},
booktitle = {British Machine Vision Conference (BMVC)},
doi = {10.5244/C.26.76},
file = {:home/nathan/Documents/Mendeley Desktop/Sattler et al/British Machine Vision Conference (BMVC)/Sattler et al.{\_}2012{\_}Image Retrieval for Image-Based Localization Revisited.pdf:pdf},
isbn = {1-901725-46-4},
pages = {76.1--76.12},
title = {{Image Retrieval for Image-Based Localization Revisited}},
url = {http://www.bmva.org/bmvc/2012/BMVC/paper076/index.html},
year = {2012}
}
@article{Janai2017,
abstract = {Recent years have witnessed amazing progress in AI related fields such as computer vision, machine learning and autonomous vehicles. As with any rapidly growing field, however, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several topic specific survey papers have been written, to date no general survey on problems, datasets and methods in computer vision for autonomous vehicles exists. This paper attempts to narrow this gap by providing a state-of-the-art survey on this topic. Our survey includes both the historically most relevant literature as well as the current state-of-the-art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding and end-to-end learning. Towards this goal, we first provide a taxonomy to classify each approach and then analyze the performance of the state-of-the-art on several challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we will also provide an interactive platform which allows to navigate topics and methods, and provides additional information and project links for each paper.},
archivePrefix = {arXiv},
arxivId = {1704.05519},
author = {Janai, Joel and G{\"{u}}ney, Fatma and Behl, Aseem and Geiger, Andreas},
doi = {10.1007/1-4020-3858-8_15},
eprint = {1704.05519},
file = {:home/nathan/Documents/Mendeley Desktop/Janai et al/arXiv preprint/Janai et al.{\_}2017{\_}Computer Vision for Autonomous Vehicles Problems, Datasets and State-of-the-Art.pdf:pdf},
journal = {arXiv preprint},
keywords = {and lowers the,autonomous vehicles,autonomous vision,by providing an exhaustive,computer vision,entry barrier for beginners,field of autonomous vision,for researchers in the,manner 1,overview,survey will become a,useful tool,we hope that our},
title = {{Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art}},
url = {http://arxiv.org/abs/1704.05519},
year = {2017}
}
@inproceedings{Krajnk2015,
annote = {in review},
author = {Krajn{\'{i}}k, Tom{\'{a}}{\v{s}} and Fentanes, Jaime P. and Santos, Joao M. and Duckett, Tom},
booktitle = {IEEE Transactions on Robotics (ToR)},
file = {:home/nathan/Documents/Mendeley Desktop/Krajn{\'{i}}k et al/IEEE Transactions on Robotics (ToR)/Krajn{\'{i}}k et al.{\_}2017{\_}FreMEn Frequency Map Enhancement for Long-Term Mobile Robot Autonomy in Changing Environments.pdf:pdf},
title = {{FreMEn: Frequency Map Enhancement for Long-Term Mobile Robot Autonomy in Changing Environments}},
year = {2017}
}
@article{Karakasis2015,
abstract = {This paper presents an image retrieval framework that uses affine image moment invariants as descriptors of local image areas. Detailed feature vectors are generated by feeding the produced moments into a Bag-of-Visual-Words representation. Image moment invariants have been selected for their compact representation of image areas as well as due to their ability to remain unchanged under affine image transformations. Three different setups were examined in order to evaluate and discuss the overall approach. The retrieval results are promising compared with other widely used local descriptors, allowing the proposed framework to serve as a reference point for future image moment local descriptors applied to the general task of content based image retrieval.},
author = {Karakasis, E. G. and Amanatiadis, A. and Gasteratos, Antonios and Chatzichristofis, S. A.},
doi = {10.1016/j.patrec.2015.01.005},
file = {:home/nathan/Documents/Mendeley Desktop/Karakasis et al/Pattern Recognition Letters/Karakasis et al.{\_}2015{\_}Image moment invariants as local features for content based image retrieval using the Bag-of-Visual-Words model.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Affine moment invariants,Bag of visual words,Content based image retrieval},
pages = {22--27},
publisher = {Elsevier Ltd.},
title = {{Image moment invariants as local features for content based image retrieval using the Bag-of-Visual-Words model}},
url = {http://dx.doi.org/10.1016/j.patrec.2015.01.005},
volume = {55},
year = {2015}
}
@article{Manek2017,
abstract = {In this work, we focus on the problem of image instance retrieval with deep descriptors extracted from pruned Convolutional Neural Networks (CNN). The objective is to heavily prune convolutional edges while maintaining retrieval performance. To this end, we introduce both data-independent and data-dependent heuristics to prune convolutional edges, and evaluate their performance across various compression rates with different deep descriptors over several benchmark datasets. Further, we present an end-to-end framework to fine-tune the pruned network, with a triplet loss function specially designed for the retrieval task. We show that the combination of heuristic pruning and fine-tuning offers 5x compression rate without considerable loss in retrieval performance.},
archivePrefix = {arXiv},
arxivId = {1707.05455},
author = {Manek, Gaurav and Lin, Jie and Chandrasekhar, Vijay and Duan, Ling-Yu and Giduthuri, Sateesh and Li, Xiaoli and Poggio, Tomaso},
eprint = {1707.05455},
file = {:home/nathan/Documents/Mendeley Desktop/Manek et al/arXiv preprint/Manek et al.{\_}2017{\_}Pruning Convolutional Neural Networks for Image Instance Retrieval.pdf:pdf},
journal = {arXiv preprint},
pages = {1--5},
title = {{Pruning Convolutional Neural Networks for Image Instance Retrieval}},
url = {http://arxiv.org/abs/1707.05455},
year = {2017}
}
@inproceedings{Schonberger2017,
annote = {R{\'{e}}sum{\'{e}}
Papier comparant des descripteurs locaux hand-cradted et learned pour la t{\^{a}}che de matching de point d'int{\'{e}}ret, avec un focus sur la reconstruction. Les auteurs on s{\'{e}}l{\'{e}}ctionn{\'{e}} pluseurs types de state of the art feature : SIFT et deux variantes r{\'{e}}centes, ainsi que 4 features bas{\'{e}} sur du deep learning. Les features permette de d{\'{e}}crire un point d'int{\'{e}}ret (sauf LIFT qui le d{\'{e}}tecte aussi). Ils mettent en avant qu'il n'y a pas vraiment d{\'{e}}cart en fonction de la tache consid{\'{e}}r{\'{e}} dans les performances entre hand-crafted et learned features, et surtout que les CNN features sont plus couteuse a extraire.

Ce que je n'ai pas compris
- Pas lu dans les d{\'{e}}tailes a fond

Ce qui est int{\'{e}}ressant
- L'article,
- Les m{\'{e}}thodes de SfM cit{\'{e}}es},
author = {Sch{\"{o}}nberger, Johannes L. and Hardmeier, Hans and Sattler, Torsten and Pollefeys, Marc},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Sch{\"{o}}nberger et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Sch{\"{o}}nberger et al.{\_}2017{\_}Comparative Evaluation of Hand-Crafted and Learned Local Features.pdf:pdf},
number = {January},
title = {{Comparative Evaluation of Hand-Crafted and Learned Local Features}},
year = {2017}
}
@inproceedings{Weyand2016,
annote = {R{\'{e}}sum{\'{e}}
CNN entrain{\'{e}} dans le but de donner la localisation d'une image au niveau mondial. C'est un r{\'{e}}seau qui fait de la classification (sur 26k classes qui sont associ{\'{e}}es {\`{a}} diff{\'{e}}rentes parties du globe, 126 million},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Weyand, Tobias and Kostrikov, Ilya and Philbin, James},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-319-46448-0},
eprint = {1311.2901},
file = {:home/nathan/Documents/Mendeley Desktop/Weyand, Kostrikov, Philbin/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Weyand, Kostrikov, Philbin{\_}2016{\_}PlaNet - Photo Geolocation with Convolutional Neural Networks.pdf:pdf},
isbn = {978-3-319-46447-3},
issn = {0302-9743},
pages = {37--55},
title = {{PlaNet - Photo Geolocation with Convolutional Neural Networks}},
url = {http://link.springer.com/10.1007/978-3-319-46448-0},
volume = {9905},
year = {2016}
}
@inproceedings{Chen2011,
abstract = {With recent advances in mobile computing, the demand for visual localization or landmark identification on mobile devices is gaining interest. We advance the state of the art in this area by fusing two popular representations of street-level image data {\&}{\#}x2014; facade-aligned and viewpoint-aligned {\&}{\#}x2014; and show that they contain complementary information that can be exploited to significantly improve the recall rates on the city scale. We also improve feature detection in low contrast parts of the street-level data, and discuss how to incorporate priors on a user's position (e.g. given by noisy GPS readings or network cells), which previous approaches often ignore. Finally, and maybe most importantly, we present our results according to a carefully designed, repeatable evaluation scheme and make publicly available a set of 1.7 million images with ground truth labels, geotags, and calibration data, as well as a difficult set of cell phone query images. We provide these resources as a benchmark to facilitate further research in the area.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de localisation de landmark (des batiments si j'ai bien compris) a partir d'une image. Intruduction d'une grosse base de donn{\'{e}}es d'images sph{\'{e}}riques avec mod{\`{e}}les 3D de batiments simplifi{\'{e}}s associ{\'{e}}s disponible. M{\'{e}}thode de localisation bas{\'{e}}e sur l'utilisation conjointe de deux vocabulary trees construit sur deux types d'images diff{\'{e}}rentes (l'une extraite directement sur l'image sph{\'{e}}rique et l'autre r{\'{e}}ctifi{\'{e}}e pour qu'elle soit align{\'{e}}e avec la facades du batiment). Possibilit{\'{e}} d'utiliser un marquage GPS pour filters les potentiels candidats hors de port{\'{e}} de la query.

Ce que je n'ai pas compris
- Comment g{\'{e}}n{\'{e}}rer les deux types d'images
- Quelle type de descripteurs "upright" ils utilisent

Ce qui est int{\'{e}}ressant
- La base de donn{\'{e}}es introduite
- La volont{\'{e}} de ne pas prendre en compte la variation d'orientation (ou de la corriger lors de la query)
- La m{\'{e}}thode pour {\'{e}}liminer la non robustesse au changement d'illumination

Critiques
- Pas de r{\'{e}}elles nouveaut{\'{e}}s apport{\'{e}}
- Utilise une connaissance {\`{a}} priori de la postion (GPS)

A approfondir
- Upright feature descriptor
- Database
- Comparaison d'images orthoalign{\'{e}}es

Computational load
Offline: Construction de vocabulary tree
Online: Double query (mais extraction de upright feature + rapide)

Scalability
Scale: City
Scalability potential: Probleme de quantization},
author = {Chen, David M. and Baatz, Georges and K{\"{o}}ser, Kevin and Tsai, Sam S. and Vedantham, Ramakrishna and Pylv{\"{a}}n{\"{a}}inen, Timo and Roimela, Kimmo and Chen, Xin and Bach, Jeff and Pollefeys, Marc and Girod, Bernd and Grzeszczuk, Radek},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2011.5995610},
file = {:home/nathan/Documents/Mendeley Desktop/Chen et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Chen et al.{\_}2011{\_}City-scale landmark identification on mobile devices.pdf:pdf},
isbn = {9781457703942},
issn = {10636919},
keywords = {Local features,Outdoor,RGB request,Visual place recognition,Visual words},
mendeley-tags = {Local features,Outdoor,RGB request,Visual place recognition,Visual words},
pages = {737--744},
title = {{City-scale landmark identification on mobile devices}},
year = {2011}
}
@article{Zhang2017c,
abstract = {Location-aware applications play an increasingly critical role in everyday life. However, the most common global localization technology - GPS - has limited accuracy and can be unusable in dense urban areas and indoors. We introduce an image-based global localization system that is accurate to a few millimeters and performs reliable localization both indoors and outside. The key idea is to capture and index distinctive local features in ground textures. This is based on the observation that ground textures including wood, carpet, tile, concrete, and asphalt may look random and homogeneous, but all contain cracks, scratches, or unique arrangements of carpet fibers. These imperfections are persistent, and can serve as local features. Our system incorporates a downward-facing camera to capture the fine texture of the ground, together with an image processing pipeline that locates the captured texture patch in a compact database constructed offline. We demonstrate the capability of our system to robustly, accurately, and quickly locate test images on various types of outdoor and indoor ground surfaces.},
archivePrefix = {arXiv},
arxivId = {1710.10687},
author = {Zhang, Linguang and Finkelstein, Adam and Rusinkiewicz, Szymon},
eprint = {1710.10687},
file = {:home/nathan/Documents/Mendeley Desktop/Zhang, Finkelstein, Rusinkiewicz/Unknown/Zhang, Finkelstein, Rusinkiewicz{\_}2017{\_}High-Precision Localization Using Ground Texture.pdf:pdf},
number = {1},
title = {{High-Precision Localization Using Ground Texture}},
url = {http://arxiv.org/abs/1710.10687},
volume = {1},
year = {2017}
}
@article{Nelson2015,
abstract = {This paper is about localising at night in urban environments using vision. Despite it being dark exactly half of the time, surprisingly little attention has been given to this problem. A defining aspect of night-time urban scenes is the presence and effect of artificial lighting – be that in the form of street or interior lighting through windows. By building a model of the environment which includes a representation of the spatial location of every light source, localisation becomes possible using monocular cameras. One of the challenges we face is the gross change in light appearance as a function of distance due to flare, saturation and bleeding – city lights certainly do not appear as point features. To overcome this, we model the appearance of each light as a function of vehicle location, using this to inform our data-association decisions and to regularise the cost function which is used to infer vehicle pose. In this way we develop a place-dependent but stable sensor model which is customised for the particular environment in which we are operating. We demonstrate that our system is able to localise successfully at night over 12 km in situations where a traditional point feature based system fails.},
author = {Nelson, Peter and Churchill, Winston and Posner, Ingmar and Newman, Paul},
doi = {10.1109/ICRA.2015.7139930},
file = {:home/nathan/Documents/Mendeley Desktop/Nelson et al/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Nelson et al.{\_}2015{\_}From Dusk till Dawn Localisation at Night using Artificial Light Sources.pdf:pdf},
isbn = {9781479969234},
issn = {1050-4729},
journal = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
pages = {5245--5252},
title = {{From Dusk till Dawn: Localisation at Night using Artificial Light Sources}},
year = {2015}
}
@inproceedings{Svarm2014,
author = {Svarm, Linus and Enqvist, Olof and Kahl, Fredrik and Oskarsson, Magnus},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2014.75},
file = {:home/nathan/Documents/Mendeley Desktop/Svarm et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Svarm et al.{\_}2014{\_}Accurate Localization and Pose Estimation for Large 3D Models.pdf:pdf},
title = {{Accurate Localization and Pose Estimation for Large 3D Models}},
year = {2014}
}
@inproceedings{Arandjelovic2016,
abstract = {We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the "Vector of Locally Aggregated Descriptors" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture obtains a large improvement in performance over non-learnt image representations as well as significantly outperforms off-the-shelf CNN descriptors on two challenging place recognition benchmarks.},
annote = {R{\'{e}}sum{\'{e}}

CNN permettant de g{\'{e}}n{\'{e}}rer un descripteur discriminatif associ{\'{e}} {\`{a}} un lieu en s'insiprant de l'agglom{\'{e}}ration de features de VLAD. Les auteurs soulignent trois avanc{\'{e}}s : la construction d'une couche du CNN NetVLAD g{\'{e}}n{\'{e}}rant le descripteur, la cr{\'{e}}ation de long terme dataset grace {\`{a}} google street view, et la m{\'{e}}thode d'apprentissage (loss function) pour entrainer le NetVLAD.

Ce que je n'ai pas compris

- Je n'ai pas vu quelle m{\'{e}}thode ils utilisent pour retrouver les images similaire apr{\`{e}}s avoir g{\'{e}}n{\'{e}}r{\'{e}} le descripteur (euclidian distance ?)
- Qu'est ce que l'op{\'{e}}ration de "whitening"
- Pourquoi il y a des queries pour la base de training {\&} validation ?
- Pourquoi ils disent que c'est geographically disjoint ?
- Comment ils trouvent les clusters pour le VLAD ?

Ce qui est int{\'{e}}ressant

- Outperform toutes les autres m{\'{e}}thodes
- La base de donn{\'{e}}e Google
- Etat de l'art cons{\'{e}}quent

Critiques

- Pas de temps de calcul
- Pas de recallage

A approfondir

- Les r{\'{e}}seaux de neurones
- Une m{\'{e}}thode pour faire un recallage apr{\`{e}}s avoir retrouv{\'{e}} l'image correspondante

Computational load
Offline: Trainded CNN + hard negative training (30 epochs)
Online: Pas d'info sur la m{\'{e}}thode de comparaison des vecteurs (4096-D)

Scalability
Scale: District scale
Scalability potential: ca depend du nombre de donn{\'{e}}es a comparer et de la m{\'{e}}thode de compariason},
archivePrefix = {arXiv},
arxivId = {1511.07247},
author = {Arandjelovi{\'{c}}, Relja and Gronat, Petr and Torii, Akihiko and Pajdla, Tomas and Sivic, Josef},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.572},
eprint = {1511.07247},
file = {:home/nathan/Documents/Mendeley Desktop/Arandjelovi{\'{c}} et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Arandjelovi{\'{c}} et al.{\_}2016{\_}NetVLAD CNN architecture for weakly supervised place recognition.pdf:pdf},
isbn = {9781467388511},
issn = {10636919},
keywords = {CNN,Challenging matching,Machine learning,Outdoor,RGB request,Supervised learning,View Synthesis,Visual place recognition},
mendeley-tags = {CNN,Challenging matching,Machine learning,Outdoor,RGB request,Supervised learning,View Synthesis,Visual place recognition},
pages = {5297--5307},
title = {{NetVLAD: CNN architecture for weakly supervised place recognition}},
url = {http://arxiv.org/abs/1511.07247},
year = {2016}
}
@article{Ganin2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1505.07818v4},
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
eprint = {arXiv:1505.07818v4},
file = {:home/nathan/Documents/Mendeley Desktop/Ganin et al/Journal of Machine Learning Research/Ganin et al.{\_}2016{\_}Domain-Adversarial Training of Neural Networks.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {1--35},
title = {{Domain-Adversarial Training of Neural Networks}},
volume = {17},
year = {2016}
}
@article{Middelberg2014,
abstract = {Recent improvements in image-based localization have produced powerful methods that scale up to the massive 3D models emerging from modern Structure-from-Motion techniques. However, these approaches are too resource intensive to run in real-time, let alone to be implemented on mobile devices. In this paper, we propose to combine the scalability of such a global localization system running on a server with the speed and precision of a local pose tracker on a mobile device. Our approach is both scalable and drift-free by design and eliminates the need for loop closure. We propose two strategies to combine the information provided by local tracking and global localization. We evaluate our system on a large-scale dataset of the historic inner city of Aachen where it achieves interactive framerates at a localization error of less than 50cm while using less than 5MB of memory on the mobile device.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de localisation globale pour appareil portable se basant sur l'utilisation conjointe d'un serveur de localisation global et d'un tracking en temps r{\'{e}}el embarqu{\'{e}} sur l'appareil. Le tracking est inspir{\'{e}} d'un PTAM monoculaire avec fusion de donn{\'{e}}es des informations proprioseptives de l'appareil (detecteur et descripteur BRISK, et BA a chaque nouvelle keyframe). La m{\'{e}}thode de localisation global fonctionnant en d{\'{e}}port{\'{e}} est celle pr{\'{e}}sent{\'{e}} dans Sattler et al. 2012 (RootSIFT {\&} SfM). Les auteurs pr{\'{e}}sentent deux m{\'{e}}thodes de fusion des donn{\'{e}}es globales et locales de leur deux syst{\`{e}}mes, en proc{\'{e}}dant {\`{a}} la fusion durant la phase de bundle adjustement, soit dans un premier temps en prenant en compte directement la position global, soit dans une autre m{\'{e}}thode en prenant en compte les matchings obtenu par le syst{\`{e}}me de localisation globale.

Ce que je n'ai pas compris
- Les d{\'{e}}tails du BA

Ce qui est int{\'{e}}ressant
- Rapide et sans drift
- Le BA modifi{\'{e}}

Critiques
- Pas de r{\'{e}}el nouveaut{\'{e}}, mise {\`{a}} part dans la fusion des positions

A approfondir
- BA

Computational load
Offline: Voir Sattler et al. 2012
Online: 100 ms

Scalability
Scale: Multiple places
Scalability potential: Possible thanks to the 2 lvl systeme},
author = {Middelberg, Sven and Sattler, Torsten and Untzelmann, Ole and Kobbelt, Leif},
doi = {10.1007/978-3-319-10605-2_18},
file = {:home/nathan/Documents/Mendeley Desktop/Middelberg et al/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Middelberg et al.{\_}2014{\_}Scalable 6-DOF localization on mobile devices.pdf:pdf},
isbn = {9783319106045},
issn = {16113349},
journal = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
number = {PART 2},
pages = {268--283},
title = {{Scalable 6-DOF localization on mobile devices}},
volume = {8690 LNCS},
year = {2014}
}
@inproceedings{Gupta2016a,
abstract = {In this work we propose a technique that transfers supervision between images from different modalities. We use learned representations from a large labeled modality as a supervisory signal for training representations for a new unlabeled paired modality. Our method enables learning of rich representations for unlabeled modalities and can be used as a pre-training procedure for new modalities with limited labeled data. We show experimental results where we transfer supervision from labeled RGB images to unlabeled depth and optical flow images and demonstrate large improvements for both these cross modal supervision transfers. Code, data and pre-trained models are available at https://github.com/s-gupta/fast-rcnn/tree/distillation},
annote = {Les auteurs font du transfert learning entre modalit{\'{e}}. En gros ils ont un bon d{\'{e}}tecteur d'objet dans une modalit{\'{e}} (RGB), et ils veulent avoir un bon d{\'{e}}tecteur d'objet dans une autre modalit{\'{e}} (depth), et ils ont des paires d'image de modalit{\'{e}}s diff{\'{e}}rentes. Ils font leur supervision learning sur 5K paires RGB-D avec une loss pour le CNN Depth qui p{\'{e}}nalise la diff{\'{e}}rence de r{\'{e}}ponse au niveau d'un layer convolutionel entre le CNN depth et CNN RGB (le mieu est les layers les plus haut, conv5). La loss est la norme L2 entre les features maps. Une fois le r{\'{e}}seau Depth pr{\'{e}}-entrain{\'{e}}, il le fine tune sur la task de localisation d'objet en utilisant les donn{\'{e}}es lab{\'{e}}lis{\'{e}}es (dans le domain Depth). Ils montrent que ca marche bien, m{\^{e}}me si apr{\`{e}}s on utilise apr{\`{e}}s les deux modalit{\'{e}}es {\`{a}} la fois.},
archivePrefix = {arXiv},
arxivId = {1507.00448},
author = {Gupta, Saurabh and Hoffman, Judy and Malik, Jitendra},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.309},
eprint = {1507.00448},
file = {:home/nathan/Documents/Mendeley Desktop/Gupta, Hoffman, Malik/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Gupta, Hoffman, Malik{\_}2016{\_}Cross Modal Distillation for Supervision Transfer.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
title = {{Cross Modal Distillation for Supervision Transfer}},
url = {http://arxiv.org/abs/1507.00448},
year = {2016}
}
@inproceedings{Chen2015,
author = {Chen, Yi and Qian, Gang and Gunda, Kiran and Gupta, Himaanshu and Shafique, Khurram},
booktitle = {Proceedings of the International Conference on Information Fusion (Fusion)},
file = {:home/nathan/Documents/Mendeley Desktop/Chen et al/Proceedings of the International Conference on Information Fusion (Fusion)/Chen et al.{\_}2015{\_}Camera geolocation from mountain images.pdf:pdf},
isbn = {9780996452717},
keywords = {cameras,digital elevation models,estimation theory},
pages = {1587--1596},
title = {{Camera geolocation from mountain images}},
year = {2015}
}
@inproceedings{Sattler2016,
abstract = {Visual location recognition is the task of determining the place depicted in a query image from a given database of geo-tagged images. Location recognition is often cast as an image retrieval problem and recent research has almost ex-clusively focused on improving the chance that a relevant database image is ranked high enough after retrieval. The implicit assumption is that the number of inliers found by spatial verification can be used to distinguish between a related and an unrelated database photo with high preci-sion. In this paper, we show that this assumption does not hold for large datasets due to the appearance of geomet-ric bursts, i.e., sets of visual elements appearing in similar geometric configurations in unrelated database photos. We propose algorithms for detecting and handling geometric bursts. Although conceptually simple, using the proposed weighting schemes dramatically improves the recall that can be achieved when high precision is required compared to the standard re-ranking based on the inlier count. Our approach is easy to implement and can easily be integrated into existing location recognition systems.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode peremettant d'am{\'{e}}liorer le classement des inliers issue d'une {\'{e}}tape de reconnaissance de lieu se basant sur l'identification de geometric burst. Le principe est de donn{\'{e}}es des poids aux features d{\'{e}}crivant l'image de requet en fonction de leur robustesse g{\'{e}}ometrique et de la situation g{\'{e}}ographique des inliers.

Ce que je n'ai pas compris
- Le principe de geometric burst (en comparaison au visual burst)
- Le calcul des poids pour la partie inter-image (c'est les poids de quoi des features ou des images ?)
- Le calcul des poids pour la partie inter-place
- Ce que c'est que le raw inlier count et le effective inlier count

Ce qui est int{\'{e}}ressant
- Les r{\'{e}}sultats sont tr{\`{e}}s interessant, {\c{c}}a {\`{a}} l'air d'am{\'{e}}liorer pas mal le systeme avec une simple action au moment du ranking
- La m{\'{e}}thode de clustering des places
- L'{\'{e}}tat de l'art ({\&} les dataset utilis{\'{e}}s)
- le mAP

Critiques
- Si j'ai bien compris c'est que des m{\'{e}}thodes de description local qu'ils comparent ({\`{a}} base de visual world)
- Pas de nouvelle m{\'{e}}thode de place recogniton

A approfondir
- kmean++
- les database
- le code dispo sur git
- plein de m{\'{e}}thodes evoqu{\'{e}}es dans le papier

Computational load
Offline: -
Online: little

Scalability
Scale: -
Scalability potential: improve actual method},
author = {Sattler, Torsten and Havlena, Michal and Schindler, Konrad and Pollefeys, Marc},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Sattler et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Sattler et al.{\_}2016{\_}Large-Scale Location Recognition and the Geometric Burstiness Problem.pdf:pdf},
keywords = {Cluster,Local features,Outdoor,RGB request,Ranking,Visual place recognition},
mendeley-tags = {Cluster,Local features,Outdoor,RGB request,Ranking,Visual place recognition},
title = {{Large-Scale Location Recognition and the Geometric Burstiness Problem}},
year = {2016}
}
@article{Jegou2010,
abstract = {We address the problem of image search on a very large scale, where three constraints have to be considered jointly: the accuracy of the search, its efficiency, and the memory usage of the representation. We first propose a simple yet efficient way of aggregating local image descriptors into a vector of limited dimension, which can be viewed as a simplification of the Fisher kernel representation. We then show how to jointly optimize the dimension reduction and the indexing algorithm, so that it best preserves the quality of vector comparison. The evaluation shows that our approach significantly outperforms the state of the art: the search accuracy is comparable to the bag-of-features approach for an image representation that fits in 20 bytes. Searching a 10 million image dataset takes about 50ms.},
author = {J{\'{e}}gou, Herv{\'{e}} and Douze, Matthijs and Schmid, Cordelia and P{\'{e}}rez, Patrick},
doi = {10.1109/CVPR.2010.5540039},
file = {:home/nathan/Documents/Mendeley Desktop/J{\'{e}}gou et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/J{\'{e}}gou et al.{\_}2010{\_}Aggregating local descriptors into a compact image representation.pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {3304--3311},
pmid = {22156101},
title = {{Aggregating local descriptors into a compact image representation}},
year = {2010}
}
@article{Song2017,
abstract = {Fault-tolerant distributed algorithms play an important role in many critical/high-availability applications. These algorithms are notori-ously difficult to implement correctly, due to asynchronous com-munication and the occurrence of faults, such as the network drop-ping messages or computers crashing. We introduce PSYNC, a domain specific language based on the Heard-Of model, which views asynchronous faulty systems as syn-chronous ones with an adversarial environment that simulates asyn-chrony and faults by dropping messages. We define a runtime sys-tem for PSYNC that efficiently executes on asynchronous networks. We formalize the relation between the runtime system and PSYNC in terms of observational refinement. The high-level lockstep ab-straction introduced by PSYNC simplifies the design and imple-mentation of fault-tolerant distributed algorithms and enables auto-mated formal verification. We have implemented an embedding of PSYNC in the SCALA programming language with a runtime system for asynchronous networks. We show the applicability of PSYNC by implementing several important fault-tolerant distributed algorithms and we com-pare the implementation of consensus algorithms in PSYNC against implementations in other languages in terms of code size, runtime efficiency, and verification.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.09436},
author = {Song, Jingkuan},
doi = {10.1145/nnnnnnn.nnnnnnn},
eprint = {arXiv:1603.09436},
file = {:home/nathan/Documents/Mendeley Desktop/Song/arXiv preprint/Song{\_}2017{\_}Binary Generative Adversarial Networks for Image Retrieval Jingkuan.pdf:pdf},
isbn = {9781450335492},
issn = {16130073},
journal = {arXiv preprint},
keywords = {Cognitive embodiment,Educational technology,Gesture recognition,One-shot machine learning},
pages = {89--93},
pmid = {397311},
title = {{Binary Generative Adversarial Networks for Image Retrieval Jingkuan}},
volume = {1828},
year = {2017}
}
@inproceedings{Kroeger2014,
abstract = {Registering image data to Structure from Motion (SfM) point clouds is widely used to find precise camera location and orientation with respect to a world model. In case of videos one constraint has previously been unexploited: temporal smoothness. Without temporal smoothness the magnitude of the pose error in each frame of a video will often dominate the magnitude of frame-to-frame pose change. This hinders application of methods requiring stable poses estimates (e.g. tracking, augmented reality). We incorporate temporal constraints into the image-based registration setting and solve the problem by pose regularization with model fitting and smoothing methods. This leads to accurate, gap-free and smooth poses for all frames. We evaluate different methods on challenging synthetic and real street-view SfM data for varying scenarios of motion speed, outlier contamination, pose estimation failures and 2D-3D correspondence noise. For all test cases a 2 to 60-fold reduction in root mean squared (RMS) positional error is observed, depending on pose estimation difficulty. For varying scenarios, different methods perform best. We give guidance which methods should be preferred depending on circumstances and requirements.},
author = {Kroeger, Till and {Van Gool}, Luc},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-319-10602-1_1},
file = {:home/nathan/Documents/Mendeley Desktop/Kroeger, Van Gool/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Kroeger, Van Gool{\_}2014{\_}Video registration to SfM models.pdf:pdf},
isbn = {978-3-319-10601-4},
issn = {16113349},
number = {PART 5},
pages = {1--16},
title = {{Video registration to SfM models}},
volume = {8693 LNCS},
year = {2014}
}
@article{Zeng2016,
abstract = {Establishing correspondences between 3D geometries is essential to a large variety of graphics and vision applications, including 3D reconstruction, localization, and shape matching. Despite significant progress, geometric matching on real-world 3D data is still a challenging task due to the noisy, low-resolution, and incomplete nature of scanning data. These difficulties limit the performance of current state-of-art methods which are typically based on histograms over geometric properties. In this paper, we introduce 3DMatch, a data-driven local feature learner that jointly learns a geometric feature representation and an associated metric function from a large collection of real-world scanning data. We represent 3D geometry using accumulated distance fields around key-point locations. This representation is suited to handle noisy and partial scanning data, and concurrently supports deep learning with convolutional neural networks directly in 3D. To train the networks, we propose a way to automatically generate correspondence labels for deep learning by leveraging existing RGB-D reconstruction algorithms. In our results, we demonstrate that we are able to outperform state-of-the-art approaches by a significant margin. In addition, we show the robustness of our descriptor in a purely geometric sparse bundle adjustment pipeline for 3D reconstruction.},
archivePrefix = {arXiv},
arxivId = {1603.08182},
author = {Zeng, Andy and Song, Shuran and Nie{\ss}ner, Matthias and Fisher, Matthew and Xiao, Jianxiong},
eprint = {1603.08182},
file = {:home/nathan/Documents/Mendeley Desktop/Zeng et al/arXiv preprint/Zeng et al.{\_}2016{\_}3DMatch Learning the Matching of Local 3D Geometry in Range Scans.pdf:pdf},
journal = {arXiv preprint},
pages = {13},
title = {{3DMatch: Learning the Matching of Local 3D Geometry in Range Scans}},
url = {http://arxiv.org/abs/1603.08182},
year = {2016}
}
@inproceedings{Jegou2009,
abstract = {Burstiness, a phenomenon initially observed in text retrieval, is the property that a given visual element appears more times in an image than a statistically independent model would predict. In the context of image search, burstiness corrupts the visual similarity measure, i.e., the scores used to rank the images. In this paper, we propose a strategy to handle visual bursts for bag-of-features based image search systems. Experimental results on three reference datasets show that our method significantly and consistently outperforms the state of the art.},
annote = {R{\'{e}}sum{\'{e}}
Am{\'{e}}lioration d'un framework bag of feature pour diminuer l'effet de burst dans l'image (= plusieurs fois le m{\^{e}}me descripteurs dans une image (intra) ou dans une base de donn{\'{e}}e d'images (inter) ). Les auteurs partent de le pr{\'{e}}c{\'{e}}dente impl{\'{e}}mentation (J{\'{e}}gou et al. 2008) qui introduit l'hamming embedding (subdivision des cellules de voronoi cr{\'{e}}{\'{e}}es par kmean et codage binaire) et la weak geometric constraint et modifie la m{\'{e}}thode d'attribution aux poids des features. Les auteurs pr{\'{e}}sentent ensuite deux am{\'{e}}liorations pour r{\'{e}}duire le burstiness problem : pour le burst intra ils p{\'{e}}nalysent les features de la query associ{\'{e}} {\`{a}} plusieurs feature d'une image requete lors du calcul de score du matching, pour le burst inter ils p{\'{e}}nalysent les features associ{\'{e}}s {\`{a}} diff{\'{e}}rents mots de la base de donn{\'{e}}e.

Ce qui est int{\'{e}}ressant
- Overhead calculatoire par trop {\'{e}}lev{\'{e}}

A approfondir
- Weak geometric constraint


Computational load
Offline: ?
Online: {\~{}}6sec pour retrouver le plus proche voisin parmi un million d'image

Scalability
Scale: City
Scalability potential: -},
author = {J{\'{e}}gou, Herv{\'{e}} and Douze, Matthijs and Schmid, Cordelia},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2009.5206609},
file = {:home/nathan/Documents/Mendeley Desktop/J{\'{e}}gou, Douze, Schmid/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)/J{\'{e}}gou, Douze, Schmid{\_}2009{\_}On the burstiness of visual elements.pdf:pdf},
isbn = {9781424439935},
issn = {1063-6919},
pages = {1169--1176},
title = {{On the burstiness of visual elements}},
year = {2009}
}
@article{Caselitz2016,
abstract = {Localizing a camera with respect to a given map is a fundamental problem in vision-based navigation. Real- world applications require methods that are capable of long-term operation because recordings of available maps often date back considerably compared to the time of localization. Unfor- tunately, the photometric appearance of the environment can change tremendously even over short periods, making image matching a difficult problem. Since geometric properties of the environment are typically more stable than its photometric characteristics, we propose to rely on matching geometry in order to achieve long-term camera localization. Thus, our approach is agnostic to changes in photometric appearance. We present real-world experiments which demonstrate that our method accurately tracks the 6-DoF pose of a camera over long trajectories and under varying conditions.We evaluate our method using publicly available and own datasets and visualize the successful tracking by post-colorizing the given geometric map.},
author = {Caselitz, Tim and Steder, Bastian and Ruhnke, Michael and Burgard, Wolfram},
file = {:home/nathan/Documents/Mendeley Desktop/Caselitz et al/Proceedings of the IEEE International Conference of Robotics and Automation Workshop (ICRAW)/Caselitz et al.{\_}2016{\_}Matching Geometry for Long-term Monocular Camera Localization.pdf:pdf},
journal = {Proceedings of the IEEE International Conference of Robotics and Automation Workshop (ICRAW)},
title = {{Matching Geometry for Long-term Monocular Camera Localization}},
url = {https://sites.google.com/site/icra2016ailta/accepted-papers},
year = {2016}
}
@article{Zhang2017,
abstract = {This paper summarise and analyse the cross-dataset recognition techniques with the emphasize on what kinds of methods can be used when the available source and target data are presented in different forms for boosting the target task. This paper for the first time summarises several transferring criteria in details from the concept level, which are the key bases to guide what kind of knowledge to transfer between datasets. In addition, a taxonomy of cross-dataset scenarios and problems is proposed according the properties of data that define how different datasets are diverged, thereby review the recent advances on each specific problem under different scenarios. Moreover, some real world applications and corresponding commonly used benchmarks of cross-dataset recognition are reviewed. Lastly, several future directions are identified.},
archivePrefix = {arXiv},
arxivId = {1705.04396},
author = {Zhang, Jing and Li, Wanqing and Ogunbona, Philip},
eprint = {1705.04396},
file = {:home/nathan/Documents/Mendeley Desktop/Zhang, Li, Ogunbona/arXiv preprint/Zhang, Li, Ogunbona{\_}2017{\_}Cross-Dataset Recognition A Survey.pdf:pdf},
journal = {arXiv preprint},
title = {{Cross-Dataset Recognition: A Survey}},
url = {http://arxiv.org/abs/1705.04396},
year = {2017}
}
@article{Zamir2016,
author = {Zamir, Amir Roshan and Hakeem, Asaad and {Van Gool}, Luc and Shah, Mubarak and Szeliski, Richard},
journal = {Advances in computer vision and pattern recognition},
publisher = {Springer},
title = {{Large-scale visual geo-localization}},
year = {2016}
}
@article{Zhou2017,
abstract = {The explosive increase and ubiquitous accessibility of visual data on the Web have led to the prosperity of research activity in image search or retrieval. With the ignorance of visual content as a ranking clue, methods with text search techniques for visual retrieval may suffer inconsistency between the text words and visual content. Content-based image retrieval (CBIR), which makes use of the representation of visual content to identify relevant images, has attracted sustained attention in recent two decades. Such a problem is challenging due to the intention gap and the semantic gap problems. Numerous techniques have been developed for content-based image retrieval in the last decade. The purpose of this paper is to categorize and evaluate those algorithms proposed during the period of 2003 to 2016. We conclude with several promising directions for future research.},
archivePrefix = {arXiv},
arxivId = {1706.06064},
author = {Zhou, Wengang and Li, Houqiang and Tian, Qi},
eprint = {1706.06064},
file = {:home/nathan/Documents/Mendeley Desktop/Zhou, Li, Tian/arXiv preprint/Zhou, Li, Tian{\_}2017{\_}Recent Advance in Content-based Image Retrieval A Literature Survey.pdf:pdf},
journal = {arXiv preprint},
pages = {1--24},
title = {{Recent Advance in Content-based Image Retrieval: A Literature Survey}},
url = {http://arxiv.org/abs/1706.06064},
year = {2017}
}
@article{Chum2007,
abstract = {Given a query image of an object, our objective is to retrieve all instances of that object in a large (1M+) image database. We adopt the bag-of-visual-words architecture which has proven successful in achieving high precision at low recall. Unfortunately, feature detection and quantization are noisy processes and this can result in variation in the particular visual words that appear in different images of the same object, leading to missed results. In the text retrieval literature a standard method for improving performance is query expansion. A number of the highly ranked documents from the original query are reissued as a new query. In this way, additional relevant terms can be added to the query. This is a form of blind rele- vance feedback and it can fail if 'outlier' (false positive) documents are included in the reissued query. In this paper we bring query expansion into the visual domain via two novel contributions. Firstly, strong spatial constraints between the query image and each result allow us to accurately verify each return, suppressing the false positives which typically ruin text-based query expansion. Secondly, the verified images can be used to learn a latent feature model to enable the controlled construction of expanded queries. We illustrate these ideas on the 5000 annotated image Oxford building database together with more than 1M Flickr images. We show that the precision is substantially boosted, achieving total recall in many cases.},
author = {Chum, Ondřej and Philbin, James and Sivic, Josef and Isard, Michael and Zisserman, Andrew},
doi = {10.1109/ICCV.2007.4408891},
file = {:home/nathan/Documents/Mendeley Desktop/Chum et al/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Chum et al.{\_}2007{\_}Total recall Automatic query expansion with a generative feature model for object retrieval.pdf:pdf},
isbn = {978-1-4244-1630-1},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
title = {{Total recall: Automatic query expansion with a generative feature model for object retrieval}},
year = {2007}
}
@inproceedings{Nister2006,
abstract = {A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD{\&}{\#}146;s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.},
annote = {From Duplicate 1 (Scalable recognition with a vocabulary tree - Nist{\'{e}}r, David; Stew{\'{e}}nius, Henrik)

R{\{}{\'{e}}{\}}sum{\{}{\'{e}}{\}}
M{\{}{\'{e}}{\}}thode utilis{\{}{\'{e}}{\}}e pour de la classification d'objet dans une image, se basant sur les Vocabulary Tree. A partir d'une base de donn{\{}{\'{e}}{\}}es on extrait des features (MSERs + SIFT), qu'on clusterise en k groupe (k premiers noeuds de l'arbre), puis on r{\{}{\'{e}}{\}}it{\{}{\`{e}}{\}}re sur chaque groupe le clustering jusqu'{\{}{\`{a}}{\}} une profondeur L. C'est une m{\{}{\'{e}}{\}}thode inspir{\{}{\'{e}}{\}}e des BoW. Pour trouver les objets semblables {\{}{\`{a}}{\}} une query on calcul un vecteur de taille "le nombre de feuille de l'arbre" qui comptabilise la fr{\{}{\'{e}}{\}}quence de passage dans ces noeuds des features extraits de la requ{\{}{\^{e}}{\}}te. On compare ce vecteur avec les vecteurs pr{\{}{\'{e}}{\}}cacul{\{}{\'{e}}{\}}s de la m{\{}{\^{e}}{\}}me mani{\{}{\`{e}}{\}}re qui composent la base d'apprentissage de l'arbre.

Ce que je n'ai pas compris
- L'utilisation des inverted files
- Les subtilit{\{}{\'{e}}{\}} du calcul de vraisemblance
- Ce qui est relatif {\{}{\`{a}}{\}} l'entropie

Ce qui est int{\{}{\'{e}}{\}}ressant
- La rapidit{\{}{\'{e}}{\}} {\{}{\&}{\}} les r{\{}{\'{e}}{\}}sultats du proc{\{}{\'{e}}{\}}d{\{}{\'{e}}{\}}
- La possibili{\{}{\'{e}}{\}} de faire de tr{\{}{\`{e}}{\}}s grosses bases de donn{\{}{\'{e}}{\}}es

From Duplicate 2 (Scalable recognition with a vocabulary tree - Nist{\'{e}}r, David; Stew{\'{e}}nius, Henrik)

R{\'{e}}sum{\'{e}}
M{\'{e}}thode utilis{\'{e}}e pour de la classification d'objet dans une image, se basant sur les Vocabulary Tree. A partir d'une base de donn{\'{e}}es on extrait des features (MSERs + SIFT), qu'on clusterise en k groupe (k premiers noeuds de l'arbre), puis on r{\'{e}}it{\`{e}}re sur chaque groupe le clustering jusqu'{\`{a}} une profondeur L. C'est une m{\'{e}}thode inspir{\'{e}}e des BoW. Pour trouver les objets semblables {\`{a}} une query on calcul un vecteur de taille "le nombre de feuille de l'arbre" qui comptabilise la fr{\'{e}}quence de passage dans ces noeuds des features extraits de la requ{\^{e}}te. On compare ce vecteur avec les vecteurs pr{\'{e}}cacul{\'{e}}s de la m{\^{e}}me mani{\`{e}}re qui composent la base d'apprentissage de l'arbre.

Ce que je n'ai pas compris
- L'utilisation des inverted files
- Les subtilit{\'{e}} du calcul de vraisemblance
- Ce qui est relatif {\`{a}} l'entropie

Ce qui est int{\'{e}}ressant
- La rapidit{\'{e}} {\&} les r{\'{e}}sultats du proc{\'{e}}d{\'{e}}
- La possibili{\'{e}} de faire de tr{\`{e}}s grosses bases de donn{\'{e}}es

Computational load
Offline:
Online:

Scalability
Scale:
Scalability potential:},
author = {Nist{\'{e}}r, David and Stew{\'{e}}nius, Henrik},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2006.264},
file = {:home/nathan/Documents/Mendeley Desktop/Nist{\'{e}}r, Stew{\'{e}}nius/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Nist{\'{e}}r, Stew{\'{e}}nius{\_}2006{\_}Scalable recognition with a vocabulary tree.pdf:pdf},
isbn = {0769525970},
issn = {10636919},
pages = {2161--2168},
title = {{Scalable recognition with a vocabulary tree}},
volume = {2},
year = {2006}
}
@article{Pascoe2015b,
abstract = {This paper presents a large-scale evaluation of a visual localisation method in a challenging city environment. Our system makes use of a map built by combining data from LIDAR and cameras mounted on a survey vehicle to build a dense appearance prior of the environment. We then localise by minimising the normalised information distance (NID) between a live camera image and an image generated from our prior. The use of NID produces a localiser that is robust to significant changes in scene appearance. Furthermore, NID can be used to compare images across different modalities, allowing us to use the same system to determine the extrinsic calibration between LIDAR and camera on the survey vehicle. We evaluate our system with a large-scale experiment consisting of over 450,000 camera frames collected over 110km of driving over a period of six months, and demonstrate reliable localisation even in the presence of illumination change, snow and seasonal effects.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Pascoe, Geoffrey and Maddern, Will and Newman, Paul},
doi = {10.1109/ICCVW.2015.23},
eprint = {arXiv:1011.1669v3},
file = {:home/nathan/Documents/Mendeley Desktop/Pascoe, Maddern, Newman/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Pascoe, Maddern, Newman{\_}2015{\_}Direct Visual Localisation and Calibration for Road Vehicles in Changing City Environments.pdf:pdf},
isbn = {9781467383905},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
keywords = {Cameras,Integrated circuits,Laser radar,Lighting,Robustness,Vehicles,Visualization},
pages = {98--105},
pmid = {15003161},
title = {{Direct Visual Localisation and Calibration for Road Vehicles in Changing City Environments}},
volume = {2016-Febru},
year = {2015}
}
@article{Wan2016,
abstract = {This paper presents an UAV (Unmanned Aerial Vehicle) localisation algorithm for its autonomous navigation based on matching between on-board UAV image sequences to a pre-installed reference satellite image. As the UAV images and the reference image are not necessarily taken under the same illumination condition, illumination-invariant image matching is essential. Based on the investigation of illumination-invariant property of Phase Correlation (PC) via mathematical derivation and experiments, we propose a PC based fast and robust illumination-invariant localisation algorithm for UAV navigation. The algorithm accurately determines the current UAV position as well as the next UAV position even the illumination condition of UAV on-board images is different from the reference satellite image. A Dirac delta function based registration quality assessment together with a risk alarming criterion is introduced to enable the UAV to perform self-correction in case the UAV deviates from the planned route. UAV navigation experiments using simulated terrain shading images and remote sensing images have demonstrated a robust high performance of the proposed PC based localisation algorithm under very different illumination conditions resulted from solar motion. The superiority of the algorithm, in comparison with two other widely used image matching algorithms, MI (Mutual Information) and NCC (Normalised Correlation Coefficient), is significant for its high matching accuracy and fast processing speed.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de localisation global bas{\'{e}} vision d'un UAV en utilisant des images "nadirs" (cam{\'{e}}ra regardant le sol) et une image de r{\'{e}}f{\'{e}}rence satellitaire (frame-reference approach != frame-frame approach plutot utilis{\'{e}}e pour la VO). La m{\'{e}}thode se veut invariante au changement de luminosit{\'{e}} (qui a une forte impacte sur les images nadirs. Les auteurs utilisent deux images : une prise par l'UAV et une autre plus grande englobant la premi{\`{e}}re image prise par le satellite (donc on connait a priori la position grossi{\`{e}}re de l'UAV). La comparaison des images se fait au travers d'un fen{\^{e}}tre glissante (frame sur reference) et le calcul de corr{\'{e}}lation de phase de la transform{\'{e}}e de Fourier des images. Les auteurs d{\'{e}}montre que cette m{\'{e}}thode de PC (Phase Correlation) est invariante {\`{a}} l'illumination et plus rapide a calculer que d'autre m{\'{e}}thode de corr{\'{e}}lation. Ils testent ensuite leur approche sur des donn{\'{e}}es simul{\'{e}}es et r{\'{e}}elles, en pr{\'{e}}cisant que sur des cas tr{\`{e}}s challenging (quand la r{\'{e}}solution des images est trop diff{\'{e}}rentes), une intervention humaine de supervisation est n{\'{e}}cessaire.

Ce que je n'ai pas compris
- Si on peut d{\'{e}}terminer la rotation relative de deux images
- Le d{\'{e}}tail des calculs des transform{\'{e}}es

Ce qui est int{\'{e}}ressant
- La bonne pr{\'{e}}cision compar{\'{e}} au temps de calcul
- Les diff{\'{e}}rentes applications {\'{e}}voqu{\'{e}}es dans le papier

Critiques
- On a besoin de savoir beaucoup de chose sur le drone (altitude, position a priori)
- D'avantage orient{\'{e}} planification de parcours que "GPS" dans un environnement inconnu

A approfondir
- TF
- M{\'{e}}thodes de rectification d'images

Computational load
Offline: -
Online: 0.2s/camera position

Scalability
Scale: -
Scalability potential: -},
author = {Wan, Xue and Liu, Jianguo and Yan, Hongshi and Morgan, Gareth L. K.},
doi = {10.1016/j.isprsjprs.2016.05.016},
file = {:home/nathan/Documents/Mendeley Desktop/Wan et al/ISPRS Journal of Photogrammetry and Remote Sensing/Wan et al.{\_}2016{\_}Illumination-invariant image matching for autonomous UAV localisation based on optical sensing.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Illumination-invariant,Image registration,Phase correlation,Vision-based navigation},
pages = {198--213},
publisher = {International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
title = {{Illumination-invariant image matching for autonomous UAV localisation based on optical sensing}},
url = {http://dx.doi.org/10.1016/j.isprsjprs.2016.05.016},
volume = {119},
year = {2016}
}
@inproceedings{Majdik2013,
abstract = {We tackle the problem of globally localizing a camera-equipped micro aerial vehicle flying within urban en- vironments for which a Google Street View image database exists. To avoid the caveats of current image-search algorithms in case of severe viewpoint changes between the query and the database images, we propose to generate virtual views of the scene, which exploit the air-ground geometry of the system. To limit the computational complexity of the algorithm, we rely on a histogram-voting scheme to select the best putative image correspondences. The proposed approach is tested on a 2km image dataset captured with a small quadroctopter flying in the streets of Zurich. The success of our approach shows that our new air-ground matching algorithm can robustly handle extreme changes in viewpoint, illumination, perceptual aliasing, and over-season variations, thus, outperforming conventional visual place-recognition approaches. MULTIMEDIA},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de localisation bas{\'{e}}e image qui confronte des panorama street view avec des clich{\'{e}}s pris par un drone vollant {\`{a}} une vingtaine de m{\`{e}}tres de hauteur. Les auteurs traitent le probl{\`{e}}me de changement de point de vue (image prise au niveau de la rue / image prise {\`{a}} 20 m{\`{e}}tres de hauteur) en dans un premier temps g{\'{e}}n{\'{e}}rant des images syth{\'{e}}tiques (du cot{\'{e}} database et query) {\`{a}} la mani{\`{e}}re de ASIFT mais adapt{\'{e}} pour le air ground matching (pour le cot{\'{e}} invariance affine), puis en matchant rapidement "les queries" {\`{a}} la base de donn{\'{e}}e de decripteurs (SIFT) par FLANN puis en selectionnant un sous-ensemble des images en se basant sur l'orientation des descripteurs match{\'{e}}s (par construction d'histogramme). Finalement les auteurs rejettent les outliers par Virtual Line Descriptor (kVLD). 

Ce qui est int{\'{e}}ressant
- Air/ground dataset
- La robustesse au changement de point de vue et aux changements environnementaux ({\'{e}}t{\'{e}} 2009 vs hiver 2012)

Critiques
- Pas de temps de calcul

A approfondir
- kVLD

Computational load
Offline: View synthesis
Online: ?

Scalability
Scale: Place
Scalability potential: ?},
author = {Majdik, Andras L. and Albers-Schoenberg, Yves and Scaramuzza, Davide},
booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2013.6696925},
file = {:home/nathan/Documents/Mendeley Desktop/Majdik, Albers-Schoenberg, Scaramuzza/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Majdik, Albers-Schoenberg, Scaramuzza{\_}2013{\_}MAV urban localization from Google street view data.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
pages = {3979--3986},
title = {{MAV urban localization from Google street view data}},
year = {2013}
}
@inproceedings{Meng2016,
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de camera relocalization s'inspirant des travaux de Shotton et al. 2013 sur les regression forests mais sans prendre en compte la profondeur au moment du query time. Pour compenser la perte de pr{\'{e}}sicision, les auteurs rajoutent une m{\'{e}}thode de refinement par NN search (SIFT features) local en partant de la pose grossi{\`{e}}re obtenu par l'arbre. Les auteurs comparent avec PoseNet et montrent des meilleurs r{\'{e}}sultats. Ils introduisent {\'{e}}galment un benchmarck sur des s{\'{e}}quences de sport (avec une cam{\'{e}}ra {\`{a}} la position fixe mais avec Pan/Tilt {\&} zoom)

Ce qui est int{\'{e}}ressant
- Entrainement sur du RGB-D et online sur du RGB
- Cascade scheme 

Critiques
- R{\'{e}}sultat pas mal mais pas sup{\'{e}}rieur {\`{a}} SCOR
- Pas de grandes zones (id{\'{e}}e interessante de r{\'{e}}duire l'espace de recherche pour le NN mais les regression forests n'ont pas non plus l'air ouf pour passer {\`{a}} l'{\'{e}}chelle)

Computational load
Offline: ?
Online: 5 FPS

Scalability
Scale: Room (or statidum with only rotation camera)
Scalability potential: ?},
author = {Meng, Lili and Chen, Jianhui and Tung, Frederick and Little, James J and de Silva, Clarence W.},
booktitle = {British Machine Vision Conference (BMVC)},
file = {:home/nathan/Documents/Mendeley Desktop/Meng et al/British Machine Vision Conference (BMVC)/Meng et al.{\_}2016{\_}Exploiting Random RGB and Sparse Features for Camera Pose Estimation.pdf:pdf},
pages = {1--12},
title = {{Exploiting Random RGB and Sparse Features for Camera Pose Estimation}},
year = {2016}
}
@article{Tolias2015,
author = {Tolias, Giorgos and Avrithis, Yannis and J{\'{e}}gou, Herv{\'{e}}},
file = {:home/nathan/Documents/Mendeley Desktop/Tolias, Avrithis, J{\'{e}}gou/International Journal of Computer Vision (IJCV)/Tolias, Avrithis, J{\'{e}}gou{\_}2015{\_}Image search with selective match kernels aggregation across single and multiple images.pdf:pdf},
journal = {International Journal of Computer Vision (IJCV)},
title = {{Image search with selective match kernels: aggregation across single and multiple images}},
year = {2015}
}
@article{Xia2017,
abstract = {Remote sensing (RS) image retrieval based on visual content is of great significance for geological information mining. Over the past two decades, a large amount of research on this task has been carried out, which mainly focuses on the following three core issues of image retrieval: visual feature, similarity metric and relevance feedback. Along with the advance of these issues, the technology of RS image retrieval has been developed comparatively mature. However, due to the complexity and multiformity of high-resolution remote sensing (HRRS) images, there is still room for improvement in the current methods on HRRS data retrieval. In this paper, we analyze the three key aspects of retrieval and provide a comprehensive review on content-based RS image retrieval methods. Furthermore, for the goal to advance the state-of-the-art in HRRS image retrieval, we focus on the visual feature aspect and delve how to use powerful deep representations in this task. We conduct systematic investigation on evaluating factors that may affect the performance of deep features. By optimizing each factor, we acquire remarkable retrieval results on publicly available HRRS datasets. Finally, we explain the experimental phenomenon in detail and draw instructive conclusions according to our analysis. Our work can serve as a guiding role for the research of content-based RS image retrieval.},
archivePrefix = {arXiv},
arxivId = {1707.07321},
author = {Xia, Gui-Song and Tong, Xin-Yi and Hu, Fan and Zhong, Yanfei and Datcu, Mihai and Zhang, Liangpei},
eprint = {1707.07321},
file = {:home/nathan/Documents/Mendeley Desktop/Xia et al/Unknown/Xia et al.{\_}2017{\_}Exploiting Deep Features for Remote Sensing Image Retrieval A Systematic Investigation.pdf:pdf},
pages = {1--38},
title = {{Exploiting Deep Features for Remote Sensing Image Retrieval: A Systematic Investigation}},
url = {http://arxiv.org/abs/1707.07321},
year = {2017}
}
@book{Hartley2003,
author = {Hartley, Richard and Zisserman, Andrew},
publisher = {Cambridge university press},
title = {{Multiple view geometry in computer vision}},
year = {2003}
}
@inproceedings{Muja2009,
abstract = {For many computer vision problems, the most time consuming component consists of nearest neighbor match- ing in high-dimensional spaces. There are no known exact algorithms for solving these high-dimensional problems that are faster than linear search. Approximate algorithms are known to provide large speedups with only minor loss in accuracy, but many such algorithms have been published with only minimal guidance on selecting an algorithm and its parameters for any given problem. In this paper, we describe a system that answers the question, “What is the fastest approximate nearest-neighbor algorithm for my data?” Our system will take any given dataset and desired degree of precision and use these to automatically determine the best algorithm and parameter values. We also describe a new algorithm that applies priority search on hierarchical k-means trees, which we have found to provide the best known performance on many datasets. After testing a range of alternatives, we have found that multiple randomized k-d trees provide the best performance for other datasets. We are releasing public domain code that implements these approaches. This library provides about one order of magnitude improvement in query time over the best previously available software and provides fully automated parameter selection.},
annote = {From Duplicate 1 (Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration - Muja, Marius; Lowe, David G.)

R{\'{e}}sum{\'{e}}

Introduction d'une librairie opensource pour le choix automatique d'algorithme pour le fast nearest neighbor search in hight space dimension : http://www.cs.ubc.ca/research/flann/

A approfondir

- kd-tree
- cross-validation

Computational load
Offline:
Online:

Scalability
Scale:
Scalability potential:

From Duplicate 2 (Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration - Muja, Marius; Lowe, David G.)

R{\{}{\'{e}}{\}}sum{\{}{\'{e}}{\}}

Introduction d'une librairie opensource pour le choix automatique d'algorithme pour le fast nearest neighbor search in hight space dimension : http://www.cs.ubc.ca/research/flann/

A approfondir

- kd-tree
- cross-validation},
archivePrefix = {arXiv},
arxivId = {10.1.1.160.1721},
author = {Muja, Marius and Lowe, David G.},
booktitle = {Proceedings of the International Conference on Computer Vision Theory and Applications (VISAPP)},
doi = {10.1.1.160.1721},
eprint = {10.1.1.160.1721},
file = {:home/nathan/Documents/Mendeley Desktop/Muja, Lowe/Proceedings of the International Conference on Computer Vision Theory and Applications (VISAPP)/Muja, Lowe{\_}2009{\_}Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration.pdf:pdf},
isbn = {9789898111692},
issn = {00301299},
pages = {1--10},
pmid = {15718756},
title = {{Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration}},
url = {papers2://publication/uuid/3C5A483A-ADCA-4121-A768-8E31BB293A4D},
year = {2009}
}
@article{Yin2017,
author = {Yin, Peng and He, Yuqing and Liu, Na and Han, Jianda and Xu, Weiliang and Zealand, New},
file = {:home/nathan/Documents/Mendeley Desktop/Yin et al/Unknown/Yin et al.{\_}2017{\_}Condition directed Multi-domain Adversarial Learning for Loop Closure Detection.pdf:pdf},
keywords = {4,5,6,7,8,brief,feature descriptor sift,generative adversarial,gist,loop closure detection,may fail,multi-domain adversarial learning,networks,or global feature descriptor,orb,such as the local,surf},
title = {{Condition directed Multi-domain Adversarial Learning for Loop Closure Detection}},
year = {2017}
}
@article{Vo2017,
abstract = {Image geolocalization, inferring the geographic location of an image, is a challenging computer vision problem with many potential applications. The recent state-of-the-art ap-proach to this problem is a deep image classification ap-proach in which the world is spatially divided into cells and a deep network is trained to predict the correct cell for a given image. We propose to combine this approach with the original Im2GPS approach in which a query image is matched against a database of geotagged images and the location is inferred from the retrieved set. We estimate the geographic location of a query image by applying kernel density estimation to the locations of its nearest neighbors in the reference database. Interestingly, we find that the best features for our retrieval task are derived from networks trained with classification loss even though we do not use a classification approach at test time. Training with clas-sification loss outperforms several deep feature learning methods (e.g. Siamese networks with contrastive of triplet loss) more typical for retrieval applications. Our simple ap-proach achieves state-of-the-art geolocalization accuracy while also requiring significantly less training data.},
archivePrefix = {arXiv},
arxivId = {1705.04838},
author = {Vo, Nam N. and Jacobs, Nathan and Hays, James},
eprint = {1705.04838},
file = {:home/nathan/Documents/Mendeley Desktop/Vo, Jacobs, Hays/arXiv preprint/Vo, Jacobs, Hays{\_}2017{\_}Revisiting IM2GPS in the Deep Learning Era.pdf:pdf},
journal = {arXiv preprint},
number = {1},
title = {{Revisiting IM2GPS in the Deep Learning Era}},
year = {2017}
}
@inproceedings{Kaplan2016,
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Kaplan, Avi and Avraham, Tamar and Lindenbaum, Michael},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-319-46448-0},
eprint = {1311.2901},
file = {:home/nathan/Documents/Mendeley Desktop/Kaplan, Avraham, Lindenbaum/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Kaplan, Avraham, Lindenbaum{\_}2016{\_}Interpreting the Ratio Criterion for Matching SIFT Descriptors.pdf:pdf},
isbn = {978-3-319-46447-3},
issn = {0302-9743},
keywords = {a contrario,matching,sift},
pages = {697--712},
title = {{Interpreting the Ratio Criterion for Matching SIFT Descriptors}},
url = {http://link.springer.com/10.1007/978-3-319-46448-0},
volume = {9905},
year = {2016}
}
@inproceedings{Sattler2017,
annote = {R{\'{e}}sum{\'{e}}
Papier qui compare les m{\'{e}}thodes de LBV indirect {\&} direct sur un large zone (San Francisco). Ils y a plusieurs contributions, tout d'abord ils ont cr{\'{e}}{\'{e}} une reference pour le dataset SF en faisant du SfM (pas vraiment une ground true mais bon). Ils comparent ensuite DisLoc, DenseVLAD (Torii 2015) et NetVLAD aux m{\'{e}}thodes directes Hyperpoint {\&} CPV (Svarm et al. 2014/TPAMI2016). Ils en concluent que indirecte -{\textgreater} plus large zone {\&} direct -{\textgreater} plus pr{\'{e}}cis. Ils ajoutent ensuite {\`{a}} la sortie des m{\'{e}}thodes indirectes un calcul de pose pr{\'{e}}cise en s'appuyant sur des mod{\`{e}}les SfM locaux et du coup arrive {\`{a}} un niveau de pr{\'{e}}cision {\'{e}}quivalent que les m{\'{e}}thodes direct.

Ce que je n'ai pas compris
- Lu vite

Ce qui est int{\'{e}}ressant
- Cascade scheme
- Pose r{\'{e}}frence pour dataset SF

Critiques
- Un peu bizarre ce papier (ils passent vachement de temps sur la cr{\'{e}}ation de la base de donn{\'{e}}es)

Computational load
Petite compariason sur les diff{\'{e}}rents temps de calcul pas mal

Scalability
Scale: City
Scalability potential: Yes !!!},
author = {Sattler, Torsten and Torii, Akihiko and Sivic, Josef and Pollefeys, Marc and Taira, Hajime and Okutomi, Masatoshi and Pajdla, Tomas},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Sattler et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Sattler et al.{\_}2017{\_}Are Large-Scale 3D Models Really Necessary for Accurate Visual Localization.pdf:pdf},
title = {{Are Large-Scale 3D Models Really Necessary for Accurate Visual Localization?}},
year = {2017}
}
@inproceedings{Lim2012,
author = {Lim, Hyon and Sinha, Sudipta N. and Cohen, Michael F. and Uyttendaele, Matthew},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Lim et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Lim et al.{\_}2012{\_}Real-time Image-based 6-DOF Localization in Large-Scale Environments.pdf:pdf},
title = {{Real-time Image-based 6-DOF Localization in Large-Scale Environments}},
year = {2012}
}
@inproceedings{Naseer2017,
author = {Naseer, Tayyab and Burgard, Wolfram},
booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
file = {:home/nathan/Documents/Mendeley Desktop/Naseer, Burgard/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Naseer, Burgard{\_}2017{\_}Deep Regression for Monocular Camera-based 6-DoF Global Localization in Outdoor Environments.pdf:pdf},
isbn = {9781538626818},
pages = {1525--1530},
title = {{Deep Regression for Monocular Camera-based 6-DoF Global Localization in Outdoor Environments}},
year = {2017}
}
@inproceedings{Hays2008,
annote = {R{\'{e}}sum{\'{e}}
Article s'interessant {\`{a}} la localisation d'image {\`{a}} une {\'{e}}chelle mondial (comme le r{\'{e}}cent PlaNet). Les auteurs s'appuient sur une base de donn{\'{e}}es de 6 millions d'images g{\'{e}}olocalis{\'{e}}s/tagu{\'{e}}s et l'utilisation conjointe d'un set de descripteurs globaux/locaux (tiny images, color {\&} texton histogramms, lines {\&} GIST). La query est compar{\'{e}} en fonction de ces descripteurs et les auteurs appliquent un groupement par Mean-sift sur les kpp voisins r{\'{e}}cup{\'{e}}r{\'{e}}s par cette comparaison pour filtrer les outliers. Discussion autour des applications en analyse g{\'{e}}ographique et d{\'{e}}mographique de cette m{\'{e}}thode.

Ce qui est int{\'{e}}ressant
- L'utilisation conjointe de plusieurs descripteur et leur comparaison

Critiques
- Pas tr{\`{e}}s adapt{\'{e}} {\`{a}} notre probl{\'{e}}matique 
- Temps de calcul au moment de la requ{\^{e}}te non mentionn{\'{e}}, mais probablement {\'{e}}norme


Computational load
Offline: 3 jours sur 400 CPU pour la pr{\'{e}}-extraction des features
Online: Non renseign{\'{e}}E

Scalability
Scale: World
Scalability potential: -},
author = {Hays, James and Efros, Alexei A.},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Hays, Efros/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Hays, Efros{\_}2008{\_}IM2GPS Estimating Geographic Information From a Single Image.pdf:pdf},
isbn = {9781424422432},
title = {{IM2GPS: Estimating Geographic Information From a Single Image}},
volume = {05},
year = {2008}
}
@inproceedings{Morioka2011,
abstract = {Vision-based mobile robot's simultaneous localization and mapping (SLAM) and navigation has been the source of countless research contributions because of rich sensory output and cost effectiveness of vision sensors. However, existing methods of vision-based SLAM and navigation are not effective for robots to be used in crowded environments such as train stations and shopping malls, because when we extract feature points from an image in crowded environments, many feature points are extracted from not only static objects but also dynamic objects such as humans. By recognizing all such feature points as landmarks, the algorithm collapses and errors occur in map building and self-localization. In this paper, we propose a SLAM and navigation method that is effective even in crowded environments by extracting robust 3D feature points from sequential vision images and odometry. By using the proposed method, we can eliminate unstable feature points extracted from dynamic objects and perform SLAM and navigation stably. We present experiments showing the utility of our approach in crowded environments, including map building and navigation.},
author = {Morioka, Hiroshi and Yi, Sangkyu and Hasegawa, Osamu},
booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2011.6048427},
file = {:home/nathan/Documents/Mendeley Desktop/Morioka, Yi, Hasegawa/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Morioka, Yi, Hasegawa{\_}2011{\_}Vision-based mobile robot's SLAM and navigation in crowded environments.pdf:pdf},
isbn = {9781612844541},
issn = {2153-0858},
pages = {3998--4005},
pmid = {6094847},
title = {{Vision-based mobile robot's SLAM and navigation in crowded environments}},
year = {2011}
}
@article{Ponti,
author = {Ponti, Moacir A. and Ribeiro, Leonardo S. F. and Nazare, Tiago S. and Bui, Tu and Collomosse, John},
file = {:home/nathan/Documents/Mendeley Desktop/Ponti et al/Unknown/Ponti et al.{\_}Unknown{\_}Everything you wanted to know about Deep Learning for Computer Vision but were afraid to ask.pdf:pdf},
keywords = {-computer vision,deep learning,image process-,ing,video processing},
pages = {9--12},
title = {{Everything you wanted to know about Deep Learning for Computer Vision but were afraid to ask}}
}
@inproceedings{Gionis1999,
author = {Gionis, Aristides and Indyk, Piotr and Motwani, Rajeev},
booktitle = {Proceedings of the 25th VLDB Conference},
file = {:home/nathan/Documents/Mendeley Desktop/Gionis, Indyk, Motwani/Proceedings of the 25th VLDB Conference/Gionis, Indyk, Motwani{\_}1999{\_}Similarity Search in High Dimensions via Hashing.pdf:pdf},
pages = {518--529},
title = {{Similarity Search in High Dimensions via Hashing}},
year = {1999}
}
@inproceedings{Zhang2014,
abstract = {We examine an under-explored visual recognition problem, where we have a main view along with an auxiliary view of visual infor- mation present in the training data, but merely the main view is avail- able in the test data. To effectively leverage the auxiliary view to train a stronger classifier, we propose a collaborative auxiliary learning frame- work based on a new discriminative canonical correlation analysis. This framework reveals a common semantic space shared across both views through enforcing a series of nonlinear projections. Such projections automatically embed the discriminative cues hidden in both views into the common space, and better visual recognition is thus achieved on the test data that stems from only the main view. The efficacy of our proposed auxiliary learning approach is demonstrated through three challenging visual recognition tasks with different kinds of auxiliary information.},
author = {Zhang, Qilin and Hua, Gang and Liu, Wei and Liu, Zicheng and Zhang, Zhengyou},
booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
doi = {10.1007/978-3-319-16865-4_5},
file = {:home/nathan/Documents/Mendeley Desktop/Zhang et al/Proceedings of the Asian Conference on Computer Vision (ACCV)/Zhang et al.{\_}2014{\_}Can visual recognition benefit from auxiliary information in training.pdf:pdf},
isbn = {9783319168647},
issn = {16113349},
pages = {65--80},
title = {{Can visual recognition benefit from auxiliary information in training?}},
volume = {9003},
year = {2014}
}
@inproceedings{Milford2015,
abstract = {Vision-based localization on robots and vehicles remains unsolved when extreme appearance change and viewpoint change are present simultaneously. The current state of the art approaches to this challenge either deal with only one of these two problems; for example FAB- MAP (viewpoint invariance) or SeqSLAM (appearance- invariance), or use extensive training within the test environment, an impractical requirement in many application scenarios. In this paper we significantly improve the viewpoint invariance of the SeqSLAM algorithm by using state-of-the-art deep learning techniques to generate synthetic viewpoints. Our approach is different to other deep learning approaches in that it does not rely on the ability of the CNN network to learn invariant features, but only to produce “good enough” depth images from day-time imagery only. We evaluate the system on a new multi-lane day-night car dataset specifically gathered to simultaneously test both appearance and viewpoint change. Results demonstrate that the use of synthetic viewpoints improves the maximum recall achieved at 100{\%} precision by a factor of 2.2 and maximum recall by a factor of 2.7, enabling correct place recognition across multiple road lanes and significantly reducing the time between correct localizations.},
annote = {R{\'{e}}sum{\'{e}}

Adaptation et am{\'{e}}lioration du SLAM SeqSLAM qui permet de faire de la place recognition sur une s{\'{e}}quence de route de jour et de nuit. Utilisation du deep learning pour cacluler la pronfondeur des images afin de synth{\'{e}}tiser d'autes vues par shifting pour simuler le placement de la voiture sur des voies diff{\'{e}}rentes de circulation.

Ce que je n'ai pas compris

- Quelle est la base d'apprentissage et quelle est la base de test ? (si on prends 80 km/h -{\textgreater} 3000 images dont 1000 utilis{\'{e}} lors des tests. On peut dire 2000*3 (view sythesis) pour la base d'apprentissage)
- Le contrast enhancement (sec. 3.3)
- Ce qu'on met en entr{\'{e}}e du CNN pour le calcul de la profondeur

Ce qui est int{\'{e}}ressant

- La view sythesis pour rendre le SLAM invariant au view point. 
- Le SeqSLAM (m{\^{e}}me si on a l'air d'avoir besoin d'informations denses)
- La difficult{\'{e}} de la s{\'{e}}quence (sc{\`{e}}nes qui se ressemble beaucoup avec changement jour/nuit)

Critiques

- Papier tr{\`{e}}s orient{\'{e}} place recognition dans une s{\'{e}}quence de route
- Pas d'info temps de calcul online (seulement sur le temps de calcul de g{\'{e}}nration des viewpoints)
- M{\'{e}}thode de comparaison d'image un peu "bourine"

A approfondir

- SeqSLAM
- CNN pour le calcul de profondeur
- FAB-MAP

Computational load
Offline: training of the CNN (10s/image) then 0.25s by images
Online: Not provided, comparison of images with global description (SAD) through sliding windows

Scalability
Scale: Multiple place
Scalability potential: Linear cost},
author = {Milford, Michael J. and Lowry, Stephanie and Shirazi, Sareh and Pepperell, Edward and Shen, Chunhua and Lin, Guosheng and Liu, Fayao and Cadena, C{\'{e}}sar and Reid, Ian},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
file = {:home/nathan/Documents/Mendeley Desktop/Milford et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)/Milford et al.{\_}2015{\_}Sequence Searching with Deep-learnt Depth for Condition- and Viewpoint- invariant Route-based Place Recognition.pdf:pdf},
isbn = {9781467367592},
keywords = {CNN,Challenging matching,Global features,Outdoor,View Synthesis,Visual place recognition},
mendeley-tags = {CNN,Challenging matching,Global features,Outdoor,View Synthesis,Visual place recognition},
pages = {18--25},
title = {{Sequence Searching with Deep-learnt Depth for Condition- and Viewpoint- invariant Route-based Place Recognition}},
year = {2015}
}
@article{Wu2017,
author = {Wu, Zizhao and Zhang, Yunhui and Zeng, Ming and Qin, Feiwei and Wang, Yigang},
doi = {10.1016/j.cag.2017.07.013},
file = {:home/nathan/Documents/Mendeley Desktop/Wu et al/Computers {\&} Graphics/Wu et al.{\_}2017{\_}Joint analysis of shapes and images via deep domain adaptation.pdf:pdf},
issn = {00978493},
journal = {Computers {\&} Graphics},
keywords = {3D shape recognition,3d shape recognition,Convolutional neural network,Cross-modal retrieval,Domain adaption,Joint analysis},
publisher = {Elsevier Ltd},
title = {{Joint analysis of shapes and images via deep domain adaptation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0097849317301061},
year = {2017}
}
@inproceedings{Krizhevsky2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
booktitle = {Communications of the ACM},
file = {:home/nathan/Documents/Mendeley Desktop/Krizhevsky, Sutskever, Hinton/Communications of the ACM/Krizhevsky, Sutskever, Hinton{\_}2017{\_}Imagenet classification with deep convolutional neural networks.pdf:pdf},
pages = {1097--1105},
title = {{Imagenet classification with deep convolutional neural networks}},
year = {2017}
}
@inproceedings{Qi2016,
abstract = {Sketch-based image retrieval (SBIR) is a challenging task due to the ambiguity inherent in sketches when compared with photos. In this paper, we propose a novel convolutional neural network based on Siamese network for SBIR. The main idea is to pull output feature vectors closer for input sketch-image pairs that are labeled as similar, and push them away if irrel-evant. This is achieved by jointly tuning two convolutional neural networks which linked by one loss function. Exper-imental results on Flickr15K demonstrate that the proposed method offers a better performance when compared with sev-eral state-of-the-art approaches.},
author = {Qi, Yonggang and Song, Yi-Zhe and Zhang, Honggang and Liu, Jun},
booktitle = {Proceedings of the IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2016.7532801},
file = {:home/nathan/Documents/Mendeley Desktop/Qi et al/Proceedings of the IEEE International Conference on Image Processing (ICIP)/Qi et al.{\_}2016{\_}Sketch-based Image Retrieval via Siamese Convolutional Neural Network.pdf:pdf},
isbn = {978-1-4673-9961-6},
number = {iii},
pages = {2460--2464},
title = {{Sketch-based Image Retrieval via Siamese Convolutional Neural Network}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7532801},
year = {2016}
}
@inproceedings{Stumm2016,
abstract = {A novel method for visual place recognition is intro- duced and evaluated, demonstrating robustness to percep- tual aliasing and observation noise. This is achieved by increasing discrimination through a more structured repre- sentation of visual observations. Estimation of observation likelihoods are based on graph kernel formulations, uti- lizing both the structural and visual information encoded in covisibility graphs. The proposed probabilistic model is able to circumvent the typically difficult and expensive posterior normalization procedure by exploiting the infor- mation available in visual observations. Furthermore, the place recognition complexity is independent of the size of the map. Results show improvements over the state-of-the- art on a diverse set of both public datasets and novel exper- iments, highlighting the benefit of the approach.},
annote = {R{\'{e}}sum{\'{e}}

Nouvelle m{\'{e}}thodologie pour faire du place recogniton plut{\^{o}}t orient{\'{e}} robotique (SLAM/Loop Closure). Utilisation de m{\'{e}}thodes de comparaison de graphs (graph kernel) pour retrouver la position d'une image dans une map. Les images sont d{\'{e}}crites au travers de local features regroup{\'{e}}es en visual words.

Ce que je n'ai pas compris

- Le calcul du la map
- Comment retrouver exactement deux visuals words

Ce qui est int{\'{e}}ressant

- Les r{\'{e}}sultats et la rapidit{\'{e}} de calcul (sur code python pas optimis{\'{e}})
- Le nouveau terme de normalisation introduit dans la loi de Bayes

Critiques

- Il faut avoir le m{\^{e}}me view point
- Pas tr{\`{e}}s matching challenging (juste des petites oculusion mais pas de long terme place recogniton)
- M{\'{e}}thode tr{\`{e}}s orient{\'{e}} SLAM/loop closur donc sur des PETITS environnement avec construction du graph en temps r{\'{e}}el (pas d'apprentissage a priori etc.)

A approfondir

- WL kernel
- Visual words

Computational load
Offline: seconds
Online: {\textless}1s

Scalability
Scale: Multiple places
Scalability potential: SLAM based approach, not to extensive},
author = {Stumm, Elena and Mei, Christopher and Lacroix, Simon and Nieto, Juan and Hutter, Marco and Siegwart, Roland},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.491},
file = {:home/nathan/Documents/Mendeley Desktop/Stumm et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Stumm et al.{\_}2016{\_}Robust Visual Place Recognition with Graph Kernels.pdf:pdf},
keywords = {Local features,Outdoor,RGB request,SURF,Visual place recognition,Visual words,WL kernel},
mendeley-tags = {Local features,Outdoor,RGB request,SURF,Visual place recognition,Visual words,WL kernel},
pages = {4535--4544},
title = {{Robust Visual Place Recognition with Graph Kernels}},
year = {2016}
}
@article{Carlevaris-Bianco2016,
abstract = {This paper documents a large scale, long-term autonomy dataset for robotics research collected on the University of Michigan's North Campus. The dataset consists of omnidirectional imagery, 3D lidar, planar lidar, GPS, and proprioceptive sensors for odometry collected using a Segway robot. The dataset was collected to facilitate research focusing on long-term autonomous operation in changing environments. The dataset is composed of 27 sessions spaced approximately biweekly over the course of 15 months. The sessions repeatedly explore the campus, both indoors and outdoors, on varying trajectories, and at different times of the day across all four seasons. This allows the dataset to capture many challenging elements including: moving obstacles (e.g. pedestrians, bicyclists and cars), changing lighting, varying viewpoint, seasonal and weather changes (e.g. falling leaves and snow), and long-term structural changes caused by construction projects. To further facilitate research, we also provide ground-truth pose for all sessions in a single frame of reference.},
author = {Carlevaris-Bianco, Nicholas and Ushani, Arash K. and Eustice, Ryan M.},
doi = {10.1177/0278364915614638},
file = {:home/nathan/Documents/Mendeley Desktop/Carlevaris-Bianco, Ushani, Eustice/The International Journal of Robotics Research (IJRR)/Carlevaris-Bianco, Ushani, Eustice{\_}2016{\_}University of Michigan North Campus long-term vision and lidar dataset.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research (IJRR)},
keywords = {computer vision,field and service robotics,lidar,long-term slam,place recognition},
number = {9},
pages = {1023--1035},
title = {{University of Michigan North Campus long-term vision and lidar dataset}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364915614638},
volume = {35},
year = {2016}
}
@article{Zhang2017b,
abstract = {—Multi-index fusion has demonstrated impressive performances in retrieval task by integrating different visual representations in a unified framework. However, previous works mainly consider propagating similarities via neighbor structure, ignoring the high order information among different visual representations. In this paper, we propose a new multi-index fusion scheme for image retrieval. By formulating this procedure as a multilinear based optimization problem, the complemen-tary information hidden in different indexes can be explored more thoroughly. Specially, we first build our multiple indexes from various visual representations. Then a so-called index-specific functional matrix, which aims to propagate similarities, is introduced for updating the original index. The functional matrices are then optimized in a unified tensor space to achieve a refinement, such that the relevant images can be pushed more closer. The optimization problem can be efficiently solved by the augmented Lagrangian method with theoretical convergence guarantee. Unlike the traditional multi-index fusion scheme, our approach embeds the multi-index subspace structure into the new indexes with sparse constraint, thus it has little additional mem-ory consumption in online query stage. Experimental evaluation on three benchmark datasets reveals that the proposed approach achieves the state-of-the-art performance, i.e., N-score 3.94 on UKBench, mAP 94.1{\%} on Holiday and 62.39{\%} on Market-1501.},
archivePrefix = {arXiv},
arxivId = {1709.09304},
author = {Zhang, Zhizhong and Xie, Yuan and Zhang, Wensheng and Tian, Qi},
eprint = {1709.09304},
file = {:home/nathan/Documents/Mendeley Desktop/Zhang et al/arXiv preprint/Zhang et al.{\_}2017{\_}Effective Image Retrieval via Multilinear Multi-index Fusion.pdf:pdf},
journal = {arXiv preprint},
keywords = {Index Terms—Image retrieval,Multi-index fusion,Person re-identification,Tensor multi-rank},
number = {X},
pages = {1--12},
title = {{Effective Image Retrieval via Multilinear Multi-index Fusion}},
volume = {XX},
year = {2017}
}
@article{Kostavelis2015,
abstract = {The evolution of contemporary mobile robotics has given thrust to a series of additional conjunct technologies. Of such is the semantic mapping, which provides an abstraction of space and a means for human-robot communication. The recent introduction and evolution of semantic mapping motivated this survey, in which an explicit analysis of the existing methods is sought. The several algorithms are categorized according to their primary characteristics, namely scalability, inference model, temporal coherence and topological map usage. The applications involving semantic maps are also outlined in the work at hand, emphasizing on human interaction, knowledge representation and planning. The existence of publicly available validation datasets and benchmarking, suitable for the evaluation of semantic mapping techniques is also discussed in detail. Last, an attempt to address open issues and questions is also made.},
author = {Kostavelis, Ioannis and Gasteratos, Antonios},
doi = {10.1016/j.robot.2014.12.006},
file = {:home/nathan/Documents/Mendeley Desktop/Kostavelis, Gasteratos/Robotics and Autonomous Systems (RAS)/Kostavelis, Gasteratos{\_}2015{\_}Semantic mapping for mobile robotics tasks A survey.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems (RAS)},
keywords = {Human-robot interaction,Knowledge representation,Mobile robots,Object recognition,Place recognition,Planning,Semantic map,Temporal coherence,Topological map},
pages = {86--103},
publisher = {Elsevier B.V.},
title = {{Semantic mapping for mobile robotics tasks: A survey}},
url = {http://dx.doi.org/10.1016/j.robot.2014.12.006},
volume = {66},
year = {2015}
}
@inproceedings{Cavallari,
abstract = {Camera relocalisation is a key problem in computer vi-sion, with applications as diverse as simultaneous locali-sation and mapping, virtual/augmented reality and naviga-tion. Common techniques either match the current image against keyframes with known poses coming from a tracker, or establish 2D-to-3D correspondences between keypoints in the current image and points in the scene in order to es-timate the camera pose. Recently, regression forests have become a popular alternative to establish such correspon-dences. They achieve accurate results, but must be trained offline on the target scene, preventing relocalisation in new environments. In this paper, we show how to circumvent this limitation by adapting a pre-trained forest to a new scene on the fly. Our adapted forests achieve relocalisation per-formance that is on par with that of offline forests, and our approach runs in under 150ms, making it desirable for real-time systems that require online relocalisation.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de relocalization de cam{\'{e}}ra RGBD se basant sur les travaux utilisant des regression forest. Les auteurs introduisent une nouvelle m{\'{e}}thode permettant de faire de la localization sur un environnement initiallement inconnu (comme Glocker), en pre-entrainant les forets sur des sets connus puis en coupant les feuilles. Donc m{\'{e}}thode plus orient{\'{e}}e sur la r{\'{e}}cup{\'{e}}ration de pose suite {\`{a}} la perte de tracking mais plus interessantes que les m{\'{e}}thodes bas{\'{e}} keyframe car elle peut faire de la regression de pose sur des positions diff{\'{e}}rentes que celles de la trajectoire initiale.

Ce que je n'ai pas compris
- Pas lu en d{\'{e}}taille

Ce qui est int{\'{e}}ressant
- Transfert learning mais pour les RF
- Peut interpoller des positions non-d{\'{e}}j{\`{a}} visit{\'{e}} ({\`{a}} la diff{\'{e}}rence des m{\'{e}}thodes bas{\'{e}}s keyframe)

Critiques
- Je trouve quand m{\^{e}}me que les temps de calcul long (bien qu'ils soient sur une GTX Titan Black) et qu'ils ont du faire beaucoup d'optimisation GPU
- Necessite la profondeur
- Ne marche pas comme une m{\'{e}}thode de localization absolue, parce qu'on a quand m{\^{e}}me besoin de r{\'{e}}cup{\'{e}}rer un certain nombre de point au d{\'{e}}but avant de pouvoir faire la regression)

A approfondir
- La m{\'{e}}thodes de SLAM avec loop closur online (ECCV2016), pour citation

Computational load
Offline: - (juste le pr{\'{e}}training, mais {\`{a}} faire qu'une seule fois)
Online 150ms (10ms pour la m{\'{e}}thode random fern)

Scalability
Scale: room
Scalability potential: No ?},
archivePrefix = {arXiv},
arxivId = {1702.02779},
author = {Cavallari, Tommaso and Golodetz, Stuart and Lord, Nicholas A. and Valentin, Julien and {Di Stefano}, Luigi and Torr, Philip H. S.},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {1702.02779},
file = {:home/nathan/Documents/Mendeley Desktop/Cavallari et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Cavallari et al.{\_}2017{\_}On-the-Fly Adaptation of Regression Forests for Online Camera Relocalisation.pdf:pdf},
title = {{On-the-Fly Adaptation of Regression Forests for Online Camera Relocalisation}},
year = {2017}
}
@article{Krajnik2017,
abstract = {—We present a method for introducing representation of dynamics into environment models that were originally tailored to represent static scenes. Rather than using a fixed probability value, the method models the uncertainty of the elementary environment states by probabilistic functions of time. These are composed of combinations of harmonic functions, which are obtained by means of frequency analysis. The use of frequency analysis allows to integrate long-term observations into memory-efficient spatio-temporal models that reflect the mid-to long-term environment dynamics. These frequency-enhanced spatio-temporal models allow to predict the future environment states, which improves the efficiency of mobile robot operation in changing environments. In a series of experiments performed over periods of days to years, we demonstrate that the proposed approach improves localization, path planning and exploration.},
annote = {Mod{\'{e}}lisation de l'apparition des features par {\'{e}}tude de la fr{\'{e}}quence d'apparition des features (par analyse de fourier), puis pr{\'{e}}diction.},
author = {Krajn{\'{i}}k, Tom{\'{a}}{\v{s}} and Fentanes, Jaime P. and Santos, Joao M. and Duckett, Tom},
doi = {10.1109/TRO.2017.2665664},
file = {:home/nathan/Documents/Mendeley Desktop/Krajn{\'{i}}k et al/IEEE Transactions on Robotics (ToR)/Krajn{\'{i}}k et al.{\_}2017{\_}FreMEn Frequency Map Enhancement for Long-Term Mobile Robot Autonomy in Changing Environments(2).pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics (ToR)},
pages = {1--14},
title = {{FreMEn: Frequency Map Enhancement for Long-Term Mobile Robot Autonomy in Changing Environments}},
year = {2017}
}
@article{Arandjelovic2017,
abstract = {We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the "Vector of Locally Aggregated Descriptors" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture obtains a large improvement in performance over non-learnt image representations as well as significantly outperforms off-the-shelf CNN descriptors on two challenging place recognition benchmarks.},
archivePrefix = {arXiv},
arxivId = {1511.07247},
author = {Arandjelovi{\'{c}}, Relja and Gronat, Petr and Torii, Akihiko and Pajdla, Tomas and Sivic, Josef},
doi = {10.1109/CVPR.2016.572},
eprint = {1511.07247},
file = {:home/nathan/Documents/Mendeley Desktop/Arandjelovi{\'{c}} et al/IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)/Arandjelovi{\'{c}} et al.{\_}2017{\_}NetVLAD CNN architecture for weakly supervised place recognition.pdf:pdf},
isbn = {9781467388511},
issn = {10636919},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
pages = {5297--5307},
pmid = {2079951},
title = {{NetVLAD: CNN architecture for weakly supervised place recognition}},
url = {http://arxiv.org/abs/1511.07247},
year = {2017}
}
@article{Carlucci2016,
abstract = {Convolutional Neural Networks (CNNs) trained on large scale RGB databases have become the secret sauce in the majority of recent approaches for object categorization from RGB-D data. Thanks to colorization techniques, these methods exploit the filters learned from 2D images to extract meaningful representations in 2.5D. Still, the perceptual signature of these two kind of images is very different, with the first usually strongly characterized by textures, and the second mostly by silhouettes of objects. Ideally, one would like to have two CNNs, one for RGB and one for depth, each trained on a suitable data collection, able to capture the perceptual properties of each channel for the task at hand. This has not been possible so far, due to the lack of a suitable depth database. This paper addresses this issue, proposing to opt for synthetically generated images rather than collecting by hand a 2.5D large scale database. While being clearly a proxy for real data, synthetic images allow to trade quality for quantity, making it possible to generate a virtually infinite amount of data. We show that the filters learned from such data collection, using the very same architecture typically used on visual data, learns very different filters, resulting in depth features (a) able to better characterize the different facets of depth images, and (b) complementary with respect to those derived from CNNs pre-trained on 2D datasets. Experiments on two publicly available databases show the power of our approach.},
archivePrefix = {arXiv},
arxivId = {1609.09713},
author = {Carlucci, Fabio Maria and Russo, Paolo and Caputo, Barbara},
doi = {10.1109/ICRA.2017.7989162},
eprint = {1609.09713},
file = {:home/nathan/Documents/Mendeley Desktop/Carlucci, Russo, Caputo/arXiv preprint/Carlucci, Russo, Caputo{\_}2016{\_}A deep representation for depth images from synthetic data.pdf:pdf},
isbn = {9781509046324},
issn = {10504729},
journal = {arXiv preprint},
title = {{A deep representation for depth images from synthetic data}},
url = {http://arxiv.org/abs/1609.09713},
year = {2016}
}
@article{Glocker2013,
abstract = {We introduce an efficient camera relocalization approach which can be easily integrated into real-time 3D reconstruction methods, such as KinectFusion. Our approach makes use of compact encoding of whole image frames which enables both online harvesting of keyframes in tracking mode, and fast retrieval of pose proposals when tracking is lost. The encoding scheme is based on randomized ferns and simple binary feature tests. Each fern generates a small block code, and the concatenation of codes yields a compact representation of each camera frame. Based on those representations we introduce an efficient frame dissimilarity measure which is defined via the block-wise hamming distance (BlockHD). We illustrate how BlockHDs between a query frame and a large set of keyframes can be simultaneously evaluated by traversing the nodes of the ferns and counting image co-occurrences in corresponding code tables. In tracking mode, this mechanism allows us to consider every frame/pose pair as a potential keyframe. A new keyframe is added only if it is sufficiently dissimilar from all previously stored keyframes. For tracking recovery, camera poses are retrieved that correspond to the keyframes with smallest BlockHDs. The pose proposals are then used to reinitialize the tracking algorithm. Harvesting of keyframes and pose retrieval are computationally efficient with only small impact on the run-time performance of the 3D reconstruction. Integrating our relocalization method into KinectFusion allows seamless continuation of mapping even when tracking is frequently lost. Additionally, we demonstrate how marker-free augmented reality, in particular, can benefit from this integration by enabling a smoother and continuous AR experience.},
author = {Glocker, Ben and Izadi, Shahram and Shotton, Jamie and Criminisi, Antonio},
doi = {10.1109/ISMAR.2013.6671777},
file = {:home/nathan/Documents/Mendeley Desktop/Glocker et al/Proceedings of the IEEE International Symposium on Mixed and Augmented Reality (ISMAR)/Glocker et al.{\_}2013{\_}Real-time RGB-D camera relocalization.pdf:pdf},
isbn = {9781479928699},
journal = {Proceedings of the IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
pages = {173--179},
title = {{Real-time RGB-D camera relocalization}},
year = {2013}
}
@inproceedings{Camposeco2016,
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Camposeco, Federico and Sattler, Torsten and Pollefeys, Marc},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-319-46454-1_13},
eprint = {1311.2901},
file = {:home/nathan/Documents/Mendeley Desktop/Camposeco, Sattler, Pollefeys/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Camposeco, Sattler, Pollefeys{\_}2016{\_}Minimal solvers for generalized pose and scale estimation from two rays and one point.pdf:pdf},
isbn = {9783319464534},
issn = {16113349},
keywords = {Absolute camera pose,Generalized cameras,Pose solver},
pages = {202--218},
pmid = {4520227},
title = {{Minimal solvers for generalized pose and scale estimation from two rays and one point}},
volume = {9909 LNCS},
year = {2016}
}
@inproceedings{Kneip2014opengv,
author = {Kneip, Laurent and Furgale, Paul},
booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {1--8},
title = {{OpenGV: A unified and generalized approach to real-time calibrated geometric vision}},
year = {2014}
}
@inproceedings{Hoffman2016,
abstract = {We present a modality hallucination architecture for training an RGB object detection model which incorpo- rates depth side information at training time. Our con- volutional hallucination network learns a new and com- plementary RGB image representation which is taught to mimic convolutional mid-level features from a depth net- work. At test time images are processed jointly through the RGB and hallucination networks to produce improved detection performance. Thus, our method transfers infor- mation commonly extracted from depth training data to a network which can extract that information from the RGB counterpart. We present results on the standard NYUDv2 dataset and report improvement on the RGB detection task.},
author = {Hoffman, Judy and Gupta, Saurabh and Darrell, Trevor},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.96},
file = {:home/nathan/Documents/Mendeley Desktop/Hoffman, Gupta, Darrell/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Hoffman, Gupta, Darrell{\_}2016{\_}Learning with Side Information through Modality Hallucination.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
pages = {826--834},
title = {{Learning with Side Information through Modality Hallucination}},
url = {http://ieeexplore.ieee.org/document/7780465/},
year = {2016}
}
@article{Shotton2013,
abstract = {We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel's correspondence to 3D points in the scene's world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines. View full abstract},
annote = {R{\'{e}}sum{\'{e}}

M{\'{e}}thode de localisation spatiale {\`{a}} partir d'une image RGBD et d'un classifieur type regression forest. Apprentissage sur base d'images RGBD avec position de la cam{\'{e}}ra associ{\'{e}}e. Recalage pr{\'{e}}cis en second {\'{e}}tape par minimisation d'erreur sur mod{\`{e}}le 3D (type ICP).

Ce que je n'ai pas compris

- Methode Preemtive RANSAC
- M{\'{e}}thode d'entrainement de la foret
- Mode fitting

Ce qui est int{\'{e}}ressant

- Recallage pr{\'{e}}cis de la cam{\'{e}}ra
- Possibilit{\'{e}} d'utilisation de ce syst{\`{e}}me pour de la scene recognition
- Relativement rapide (mise {\`{a}} part la m{\'{e}}thode d'apprentissage)
- Data base 7 scenes

Critiques

- Utilisation des pixels RGB tr{\`{e}}s mauvaise (on utilise juste l'intensit{\'{e}} du pixel)
- R{\'{e}}sultats sur des dataset de tr{\`{e}}s petites tailles
- Pas tr{\`{e}}s efficaces dans le cas de sc{\`{e}}nes avec motifs redondants
- Que des tests en ent{\'{e}}rieur

A approfondir

- Possibilit{\'{e}} de g{\'{e}}n{\'{e}}rer des images RGBD issues de la reconstruction 3D
- Meilleur utilisation des pixels RGB avec des descripteurs locals

Computational load
Offline: 1-10 min/scene
Online: 100 ms

Scalability
Scale: Room
Scalability potential: Low},
author = {Shotton, Jamie and Glocker, Ben and Zach, Christopher and Izadi, Shahram and Criminisi, Antonio and Fitzgibbon, Andrew},
doi = {10.1109/CVPR.2013.377},
file = {:home/nathan/Documents/Mendeley Desktop/Shotton et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Shotton et al.{\_}2013{\_}Scene coordinate regression forests for camera relocalization in RGB-D images.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
keywords = {3D models,Alignement,ICP,Indoor,Machine learning,RANSAC,RGBD request,Robotic,SVD,Visual place recognition},
mendeley-tags = {3D models,Alignement,ICP,Indoor,Machine learning,RANSAC,RGBD request,Robotic,SVD,Visual place recognition},
pages = {2930--2937},
title = {{Scene coordinate regression forests for camera relocalization in RGB-D images}},
year = {2013}
}
@inproceedings{Heisterklaus2014,
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de localisation visuelle {\`{a}} partir d'un mod{\`{e}}le 3D (SfM). Compression du mod{\`{e}}le par l'utilisation d'un descripteur global (MPEG) qui encapsule des descripteurs locaux quantifi{\'{e}}s (SIFT+FV). Ajout de vues syth{\'{e}}tiques pour couvrir l'ensemble du mod{\`{e}}le (selection des keypoints des vues par d{\'{e}}tection d'occlusion et ranking de la precision des descripteurs locaux). Pipeline classique lors d'une requ{\^{e}}te en comparant les descripteurs globaux puis les descripteurs locaux (r{\'{e}}sultats pour les modes quantifi{\'{e}}s et non-quantifi{\'{e}}s). Reconstruction de la position par correspondance 2D/3D (6 points) puis DLT puis optimisation par Levenberg-Marquardt

Ce qui est int{\'{e}}ressant
- La compression des donn{\'{e}}es
- Le crit{\`{e}}res de selection des keypoints lors de la cr{\'{e}}ation d'une vue sythetique

Critiques
- Pas de temps de calcul
- Test sur des images originalement utilis{\'{e}}es pour la SfM

A approfondir
- MPEG compression et SCFV

Computational load
Offline: View synthesis global image comparison
Online: Linear comparison of global descriptor

Scalability
Scale: Place
Scalability potential: Possible with increasing cost time implication},
author = {Heisterklaus, Iris and Qian, Ningqing and Miller, Artur},
booktitle = {IEEE International Conference on Consumer Electronics Berlin (ICCE-Berlin)},
file = {:home/nathan/Documents/Mendeley Desktop/Heisterklaus, Qian, Miller/IEEE International Conference on Consumer Electronics Berlin (ICCE-Berlin)/Heisterklaus, Qian, Miller{\_}2014{\_}Image-based pose estimation using a compact 3D model.pdf:pdf},
isbn = {9781479961658},
keywords = {Global features,Local features,SfM,View Synthesis},
mendeley-tags = {Global features,Local features,SfM,View Synthesis},
pages = {327--330},
title = {{Image-based pose estimation using a compact 3D model}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=7034307},
year = {2014}
}
@inproceedings{Chan2016,
author = {Chan, Jacob and Lee, Jimmy Addison and Kemao, Qian},
booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
file = {:home/nathan/Documents/Mendeley Desktop/Chan, Lee, Kemao/Proceedings of the Asian Conference on Computer Vision (ACCV)/Chan, Lee, Kemao{\_}2016{\_}F-SORT An Alternative for Faster Geometric Verification.pdf:pdf},
pages = {1--15},
title = {{F-SORT : An Alternative for Faster Geometric Verification}},
year = {2016}
}
@inproceedings{Kendall2016,
abstract = {We present a robust and real-time monocular six degree of freedom visual relocalization system. We use a Bayesian convolutional neural network to regress the 6-DOF camera pose from a single RGB image. It is trained in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking under 6ms to compute. It obtains approximately 2m and 6 degrees accuracy for very large scale outdoor scenes and 0.5m and 10 degrees accuracy indoors. Using a Bayesian convolutional neural network implementation we obtain an estimate of the model's relocalization uncertainty and improve state of the art localization accuracy on a large scale outdoor dataset. We leverage the uncertainty measure to estimate metric relocalization error and to detect the presence or absence of the scene in the input image. We show that the model's uncertainty is caused by images being dissimilar to the training dataset in either pose or appearance.},
annote = {R{\'{e}}sum{\'{e}}

Am{\'{e}}lioration du CNN PoseNet qui permet de regresser la position d'ou a ete prise une image. Modification de r{\'{e}}seau pour qu'il soit un Bayesian CNN afin d'am{\'{e}}liorer la qualit{\'{e}} de la pr{\'{e}}diction (grace a une moyenne sur les Monte Carlo dropout), et d'obtenir une estimation de l'insertitude sur la postion renvoy{\'{e}} (pour {\'{e}}ventuellement faire du rejet ou de la classification).

Ce que je n'ai pas compris

- Le principe du dropout et comment on obtient plusieurs estimation de la postion
- Section IV

Ce qui est int{\'{e}}ressant

- l'insertitude obtenu par le r{\'{e}}seau
- le temps de calcul

Critiques

- M{\^{e}}me remarque que sur le papier PoseNet (dataset peu challenging, r{\'{e}}sultat pas formidable, pas de recallage pr{\'{e}}cis)

A approfondir

- Le dropout

Computational load
Offline: CNN pretrained
Online: Near real-time (6ms), bit more than Kendal et al. 2015

Scalability
Scale: Place
Scalability potential: Theorically possible},
archivePrefix = {arXiv},
arxivId = {1509.05909},
author = {Kendall, Alex and Cipolla, Roberto},
booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
eprint = {1509.05909},
file = {:home/nathan/Documents/Mendeley Desktop/Kendall, Cipolla/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Kendall, Cipolla{\_}2016{\_}Modelling Uncertainty in Deep Learning for Camera Relocalization.pdf:pdf},
keywords = {3D models,Bayesian probability,CNN,Global features,Indoor,Long terme place recognition,Outdoor,RGB request,Visual place recognition},
mendeley-tags = {3D models,Bayesian probability,CNN,Global features,Indoor,Long terme place recognition,Outdoor,RGB request,Visual place recognition},
title = {{Modelling Uncertainty in Deep Learning for Camera Relocalization}},
year = {2016}
}
@article{Kim2017,
abstract = {We propose a vision-based method that localizes a ground vehicle using publicly available satellite imagery as the only prior knowledge of the environment. Our approach takes as input a sequence of ground-level images acquired by the vehicle as it navigates, and outputs an estimate of the vehicle's pose relative to a georeferenced satellite image. We overcome the significant viewpoint and appearance variations between the images through a neural multi-view model that learns location-discriminative embeddings in which ground-level images are matched with their corresponding satellite view of the scene. We use this learned function as an observation model in a filtering framework to maintain a distribution over the vehicle's pose. We evaluate our method on different benchmark datasets and demonstrate its ability localize ground-level images in environments novel relative to training, despite the challenges of significant viewpoint and appearance variations.},
archivePrefix = {arXiv},
arxivId = {1704.01133},
author = {Kim, Dong-Ki and Walter, Matthew R.},
eprint = {1704.01133},
file = {:home/nathan/Documents/Mendeley Desktop/Kim, Walter/Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)/Kim, Walter{\_}2017{\_}Satellite Image-based Localization via Learned Embeddings.pdf:pdf},
journal = {Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)},
title = {{Satellite Image-based Localization via Learned Embeddings}},
url = {http://arxiv.org/abs/1704.01133},
year = {2017}
}
@article{Bay2008,
abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1016/j.cviu.2007.09.014},
file = {:home/nathan/Documents/Mendeley Desktop/Bay et al/Computer Vision and Image Understanding (CVIU)/Bay et al.{\_}2008{\_}Speeded-Up Robust Features (SURF).pdf:pdf},
isbn = {9783540338321},
issn = {10773142},
journal = {Computer Vision and Image Understanding (CVIU)},
keywords = {Camera calibration,Feature description,Interest points,Local features,Object recognition},
number = {3},
pages = {346--359},
pmid = {16081019},
title = {{Speeded-Up Robust Features (SURF)}},
volume = {110},
year = {2008}
}
@inproceedings{Cadena2010,
author = {Cadena, C{\'{e}}sar and Galvez-Lopez, Dorian and Ramos, Fabio and Tard{\'{o}}s, Juan D. and Neira, Jos{\'{e}}},
booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2010.5650234},
file = {:home/nathan/Documents/Mendeley Desktop/Cadena et al/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Cadena et al.{\_}2010{\_}Robust Place Recognition with Stereo Cameras.pdf:pdf},
isbn = {9781424466764},
pages = {5182--5189},
title = {{Robust Place Recognition with Stereo Cameras}},
year = {2010}
}
@article{Naseer2017a,
author = {Naseer, Tayyab and Oliveira, Gabriel L. and Brox, Thomas and Burgard, Wolfram},
file = {:home/nathan/Documents/Mendeley Desktop/Naseer et al/Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)/Naseer et al.{\_}2017{\_}Semantics-aware Visual Localization under Challenging Perceptual Conditions.pdf:pdf},
isbn = {9781509046324},
journal = {Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)},
pages = {2614--2620},
title = {{Semantics-aware Visual Localization under Challenging Perceptual Conditions}},
year = {2017}
}
@article{Zhao2017,
author = {Zhao, Xin and Ding, Guiguang and Guo, Yuchen and Han, Jungong and Gao, Yue},
file = {:home/nathan/Documents/Mendeley Desktop/Zhao et al/Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)/Zhao et al.{\_}2017{\_}TUCH Turning Cross-view Hashing into Single-view Hashing via Generative Adversarial Nets.pdf:pdf},
journal = {Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
keywords = {Deep Learning,Feature Selection/Construction,Information Retrieval,Machine Learning,Natural Language Processing},
number = {61571269},
pages = {3511--3517},
title = {{TUCH: Turning Cross-view Hashing into Single-view Hashing via Generative Adversarial Nets}},
year = {2017}
}
@inproceedings{Rublee2011,
author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
file = {:home/nathan/Documents/Mendeley Desktop/Rublee et al/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Rublee et al.{\_}2011{\_}ORB an efficient alternative to SIFT or SURF.pdf:pdf},
title = {{ORB: an efficient alternative to SIFT or SURF}},
year = {2011}
}
@inproceedings{McManus2014,
abstract = {This paper is about localising across extreme light- ing and weather conditions. We depart from the traditional point-feature-based approach since matching under dramatic appearance changes is a brittle and hard. Point-feature detectors are rigid procedures which pass over an image examining small, low-level structure such as corners or blobs. They apply the same criteria to all images of all places. This paper takes a contrary view and asks what is possible if instead we learn a bespoke detector for every place. Our localisation task then turns into curating a large bank of spatially indexed detectors and we show that this yields vastly superior performance in terms of robustness in exchange for a reduced but tolerable metric precision. We present an unsupervised system that produces broad-region detectors for distinctive visual elements, called scene signatures, which can be associated across almost all appearance changes. We show, using 21km of data collected over a period of 3 months, that our system is capable of producing metric estimates from night-to-day or summer-to-winter conditions.},
annote = {R{\'{e}}sum{\'{e}}

M{\'{e}}thode de VO pour faire de la regression de poses {\`{a}} partir d'images st{\'{e}}r{\'{e}}o dans des conditions tr{\`{e}}s challenging. Utilisation de descripteurs globaux appris sur un grand nombre d'images d'un m{\^{e}}me lieu {\`{a}} des p{\'{e}}riodes diff{\'{e}}rentes de l'ann{\'{e}}e. Cr{\'{e}}ation de classifieurs (SVM) pour chaque lieu enregistr{\'{e}}, et utilisation des patches extraits des images pour recalculer la pose de la cam{\'{e}}ra.

Ce que je n'ai pas compris

- Le calcul de la postion de la cam{\'{e}}ra par rapport aux patches
- Comment on calcul les patches lors d'une query ? 

Ce qui est int{\'{e}}ressant

- Temps r{\'{e}}el avec l'impl{\'{e}}mentation C++
- M{\'{e}}thode d'allignement
- M{\'{e}}thode de description de l'image

Critiques

- Temps de calcul lors de la creation des SVM
- Position a priori de la requete
- Image RGBD en requete
- On a besoin de beaucoup d'images du m{\^{e}}me lieu, avec le m{\^{e}}me point de vue

A approfondir

- Descripteurs locaux utilis{\'{e}}es

Computational load
Offline: 1h/place
Online: 2/5hz

Scalability
Scale: Place
Scalability potential: Theoricaly feasible but hard due to the large amounth of data needed by place},
author = {McManus, Colin and Upcroft, Ben and Newman, Paul},
booktitle = {Robotics Science and Systems (RSS)},
file = {:home/nathan/Documents/Mendeley Desktop/McManus, Upcroft, Newman/Robotics Science and Systems (RSS)/McManus, Upcroft, Newman{\_}2014{\_}Scene Signatures Localised and Point-less Features for Localisation.pdf:pdf},
isbn = {9780992374709},
keywords = {Alignement,Challenging matching,HOG,Machine learning,Outdoor,RGBD request,SVM,Unsupervised learning,Visual place recognition},
mendeley-tags = {Alignement,Challenging matching,HOG,Machine learning,Outdoor,RGBD request,SVM,Unsupervised learning,Visual place recognition},
title = {{Scene Signatures : Localised and Point-less Features for Localisation}},
year = {2014}
}
@inproceedings{Torralba2003,
abstract = {While navigating in an environment, a vision system has to be able to recognize where it is and what the main objects in the scene are. We present a context-based vision system for place and object recognition. The goal is to identify familiar locations (e.g., office 610, conference room 941, main street), to categorize new environments (office, corridor, street) and to use that information to provide contextual priors for object recognition (e.g., tables are more likely in an office than a street). We present a low-dimensional global image representation that provides relevant information for place recognition and categorization, and show how such contextual information introduces strong priors that simplify object recognition. We have trained the system to recognize over 60 locations (indoors and outdoors) and to suggest the presence and locations of more than 20 different object types. The algorithm has been integrated into a mobile system that provides realtime feedback to the user.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Torralba, Antonio and Murphy, Kevin P. and Freeman, William T. and Rubin, Mark A.},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2003.1238354},
eprint = {9411012},
file = {:home/nathan/Documents/Mendeley Desktop/Torralba et al/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Torralba et al.{\_}2003{\_}Context-based vision system for place and object recognition(2).pdf:pdf},
isbn = {0-7695-1950-4},
issn = {0769519504},
pages = {273--280},
pmid = {15208015},
primaryClass = {chao-dyn},
title = {{Context-based vision system for place and object recognition}},
volume = {1},
year = {2003}
}
@inproceedings{Robertson2004,
abstract = {We describe the prototype of a system intended to allow a user to navigate in an urban environment using a mobile telephone equipped with a camera. The system uses a database of views of building facades to determine the pose of a query view provided by the user. Our method is based on a novel wide-baseline matching algorithm that can identify corresponding building facades in two views despite significant changes of viewpoint and lighting. We show that our system is capable of localising query views reliably in a large part of Cambridge city centre.},
author = {Robertson, Duncan and Cipolla, Roberto},
booktitle = {British Machine Vision Conference (BMVC)},
doi = {10.5244/C.18.84},
file = {:home/nathan/Documents/Mendeley Desktop/Robertson, Cipolla/British Machine Vision Conference (BMVC)/Robertson, Cipolla{\_}2004{\_}An Image-Based System for Urban Navigation.pdf:pdf},
title = {{An Image-Based System for Urban Navigation}},
year = {2004}
}
@inproceedings{Schindler2007,
abstract = {We look at the problem of location recognition in a large image dataset using a vocabulary tree. This entails finding the location of a query image in a large dataset containing 3 × 10{\^{}}4 streetside images of a city. We investigate how the traditional invariant feature matching approach falls down as the size of the database grows. In particular we show that by carefully selecting the vocabulary using the most informative features, retrieval performance is significantly improved, allowing us to increase the number of database images by a factor of 10. We also introduce a generalization of the traditional vocabulary tree search algorithm which improves performance by effectively increasing the branch- ing factor of a fixed vocabulary tree.},
annote = {R{\'{e}}sum{\'{e}}
Utilisation et am{\'{e}}lioration des vocabulary trees pour faire du large scale image loclization based on image-retrivial. Base de donn{\'{e}}es de 30k images. Deux am{\'{e}}liorations notables : permet de faire de lar recherche multi-chemin (Greddy N-Best != Best Bin First) pour augmenter le nombre de comparaison entre les noeuds et la requ{\^{e}}te sans augmenter le nombre de clusters k. Deuxi{\`{e}}me apport : dessimation de la base de donn{\'{e}}es des visuals words avant de construire les arbres en ne choississant que les plus discriminatifs (en calculant une grandeur relative {\`{a}} l'entropie appel{\'{e}} Inforamtion Gain)

Ce que je n'ai pas compris
- Pourquoi le calcul de l'entropie se fait ainsi ?

Ce qui est int{\'{e}}ressant
- Le temps de calcul adaptatif (bons r{\'{e}}sultats pour 0.25s)
- Base de donn{\'{e}}e relativement grande

Critiques
- Besoin d'une base de donn{\'{e}}e avec beaucoup d'overlapping

Computational load
Offline: Advanced vocabulary tree
Online: {\textless} 1s

Scalability
Scale: City
Scalability potential: Theoricaly more robuste than other methods},
author = {Schindler, Grant and Brown, Matthew and Szeliski, Richard},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Schindler, Brown, Szeliski/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Schindler, Brown, Szeliski{\_}2007{\_}City-Scale Location Recognition.pdf:pdf},
isbn = {1424411807},
keywords = {Local features,Outdoor,SIFT,Visual place recognition,Visual words},
mendeley-tags = {Local features,Outdoor,SIFT,Visual place recognition,Visual words},
title = {{City-Scale Location Recognition}},
year = {2007}
}
@book{Forstner2016,
author = {F{\"{o}}rstner, Wolfgang and Wrobel, Bernhard P.},
publisher = {Springer},
title = {{Photogrammetric Computer Vision}},
year = {2016}
}
@inproceedings{Linegar2016,
abstract = {— This paper is about camera-only localisation in challenging outdoor environments, where changes in lighting, weather and season cause traditional localisation systems to fail. Conventional approaches to the localisation problem rely on point-features such as SIFT, SURF or BRIEF to associate landmark observations in the live image with landmarks stored in the map; however, these features are brittle to the severe appearance change routinely encountered in outdoor environ-ments. In this paper, we propose an alternative to traditional point-features: we train place-specific linear SVM classifiers to recognise distinctive elements in the environment. The core contribution of this paper is an unsupervised mining algorithm which operates on a single mapping dataset to extract distinct elements from the environment for localisation. We evaluate our system on 205 km of data collected from central Oxford over a period of six months in bright sun, night, rain, snow and at all times of the day. Our experiment consists of a comprehensive N-vs-N analysis on 22 laps of the approximately 10 km route in central Oxford. With our proposed system, the portion of the route where localisation fails is reduced by a factor of 6, from 33.3{\%} to 5.5{\%}.},
author = {Linegar, Chris and Churchill, Winston and Newman, Paul},
booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2016.7487208},
file = {:home/nathan/Documents/Mendeley Desktop/Linegar, Churchill, Newman/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Linegar, Churchill, Newman{\_}2016{\_}Made to measure Bespoke landmarks for 24-hour, all-weather localisation with a camera.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
pages = {787--794},
title = {{Made to measure: Bespoke landmarks for 24-hour, all-weather localisation with a camera}},
volume = {2016-June},
year = {2016}
}
@article{Sattler2016a,
abstract = {—Accurately determining the position and orientation from which an image was taken, i.e., computing the camera pose, is a fundamental step in many Computer Vision applications. The pose can be recovered from 2D-3D matches between 2D image positions and points in a 3D model of the scene. Recent advances in Structure-from-Motion allow us to reconstruct large scenes and thus create the need for image-based localization methods that efficiently handle large-scale 3D models while still being effective, i.e., while localizing as many images as possible. This paper presents an approach for large scale image-based localization that is both efficient and effective. At the core of our approach is a novel prioritized matching step that enables us to first consider features more likely to yield 2D-to-3D matches and to terminate the correspondence search as soon as enough matches have been found. Matches initially lost due to quantization are efficiently recovered by integrating 3D-to-2D search. We show how visibility information from the reconstruction process can be used to improve the efficiency of our approach. We evaluate the performance of our method through extensive experiments and demonstrate that it offers the best combination of efficiency and effectiveness among current state-of-the-art approaches for localization.},
annote = {R{\'{e}}sum{\'{e}}
Pipeline pour l'image based localization sur un mod{\`{e}}le 3D reconstruit par SfM. Trois grands apports : une m{\'{e}}thode de prioretisation des features au travers d'un visual dictionnary (par lineare search de keypoint issue de query images en mode base d'apprentissage pour renforcer les features dicriminantes par ratio Lowe). Un syst{\`{e}}me permettant de faire du 3D to 2D match {\`{a}} partir d'une feature issue du matching 2D to 3D en r{\'{e}}cup{\'{e}}rant ses plus proche voisins g{\'{e}}om{\'{e}}trique (grace a {\`{a}} dictonnary tree). Et une m{\'{e}}thode de filtrage des plus proches voisins et des inliers se basant sur un graph de co- visibilit{\'{e}} (avec ajout d'un clutering des cam{\'{e}}ras qui se situent proches les unes des autres). 
Tests sur beaucoup de datasets.

Ce que je n'ai pas compris
- Principe et application du VPS

Ce qui est int{\'{e}}ressant
- R{\'{e}}sultats
- Vitesse de clacul ++
- Papier tr{\`{e}}s bien {\'{e}}crit

Critiques
- Fonctionne seulement sur modele SfM
- Pas de comparaison avec les derni{\`{e}}res m{\'{e}}thodes de image based-retrieval localization (NetVlad)

A approfondir
- SRT-RANSAC

Computational load
Offline: Construction du viusal dictonnary
Online: {\textless} 1s

Scalability
Scale: City
Scalability potential: Real (with low computational cost augmentation)},
author = {Sattler, Torsten and Leibe, Bastian and Kobbelt, Leif},
doi = {10.1109/TPAMI.2016.2611662},
file = {:home/nathan/Documents/Mendeley Desktop/Sattler, Leibe, Kobbelt/IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)/Sattler, Leibe, Kobbelt{\_}2016{\_}Efficient {\&} Effective Prioritized Matching for Large-Scale Image-Based Localization.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
keywords = {Camera Pose Estimation,Index Terms—Image-based Localization,Location Recognition,Prioritized Feature Matching},
number = {1},
title = {{Efficient {\&} Effective Prioritized Matching for Large-Scale Image-Based Localization}},
volume = {X},
year = {2016}
}
@inproceedings{Bai2017a,
author = {Bai, Song and Zhou, Zhichao and Wang, Jingdong and Bai, Xiang and Latecki, Longin Jan and Tian, Qi},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.90},
file = {:home/nathan/Documents/Mendeley Desktop/Bai et al/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Bai et al.{\_}2017{\_}Ensemble Diffusion for Retrieval.pdf:pdf},
pages = {774--783},
title = {{Ensemble Diffusion for Retrieval}},
year = {2017}
}
@article{Qu2015,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Qu, Xiaozhi and Soheilian, Bahman and Paparoditis, Nicolas},
doi = {10.1109/IVS.2015.7225751},
eprint = {arXiv:1011.1669v3},
file = {:home/nathan/Documents/Mendeley Desktop/Qu, Soheilian, Paparoditis/Proceedings of the IEEE Intelligent Vehicles Symposium (IV)/Qu, Soheilian, Paparoditis{\_}2015{\_}Vehicle localization using mono-camera and geo-referenced traffic signs.pdf:pdf},
isbn = {9781467372664},
issn = {1098-6596},
journal = {Proceedings of the IEEE Intelligent Vehicles Symposium (IV)},
pages = {605--610},
pmid = {25246403},
title = {{Vehicle localization using mono-camera and geo-referenced traffic signs}},
volume = {2015-Augus},
year = {2015}
}
@inproceedings{Rocco2017,
abstract = {We address the problem of determining correspondences between two images in agreement with a geometric model such as an affine or thin-plate-spline transformation, and estimating its parameters. The contributions of this work are three-fold. First, we propose a convolutional neural net-work architecture for geometric matching. The architecture is based on three main components that mimic the standard steps of feature extraction, matching and simultaneous in-lier detection and model parameter estimation, while being trainable end-to-end. Second, we demonstrate that the net-work parameters can be trained from synthetically gener-ated imagery without the need for manual annotation and that our matching layer significantly increases generaliza-tion capabilities to never seen before images. Finally, we show that the same model can perform both instance-level and category-level matching giving state-of-the-art results on the challenging Proposal Flow dataset.},
archivePrefix = {arXiv},
arxivId = {1703.05593},
author = {Rocco, Ignacio and Arandjelovi{\'{c}}, Relja and Sivic, Josef},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {1703.05593},
file = {:home/nathan/Documents/Mendeley Desktop/Rocco, Arandjelovi{\'{c}}, Sivic/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Rocco, Arandjelovi{\'{c}}, Sivic{\_}2017{\_}Convolutional neural network architecture for geometric matching.pdf:pdf},
title = {{Convolutional neural network architecture for geometric matching}},
url = {https://arxiv.org/pdf/1703.05593.pdf},
year = {2017}
}
@article{Castaldo2015,
abstract = {Matching cross-view images is challenging because the appearance and viewpoints are significantly different. While low-level features based on gradient orientations or filter responses can drastically vary with such changes in viewpoint, semantic information of images however shows an invariant characteristic in this respect. Consequently, semantically labeled regions can be used for performing cross-view matching. In this paper, we therefore explore this idea and propose an automatic method for detecting and representing the semantic information of an RGB image with the goal of performing cross-view matching with a (non-RGB) geographic information system (GIS). A segmented image forms the input to our system with segments assigned to semantic concepts such as traffic signs, lakes, roads, foliage, etc. We design a descriptor to robustly capture both, the presence of semantic concepts and the spatial layout of those segments. Pairwise distances between the descriptors extracted from the GIS map and the query image are then used to generate a shortlist of the most promising locations with similar semantic concepts in a consistent spatial layout. An experimental evaluation with challenging query images and a large urban area shows promising results.},
archivePrefix = {arXiv},
arxivId = {1511.00098},
author = {Castaldo, Francesco and Zamir, Amir Roshan and Angst, Roland and Palmieri, Francesco and Savarese, Silvio},
doi = {10.1109/ICCVW.2015.137},
eprint = {1511.00098},
file = {:home/nathan/Documents/Mendeley Desktop/Castaldo et al/Proceedings of the IEEE International Conference on Computer Vision Workshop (ICCVW)/Castaldo et al.{\_}2015{\_}Semantic Cross-View Matching.pdf:pdf},
isbn = {9781467383905},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision Workshop (ICCVW)},
keywords = {Cameras,Distortion,Geographic information systems,Image segmentation,Layout,Satellites,Semantics},
pages = {1044--1052},
title = {{Semantic Cross-View Matching}},
volume = {2016-Febru},
year = {2015}
}
@article{Valgren2007,
abstract = {Local feature matching has become a commonly used method to compare images. For mobile robots, a reliable method for comparing images can constitute a key component for localization and loop closing tasks. In this paper, we address the issues of outdoor appearance-based topological localization for a mobile robot over time. Our data sets, each consisting of a large number of panoramic images, have been acquired over a period of nine months with large seasonal changes (snow- covered ground, bare trees, autumn leaves, dense foliage, etc.). Two different types of image feature algorithms, SIFT and the more recent SURF, have been used to compare the images. We show that two variants of SURF, called U-SURF and SURF-128, outperform the other algorithms in terms of accuracy and speed.},
author = {Valgren, Christoffer and Lilienthal, Achim J.},
file = {:home/nathan/Documents/Mendeley Desktop/Valgren, Lilienthal/Proceedings of the European Conference on Mobile Robots (ECMR)/Valgren, Lilienthal{\_}2007{\_}SIFT , SURF and Seasons Long-term Outdoor Localization Using Local Features.pdf:pdf},
journal = {Proceedings of the European Conference on Mobile Robots (ECMR)},
number = {February},
pages = {1--6},
title = {{SIFT , SURF and Seasons : Long-term Outdoor Localization Using Local Features}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.2497{\&}rep=rep1{\&}type=pdf},
volume = {128},
year = {2007}
}
@inproceedings{Wu2017a,
abstract = {— Convolutional Neural Networks (CNNs) have been applied to camera relocalization, which is to infer the pose of the camera given a single monocular image. However, there are still many open problems for camera relocalization with CNNs. We delve into the CNNs for camera relocalization. First, a variant of Euler angles named Euler6 is proposed to represent orientation. Then a data augmentation method named pose synthesis is designed to reduce sparsity of poses in the whole pose space to cope with overfitting in training. Third, a multi-task CNN named BranchNet is proposed to deal with the complex coupling of orientation and translation. The network consists of several shared convolutional layers and splits into two branches which predict orientation and translation, respectively. Experiments on the 7Scenes dataset show that incorporating these techniques one by one into an existing model PoseNet always leads to better results. Together these techniques reduce the orientation error by 15.9{\%} and the translation error by 38.3{\%} compared to the state-of-the-art model Bayesian PoseNet. We implement BranchNet on an Intel NUC mobile platform and reach a speed of 43 fps, which meets the real-time requirement of many robotic applications.},
author = {Wu, Jian and Ma, Liwei and Hu, Xiaolin},
booktitle = {Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)},
file = {:home/nathan/Documents/Mendeley Desktop/Wu, Ma, Hu/Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)/Wu, Ma, Hu{\_}2017{\_}Delving Deeper into Convolutional Neural Networks for Camera Relocalization.pdf:pdf},
isbn = {9781509046324},
keywords = {Computer Vision for Other Robotic Applications,Localization},
title = {{Delving Deeper into Convolutional Neural Networks for Camera Relocalization}},
year = {2017}
}
@article{Lowe2004,
author = {Lowe, David G.},
journal = {International Journal of Computer Vision (IJCV)},
number = {2},
pages = {91--110},
publisher = {Springer},
title = {{Distinctive image features from scale-invariant keypoints}},
volume = {60},
year = {2004}
}
@mastersthesis{Kumar2016mastersThesis,
author = {Kumar, Devinder},
booktitle = {arXiv preprint},
school = {University of Waterloo},
title = {{Deep Learning Based Place Recognition for Challenging Environments}},
year = {2016}
}
@article{Loquercio2017,
author = {Loquercio, Antonio and Dymczyk, Marcin and Zeisl, Bernhard and Lynen, Simon and Gilitschenski, Igor and Siegwart, Roland},
file = {:home/nathan/Documents/Mendeley Desktop/Loquercio et al/Unknown/Loquercio et al.{\_}2017{\_}Efficient Descriptor Learning for Large Scale Localization.pdf:pdf},
number = {March},
title = {{Efficient Descriptor Learning for Large Scale Localization}},
year = {2017}
}
@article{Finlayson2006,
author = {Finlayson, Graham D. and Hordley, Steven D. and Lu, Cheng and Drew, Mark S.},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
number = {1},
pages = {59--68},
publisher = {IEEE},
title = {{On the removal of shadows from images}},
volume = {28},
year = {2006}
}
@inproceedings{Piasco2016,
abstract = {{\textcopyright} 2016 IEEE.This paper considers collaborative stereo-vision as a mean of localization for a fleet of micro-air vehicles (MAV) equipped with monocular cameras, inertial measurement units and sonar sensors. A sensor fusion scheme using an extended Kalman filter is designed to estimate the positions and orientations of all the vehicles from these distributed measurements. The estimation is completed by a formation control to maximize the overlapping fields of view of the vehicles. Experimental tests for the complete perception and control loop have been performed on multiple MAVs with centralized processing on a ROS ground station.},
author = {Piasco, Nathan and Marzat, Julien and Sanfourche, Martial},
booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2016.7487251},
isbn = {9781467380263},
keywords = {[collaborative localization,formation control,mi},
title = {{Collaborative localization and formation flying using distributed stereo-vision}},
volume = {2016-June},
year = {2016}
}
@article{Hartley1997,
author = {Hartley, Richard},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
number = {6},
pages = {580--593},
publisher = {IEEE},
title = {{In defense of the eight-point algorithm}},
volume = {19},
year = {1997}
}
@article{Cevikalp2017,
author = {Cevikalp, Hakan and Elmas, Merve and {\"{O}}zkan, Savas},
doi = {10.1016/j.cviu.2017.07.004},
file = {:home/nathan/Documents/Mendeley Desktop/Cevikalp, Elmas, {\"{O}}zkan/Computer Vision and Image Understanding (CVIU)/Cevikalp, Elmas, {\"{O}}zkan{\_}2017{\_}Towards Category Based Large-Scale Image Retrieval Using Transductive Support Vector Machines.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding (CVIU)},
keywords = {Image retrieval,Ramp loss,hashing,semi-supervised learning,transductive support vector machines},
pages = {621--637},
publisher = {Elsevier Inc.},
title = {{Towards Category Based Large-Scale Image Retrieval Using Transductive Support Vector Machines.}},
url = {http://dblp.uni-trier.de/db/conf/eccv/eccv2016w1.html{\#}CevikalpEO16},
year = {2017}
}
@inproceedings{Li2015,
abstract = {Large-scale object 들에 관해 pairwise geometric matching을 이용해 retrieval 하는 방법 소개},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode d'image retrieval am{\'{e}}liorant le sch{\'{e}}ma classique des BoF (bag of features) en proc{\'{e}}dant {\`{a}} une verification g{\'{e}}om{\'{e}}trique dans l'espace de l'image. Les auteurs commencent par extraire des features (hessiant-affine + SURF + generic vocabulary of 20k word with Hamming Embedding weigthing scheme) et proc{\`{e}}dent {\`{a}} une premi{\`{e}}re {\'{e}}tape de matching (en utilisant une m{\'{e}}thode de matching favorisant un grand nombre de matching entre les points). Ils affinent ensuite les matches en comparant les echelles et orientations des features (Hough voting). Ils cr{\'{e}}ent finalement un score final en prenant en compte les matching individuels entre les deux images et les vecteurs cr{\'{e}}{\'{e}}s par la jonction de deux features (comparaison de la taille et de l'orientation des vecteurs). 

Ce que je n'ai pas compris
- Si les auteurs font bien deux {\'{e}}tapes de matching (1er classique BoF et ensuite PGM)

Ce qui est int{\'{e}}ressant
- Rare m{\'{e}}thode {\`{a}} utiliser cette information purement g{\'{e}}om{\'{e}}trique

Critiques
- Temps de calcul un peu long

A approfondir
- Hessian-affine detector

Computational load
Offline: BoF
Online: {\~{}}2s for 1000 test images

Scalability
Scale: City
Scalability potential: -},
author = {Li, Xinchao and Larson, Martha and Hanjalic, Alan},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2015.7299151},
file = {:home/nathan/Documents/Mendeley Desktop/Li, Larson, Hanjalic/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Li, Larson, Hanjalic{\_}2015{\_}Pairwise geometric matching for large-scale object retrieval.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pages = {5153--5161},
title = {{Pairwise geometric matching for large-scale object retrieval}},
volume = {07-12-June},
year = {2015}
}
@inproceedings{Liu2017a,
author = {Liu, Liu and Li, Hongdong and Dai, Yuchao},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
file = {:home/nathan/Documents/Mendeley Desktop/Liu, Li, Dai/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Liu, Li, Dai{\_}2017{\_}Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale 3D Map Australia Centre for Robotic Vision.pdf:pdf},
title = {{Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale 3D Map Australia Centre for Robotic Vision}},
year = {2017}
}
@article{Zhu2017,
abstract = {Place recognition is one of the most fundamental topics in computer vision and robotics communities, where the task is to accurately and efficiently recognize the location of a given query image. Despite years of wisdom accumulated in this field, place recognition still remains an open problem due to the various ways in which the appearance of real-world places may differ. This paper presents an overview of the place recognition literature. Since condition invariant and viewpoint invariant features are essential factors to long-term robust visual place recognition system, We start with traditional image description methodology developed in the past, which exploit techniques from image retrieval field. Recently, the rapid advances of related fields such as object detection and image classification have inspired a new technique to improve visual place recognition system, i.e., convolutional neural networks (CNNs). Thus we then introduce recent progress of visual place recognition system based on CNNs to automatically learn better image representations for places. Eventually, we close with discussions and future work of place recognition.},
archivePrefix = {arXiv},
arxivId = {1707.03470},
author = {Zhu, Chaoyang},
eprint = {1707.03470},
file = {:home/nathan/Documents/Mendeley Desktop/Zhu/arXiv preprint/Zhu{\_}2017{\_}Place recognition An Overview of Vision Perspective.pdf:pdf},
journal = {arXiv preprint},
number = {2007},
pages = {1--8},
title = {{Place recognition: An Overview of Vision Perspective}},
url = {http://arxiv.org/abs/1707.03470},
year = {2017}
}
@inproceedings{Wang2016,
abstract = {In this paper we introduce the TorontoCity benchmark, which covers the full greater Toronto area (GTA) with 712.5 {\$}km{\^{}}2{\$} of land, 8439 {\$}km{\$} of road and around 400,000 buildings. Our benchmark provides different perspectives of the world captured from airplanes, drones and cars driving around the city. Manually labeling such a large scale dataset is infeasible. Instead, we propose to utilize different sources of high-precision maps to create our ground truth. Towards this goal, we develop algorithms that allow us to align all data sources with the maps while requiring minimal human supervision. We have designed a wide variety of tasks including building height estimation (reconstruction), road centerline and curb extraction, building instance segmentation, building contour extraction (reorganization), semantic labeling and scene type classification (recognition). Our pilot study shows that most of these tasks are still difficult for modern convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {1612.00423},
author = {Wang, Shenlong and Bai, Min and Mattyus, Gellert and Chu, Hang and Luo, Wenjie and Yang, Bin and Liang, Justin and Cheverie, Joel and Fidler, Sanja and Urtasun, Raquel},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
eprint = {1612.00423},
file = {:home/nathan/Documents/Mendeley Desktop/Wang et al/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Wang et al.{\_}2017{\_}TorontoCity Seeing the World with a Million Eyes.pdf:pdf},
title = {{TorontoCity: Seeing the World with a Million Eyes}},
url = {http://arxiv.org/abs/1612.00423},
year = {2017}
}
@article{Sommer2017,
author = {Sommer, Kent and Kim, Keonhee and Kim, Youngji and Jo, Sungho},
doi = {10.9789/2175-5361.2013v5n4p408},
file = {:home/nathan/Documents/Mendeley Desktop/Sommer et al/Proceedings of the IEEE International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)/Sommer et al.{\_}2017{\_}Towards Accurate Kidnap Resolution Through Deep Learning.pdf:pdf},
isbn = {9781509030569},
journal = {Proceedings of the IEEE International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)},
keywords = {cognitive robotics,computer vision,learning,machine,simultaneous localization and mapping},
number = {4},
pages = {408--416},
title = {{Towards Accurate Kidnap Resolution Through Deep Learning}},
volume = {5},
year = {2017}
}
@article{Morel2009,
author = {Morel, Jean-Michel and Yu, Guoshen},
journal = {SIAM Journal on Imaging Sciences},
number = {2},
pages = {438--469},
publisher = {SIAM},
title = {{ASIFT: A new framework for fully affine invariant image comparison}},
volume = {2},
year = {2009}
}
@article{Sattler2017b,
author = {Sattler, Torsten},
file = {:home/nathan/Documents/Mendeley Desktop/Sattler/arXiv preprint/Sattler{\_}2017{\_}Out with the Old Convolutional Neural Networks for Feature Matching and Visual Localization.pdf:pdf},
journal = {arXiv preprint},
pages = {1--3},
title = {{Out with the Old? Convolutional Neural Networks for Feature Matching and Visual Localization}},
year = {2017}
}
@article{Paton2017,
abstract = {Vision-based, autonomous, route-following algorithms enable robots to autonomously repeat manually driven routes over long distances. Through the use of inexpensive, commercial vision sensors, these algorithms have the potential to enable robotic applications across multiple industries. However, in order to extend these algorithms to long-term autonomy, they must be able to operate over long periods of time. This poses a difficult challenge for vision-based systems in unstructured and outdoor environments, where appearance is highly variable. While many techniques have been developed to perform localization across extreme appearance change, most are not suitable or untested for vision-in-the-loop systems such as autonomous route following, which requires continuous metric localization to keep the robot driving. In this paper, we present a vision-based, autonomous, route-following algorithm that combines multiple channels of information during localization to increase robustness against daily appearance change such as lighting. We explore this multichannel visual teach and repeat framework by adding the following channels of information to the basic single-camera, gray-scale, localization pipeline: images that are resistant to lighting change and images from additional stereo cameras to increase the algorithm's field of view. Using these methods, we demonstrate robustness against appearance change through extensive field deployments spanning over 26 km with an autonomy rate greater than 99.9{\%}. We furthermore discuss the limits of this system when subjected to harsh environmental conditions by investigating keypoint match degradation through time.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Paton, Michael and Pomerleau, Fran{\c{c}}ois and Mactavish, Kirk and Ostafew, Chris J. and Barfoot, Timothy D.},
doi = {10.1002/rob.21669},
eprint = {10.1.1.91.5767},
file = {:home/nathan/Documents/Mendeley Desktop/Paton et al/Journal of Field Robotics/Paton et al.{\_}2017{\_}Expanding the Limits of Vision-based Localization for Long-term Route-following Autonomy.pdf:pdf},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {1},
pages = {98--122},
pmid = {22164016},
title = {{Expanding the Limits of Vision-based Localization for Long-term Route-following Autonomy}},
volume = {34},
year = {2017}
}
@inproceedings{Ng2015,
abstract = {Deep convolutional neural networks have been success- fully applied to image classification tasks. When these same networks have been applied to image retrieval, the assump- tion has been made that the last layers would give the best performance, as they do in classification. We show that for instance-level image retrieval, lower layers often per- form better than the last layers in convolutional neural net- works. We present a novel approach for extracting convo- lutional features from different layers of the networks, and adopt VLAD encoding to encode features into a single vec- tor for each image. We investigate the effect of different layers and scales of input images on the performance of convolutional features using the recent deep networks Ox- fordNet and GoogLeNet. Experiments demonstrate that in- termediate layers or higher layers with finer scales pro- duce better results for image retrieval, compared to the last layer. When using compressed 128-D VLAD descriptors, our method obtains state-of-the-art results and outperforms other VLAD and CNN based approaches on two out of three test datasets. Our work provides guidance for transferring deep networks trained on image classification to image re- trieval tasks.},
archivePrefix = {arXiv},
arxivId = {1504.0513},
author = {Ng, Joe Yue Hei and Yang, Fan and Davis, Larry S.},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2015.7301272},
eprint = {1504.0513},
file = {:home/nathan/Documents/Mendeley Desktop/Ng, Yang, Davis/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)/Ng, Yang, Davis{\_}2015{\_}Exploiting local features from deep networks for image retrieval.pdf:pdf},
isbn = {9781467367592},
issn = {21607516},
keywords = {Artificial neural networks},
pages = {53--61},
title = {{Exploiting local features from deep networks for image retrieval}},
volume = {2015-Octob},
year = {2015}
}
@article{Razavian2014a,
abstract = {This paper provides an extensive study on the availability of image representations based on convolutional networks (ConvNets) for the task of visual instance retrieval. Besides the choice of convolutional layers, we present an efficient pipeline exploiting multi-scale schemes to extract local features, in particular, by taking geometric invariance into explicit account, i.e. positions, scales and spatial consistency. In our experiments using five standard image retrieval datasets, we demonstrate that generic ConvNet image representations can outperform other state-of-the-art methods if they are extracted appropriately.},
archivePrefix = {arXiv},
arxivId = {1412.6574},
author = {Razavian, Ali Sharif and Sullivan, Josephine and Carlsson, Stefan and Maki, Atsuto},
doi = {10.3169/mta.4.251},
eprint = {1412.6574},
file = {:home/nathan/Documents/Mendeley Desktop/Razavian et al/arXiv preprint/Razavian et al.{\_}2014{\_}Visual Instance Retrieval with Deep Convolutional Networks.pdf:pdf},
issn = {2186-7364},
journal = {arXiv preprint},
keywords = {convolutional network,learning representation,multi-resolution search,visual instance retrieval},
number = {3},
pages = {251--258},
title = {{Visual Instance Retrieval with Deep Convolutional Networks}},
url = {http://arxiv.org/abs/1412.6574},
volume = {4},
year = {2014}
}
@inproceedings{Zhi2016,
annote = {M{\'{e}}thode d'aggregation :

Pour chaque feature map (on a donc M maps de taille w*h), il r{\'{e}}duit la taille du descripteur en prenant les K1 premi{\`{e}}res plus grande valeur de la map. On passe donc d'un vecteur de taille M*w*h {\`{a}} M*K1

Ensuite ils font la m{\^{e}}me chose pour chaque patche (parce qu'ils font d'abord une extraction de N ROI), donc ils prennent les K2 plus grand valeur des M maps pari les N patches. On passe donc d'un vecteur de taille N*(M*K1) {\`{a}} K2*(M*K1). Donc ce qui est selectionn{\'{e}}, c'est les K2 plus grandes valeurs de chaque map M},
author = {Zhi, Tiancheng and Duan, Ling-Yu and Wang, Yitong and Huang, Tiejun},
booktitle = {Proceedings of the IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2016.7532802},
file = {:home/nathan/Documents/Mendeley Desktop/Zhi et al/Proceedings of the IEEE International Conference on Image Processing (ICIP)/Zhi et al.{\_}2016{\_}Two-Stage Pooling of Deep Convolutional Features for Image Retrieval.pdf:pdf},
title = {{Two-Stage Pooling of Deep Convolutional Features for Image Retrieval}},
year = {2016}
}
@inproceedings{Philbin2008,
author = {Philbin, James and Chum, Ondřej and Isard, Michael and Sivic, Josef and Zisserman, Andrew},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{Lost in Quantization: Improving Particular Object Retrieval in Large Scale Image Databases}},
year = {2008}
}
@inproceedings{Gong2014,
abstract = {Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. However, global CNN activations lack geometric invariance, which limits their robustness for classification and matching of highly variable scenes. To improve the invariance of CNN activations without degrading their discriminative power, this paper presents a simple but effective scheme called multi-scale orderless pooling (MOP-CNN). This scheme extracts CNN activations for local patches at multiple scale levels, performs orderless VLAD pooling of these activations at each level separately, and concatenates the result. The resulting MOP-CNN representation can be used as a generic feature for either supervised or unsupervised recognition tasks, from image classification to instance-level retrieval; it consistently outperforms global CNN activations without requiring any joint training of prediction layers for a particular target dataset. In absolute terms, it achieves state-of-the-art results on the challenging SUN397 and MIT Indoor Scenes classification datasets, and competitive results on ILSVRC2012/2013 classification and INRIA Holidays retrieval datasets.},
archivePrefix = {arXiv},
arxivId = {1403.1840},
author = {Gong, Yunchao and Wang, Liwei and Guo, Ruiqi and Lazebnik, Svetlana},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
eprint = {1403.1840},
file = {:home/nathan/Documents/Mendeley Desktop/Gong et al/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Gong et al.{\_}2014{\_}Multi-scale Orderless Pooling of Deep Convolutional Activation Features.pdf:pdf},
pages = {1--17},
title = {{Multi-scale Orderless Pooling of Deep Convolutional Activation Features}},
url = {http://arxiv.org/abs/1403.1840},
year = {2014}
}
@article{Sattler2011,
abstract = {Recently developed Structure from Motion (SfM) reconstruction approaches enable the creation of large scale 3D models of urban scenes. These compact scene representations can then be used for accurate image-based localization, creating the need for localization approaches that are able to efficiently handle such large amounts of data. An important bottleneck is the computation of 2D-to-3D correspondences required for pose estimation. Current stateof- the-art approaches use indirect matching techniques to accelerate this search. In this paper we demonstrate that direct 2D-to-3D matching methods have a considerable potential for improving registration performance. We derive a direct matching framework based on visual vocabulary quantization and a prioritized correspondence search. Through extensive experiments, we show that our framework efficiently handles large datasets and outperforms current state-of-the-art methods.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de localisation d'images 2D {\`{a}} partir d'un mod{\`{e}}le 3D d'une ville (obtenu par SfM). Utilisation de descripteurs locaux (SIFT) extraits sur les images en requ{\^{e}}tes mais {\'{e}}galement sur les points 3D (obtenu par SfM donc associ{\'{e}}s {\`{a}} un descripteur 2D SIFT). Fast matching en utilisant une variante des BoW et en priorisant une recherche lin{\'{e}}aire de correspondances dans les visual words qui ont peu de features associ{\'{e}}es.

Ce que je n'ai pas compris
- Comment ils am{\'{e}}liorent le rejection time
- L'influence de la taille du dictionnaire (ils disent que plus il est petit plus c'est long de
rechercher..?)

Ce qui est int{\'{e}}ressant
- R{\'{e}}gression direct de la pose de la cam{\'{e}}ra
- Localisation pr{\'{e}}cise
- Le temps de calcul

Critiques
- Faible taux de r{\'{e}}ussite ({\textless}50{\%})
- Dataset peut {\^{e}}tre pas tr{\`{e}}s challenging

A approfondir
- Les m{\'{e}}thodes de r{\'{e}}gression de pose {\`{a}} partir de correspondance 2D/3D : p6p, p4pfr, p3p

Computational load
Offline: BoW dictonnary creation
Online: {\textless} 1s

Scalability
Scale: Place, small City
Scalability potential: -},
author = {Sattler, Torsten and Leibe, Bastian and Kobbelt, Leif},
doi = {10.1109/ICCV.2011.6126302},
file = {:home/nathan/Documents/Mendeley Desktop/Sattler, Leibe, Kobbelt/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Sattler, Leibe, Kobbelt{\_}2011{\_}Fast image-based localization using direct 2D-to-3D matching.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
keywords = {3D models,Alignement,Local features,Outdoor,RGB request,SIFT,Visual place recognition,Visual words},
mendeley-tags = {3D models,Alignement,Local features,Outdoor,RGB request,SIFT,Visual place recognition,Visual words},
pages = {667--674},
title = {{Fast image-based localization using direct 2D-to-3D matching}},
year = {2011}
}
@article{Saurer2016,
abstract = {Given a picture taken somewhere in the world, automatic geo-localization of such an image is an extremely useful task especially for historical and forensic sciences, documentation purposes, organization of the world's photographs and intelligence applications. While tremendous progress has been made over the last years in visual location recognition within a single city, localization in natural environments is much more difficult, since vegetation, illumination, seasonal changes make appearance-only approaches impractical. In this work, we target mountainous terrain and use digital elevation models to extract representations for fast visual database lookup. We propose an automated approach for very large scale visual localization that can efficiently exploit visual information (contours) and geometric constraints (consistent orientation) at the same time. We validate the system at the scale of Switzerland (40,000 ) using over 1000 landscape query images with ground truth GPS position.},
author = {Saurer, Olivier and Baatz, Georges and K{\"{o}}ser, Kevin and Ladicky, L'ubor and Pollefeys, Marc},
doi = {10.1007/s11263-015-0830-0},
file = {:home/nathan/Documents/Mendeley Desktop/Saurer et al/International Journal of Computer Vision (IJCV)/Saurer et al.{\_}2016{\_}Image Based Geo-localization in the Alps.pdf:pdf},
isbn = {1126301508300},
issn = {15731405},
journal = {International Journal of Computer Vision (IJCV)},
keywords = {Camera calibration,Computer vision,Geo-localization,Localization},
number = {3},
pages = {213--225},
title = {{Image Based Geo-localization in the Alps}},
volume = {116},
year = {2016}
}
@inproceedings{Rezende2017,
author = {Rezende, Rafael and Zepeda, Joaquin and Ponce, Jean and Bach, Francis and Perez, Patrick},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Rezende et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Rezende et al.{\_}2017{\_}Kernel Square-Loss Exemplar Machines for Image Retrieval.pdf:pdf},
title = {{Kernel Square-Loss Exemplar Machines for Image Retrieval}},
year = {2017}
}
@inproceedings{Schonberger2016,
abstract = {Spatial verification is a crucial part of every image retrieval system, as it accounts for the fact that geometric feature configurations are typically ignored by the Bag-of-Words representation. Since spatial verification quickly becomes the bottleneck of the retrieval process, run- time efficiency is extremely important. At the same time, spatial veri- fication should be able to reliably distinguish between related and un- related images. While methods based on RANSAC's hypothesize-and- verify framework achieve high accuracy, they are not particularly ef- ficient. Conversely, verification approaches based on Hough voting are extremely efficient but not as accurate. In this paper, we develop a novel spatial verification approach that uses an efficient voting scheme to identify promising transformation hypotheses that are subsequently ver- ified and refined. Through comprehensive experiments, we show that our method is able to achieve a verification accuracy similar to state-of-the- art hypothesize-and-verify approaches while providing faster runtimes than state-of-the-art voting-based methods.},
author = {Sch{\"{o}}nberger, Johannes L. and Price, True and Sattler, Torsten and Frahm, Jan-Michael and Pollefeys, Marc},
booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
file = {:home/nathan/Documents/Mendeley Desktop/Sch{\"{o}}nberger et al/Proceedings of the Asian Conference on Computer Vision (ACCV)/Sch{\"{o}}nberger et al.{\_}2016{\_}A Vote-and-Verify Strategy for Fast Spatial Verification in Image Retrieval.pdf:pdf},
pages = {1--17},
title = {{A Vote-and-Verify Strategy for Fast Spatial Verification in Image Retrieval}},
year = {2016}
}
@article{Chen2016,
author = {Chen, Kuan-wen and Wang, Chun-hsin and Wei, Xiao and Liang, Qiao and Chen, Chu-song and Yang, Ming-hsuan and Hung, Yi-ping},
file = {:home/nathan/Documents/Mendeley Desktop/Chen et al/IEEE Transaction on Intelligent Transportation Systems/Chen et al.{\_}2016{\_}Vision-Based Positioning for Internet-of-Vehicles.pdf:pdf},
journal = {IEEE Transaction on Intelligent Transportation Systems},
pages = {1--13},
title = {{Vision-Based Positioning for Internet-of-Vehicles}},
year = {2016}
}
@article{Griffith2017,
annote = {Probl{\'{e}}matique de matching d'image en milieu naturel. Les auteurs introduisent une m{\'{e}}thode de SLAM visuaelle pour avori de meilleur coarse matching qu'ils alignent (register) correctement avec du SIFT dense FLOW matching pour faire de la reconnaissance d'anomalies.},
author = {Griffith, Shane and Pradalier, C{\'{e}}dric},
file = {:home/nathan/Documents/Mendeley Desktop/Griffith, Pradalier/Journal of Field Robotics/Griffith, Pradalier{\_}2017{\_}Survey Registration For Long–Term Natural Environment Monitoring.pdf:pdf},
journal = {Journal of Field Robotics},
title = {{Survey Registration For Long–Term Natural Environment Monitoring}},
year = {2017}
}
@article{Wan2014,
abstract = {Localization of the rover is critical to support science and engineering operations in planetary rover missions, such as rover traverse planning and hazard avoidance. It is desirable for planetary rover to have visual localization capability with high degree of automation and quick turnaround time. In this research, we developed a visual localization method for lunar rover, which is capable of deriving accurate localization results from cross-site stereo images. Tie points are searched in correspondent areas predicted by initial localization results and determined by ASIFT matching algorithm. Accurate localization results are derived from bundle adjustment based on an image network constructed by the tie points. In order to investigate the performance of proposed method, theoretical accuracy analysis on is implemented by means of error propagation principles. Field experiments were conducted to verify the effectiveness of the proposed method in practical applications. Experiment results prove that the proposed method provides more accurate localization results (1 {\%}{\~{}}4 {\%}) than dead-reckoning. After more validations and enhancements, the developed rover localization method has been successfully used in Chang'e-3 mission operations.},
annote = {R{\'{e}}sum{\'{e}}
Description du syst{\`{e}}me de relocalisation bas{\'{e}} vision du rover lunaire chinois. Utilisation d'une pose bruit{\'{e}}e calcul{\'{e}} par odom{\'{e}}trie (dead-reckoning) pour r{\'{e}}cup{\'{e}}rer une zone de l'image probablement en commun entre deux prises de vues, puis calcul des appariements par ASIFT et renforcement de la pose par BA (les cam{\'{e}}ras sont st{\'{e}}r{\'{e}}os). Les auteurs pr{\'{e}}sentent {\'{e}}galement un calcul th{\'{e}}orique de l'erreur associ{\'{e}}e {\`{a}} ce syst{\`{e}}me de localisation.

Ce que je n'ai pas compris
- L'analyse de l'erreur

Ce qui est int{\'{e}}ressant
- Le cas de figure quand les images sont pris a des points de vue oppos{\'{e}}s
- Texture enhancement par Wallis Filter

Critiques
- Necessite une position initiale et de la s{\'{e}}t{\'{e}}ro

A approfondir
- ASIFT

Computational load
Offline: -
Online: {\textless} 2 min

Scalability
Scale: - 
Scalability potential: -},
author = {Wan, Wenhui and Liu, Zhaoqin and Di, Kaichang and Wang, B. and Zhou, J.},
doi = {10.5194/isprsarchives-XL-4-279-2014},
file = {:home/nathan/Documents/Mendeley Desktop/Wan et al/The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences/Wan et al.{\_}2014{\_}A Cross-Site Visual Localization Method for Yutu Rover.pdf:pdf},
issn = {2194-9034},
journal = {The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {asift,bundle adjustment,chang,e-3,visual localization,yutu rover},
number = {May},
pages = {279--284},
title = {{A Cross-Site Visual Localization Method for Yutu Rover}},
url = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XL-4/279/2014/},
volume = {XL-4},
year = {2014}
}
@inproceedings{Chen2017,
author = {Chen, Zetao and Jacobson, Adam and Upcroft, Ben and Liu, Lingqiao and Shen, Chunhua and Reid, Ian and Milford, Michael J.},
booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
file = {:home/nathan/Documents/Mendeley Desktop/Chen et al/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Chen et al.{\_}2017{\_}Deep Learning Features at Scale for Visual Place Recognition.pdf:pdf},
title = {{Deep Learning Features at Scale for Visual Place Recognition}},
volume = {1},
year = {2017}
}
@article{Melekhova,
abstract = {We present a method for estimating relative camera pose between a pair of images. The goal is to propose accurate estimations the relative orientation vector representing by rotation matrix and translation vector of two cameras capturing the same scene. Our approach is based on convolutional neural networks and directly estimates camera motion between two RGB images by solving regression problem. The proposed network is trained in an end-to-end manner utilizing transfer learning from large scale classification data. The method is compared to a classical local feature based pipeline (SURF, ORB) of relative pose estimation and we demonstrate the cases where our deep model outperforms the traditional approach significantly. Finally, we evaluated experiments with applying Spatial Pyramid Pooling (SPP) layer which can produce a fixed-size representation regardless the size of the input image. The results confirm that SPP further improves the performance of the proposed approach.},
archivePrefix = {arXiv},
arxivId = {1702.01381},
author = {Melekhov, Iaroslav and Kannala, Juho and Rahtu, Esa},
eprint = {1702.01381},
file = {:home/nathan/Documents/Mendeley Desktop/Melekhov, Kannala, Rahtu/arXiv preprint/Melekhov, Kannala, Rahtu{\_}2017{\_}Relative Camera Pose Estimation Using Convolutional Neural Networks.pdf:pdf},
journal = {arXiv preprint},
keywords = {deep neural networks,relative camera pose estimation,spa-,tial pyramid pooling},
pages = {1--12},
title = {{Relative Camera Pose Estimation Using Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1702.01381},
year = {2017}
}
@article{Kumar2016,
author = {Kumar, Devinder},
file = {:home/nathan/Documents/Mendeley Desktop/Kumar/arXiv preprint/Kumar{\_}2016{\_}Deep Learning Based Place Recognition for Challenging Environments.pdf:pdf},
journal = {arXiv preprint},
title = {{Deep Learning Based Place Recognition for Challenging Environments}},
year = {2016}
}
@inproceedings{Jegou2014,
author = {J{\'{e}}gou, Herv{\'{e}} and Zisserman, Andrew},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/J{\'{e}}gou, Zisserman/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/J{\'{e}}gou, Zisserman{\_}2014{\_}Triangulation embedding and democratic aggregation for image search.pdf:pdf},
title = {{Triangulation embedding and democratic aggregation for image search}},
year = {2014}
}
@inproceedings{Pepperell2014,
author = {Pepperell, Edward and Corke, Peter and Milford, Michael J.},
booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
file = {:home/nathan/Documents/Mendeley Desktop/Pepperell, Corke, Milford/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Pepperell, Corke, Milford{\_}2014{\_}All - Environment Visual Place Recognition with SMART.pdf:pdf},
isbn = {9781479936854},
title = {{All - Environment Visual Place Recognition with SMART}},
year = {2014}
}
@inproceedings{Perronnin2010,
author = {Perronnin, Florent and Liu, Yan},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Perronnin, Liu/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Perronnin, Liu{\_}2010{\_}Large-Scale Image Retrieval with Compressed Fisher Vectors.pdf:pdf},
isbn = {9781424469833},
title = {{Large-Scale Image Retrieval with Compressed Fisher Vectors}},
year = {2010}
}
@inproceedings{Sunderhauf2015a,
abstract = {Place recognition has long been an incompletely solved problem in that all approaches involve significant com- promises. Current methods address many but never all of the critical challenges of place recognition – viewpoint-invariance, condition-invariance and minimizing training requirements. Here we present an approach that adapts state-of-the-art object proposal techniques to identify potential landmarks within an image for place recognition. We use the astonishing power of convolutional neural network features to identify matching landmark proposals between images to perform place recognition over extreme appearance and viewpoint variations. Our system does not require any form of training, all components are generic enough to be used off-the-shelf.We present a range of challenging experiments in varied viewpoint and environmental conditions. We demonstrate superior performance to current state-of-the- art techniques. Furthermore, by building on existing and widely used recognition frameworks, this approach provides a highly compatible place recognition system with the potential for easy integration of other techniques such as object detection and semantic scene interpretation.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de place recognition robuste aux changements de point de vue et au changement locaux de l'environnement. Elle s'appuie sur les CNN pour d{\'{e}}crire des features extraites dans l'image par une m{\'{e}}thode robuste de bounding boxing sur des zones d'interets (initiallement pour l'extraction automatique d'objet non-supervis{\'{e}}e). Une fois le set de descripteurs extraits, les auteurs proc{\`{e}}dent {\`{a}} un matching en calculant une cosinus distance par rapport {\`{a}} la base de donn{\'{e}}es.

Ce que je n'ai pas compris
- Pourquoi privil{\'{e}}gi{\'{e}} la cosinus distance par rapport {\`{a}} la distance euclidienne ?

Ce qui est int{\'{e}}ressant
- Les challenging match
- La non-necessite d'entrainer le CNN
- La conclusion {\&} les futures travaux

Critiques
- Temps de calcul trop long
- Ils ne pr{\'{e}}cisent pas la m{\'{e}}trique utilis{\'{e}} pour les courbe recall/precision
- Datasets tr{\`{e}}s petits

A approfondir
- Utilisation de m{\'{e}}thode plus efficaces pour la regression de dimension et le matching (BoW-kdtree)
- Edge boxes
- Utilisation du CNN pour dire si les features extraites sont pertinentes (par le biai du classement -{\textgreater} v{\'{e}}g{\'{e}}ation, objet mobile, etc.)

Computational load
Offline: Temps d'extraction des bouding boxes (2s/images)
Online: {\textless} 1 minute (estimated), edge boxe detection + matching across the dataset

Scalability
Scale: City
Scalability potential: Real},
author = {S{\"{u}}nderhauf, Niko and Shirazi, Sareh and Jacobson, Adam and Dayoub, Feras and Pepperell, Edward and Upcroft, Ben and Milford, Michael J.},
booktitle = {Robotics Science and Systems (RSS)},
doi = {10.15607/RSS.2015.XI.022},
file = {:home/nathan/Documents/Mendeley Desktop/S{\"{u}}nderhauf et al/Robotics Science and Systems (RSS)/S{\"{u}}nderhauf et al.{\_}2015{\_}Place Recognition with ConvNet Landmarks Viewpoint-Robust, Condition-Robust, Training-Free.pdf:pdf},
isbn = {9780992374716},
keywords = {CNN,Challenging matching,Global features,Indoor,Long terme place recognition,Machine learning,Outdoor,RGB request,Unsupervised learning,Visual place recognition},
mendeley-tags = {CNN,Challenging matching,Global features,Indoor,Long terme place recognition,Machine learning,Outdoor,RGB request,Unsupervised learning,Visual place recognition},
title = {{Place Recognition with ConvNet Landmarks: Viewpoint-Robust, Condition-Robust, Training-Free}},
year = {2015}
}
@inproceedings{Vaca-Castano2012,
abstract = {This paper presents a novel method for estimating the geospatial trajectory of a moving camera with unknown in-trinsic parameters, in a city-scale urban environment. The proposed method is based on a three step process that in-cludes: 1) finding the best visual matches of individual im-ages to a dataset of geo-referenced street view images, 2) Bayesian tracking to estimate the frame localization and its temporal evolution, and 3) a trajectory reconstruction algorithm to eliminate inconsistent estimations. As a re-sult of matching features in query image with the features in the reference geo-taged images, in the first step, we ob-tain a distribution of geolocated votes of matching features which is interpreted as the likelihood of the location (lati-tude and longitude) given the current observation. In the second step, Bayesian tracking framework is used to esti-mate the temporal evolution of frame geolocalization based on the previous state probabilities and current likelihood. Finally, once a trajectory is estimated, we perform a Min-imum Spanning Trees (MST) based trajectory reconstruc-tion algorithm to eliminate trajectory loops or noisy esti-mations. The proposed method was tested on sixty minutes of video, which included footage downloaded from YouTube and footage captured by random users in Orlando and Pitts-burgh.},
author = {Vaca-Castano, Gonzalo and Zamir, Amir Roshan and Shah, Mubarak},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Vaca-Castano, Zamir, Shah/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Vaca-Castano, Zamir, Shah{\_}2012{\_}City Scale Geo-Spatial Trajectory Estimation of a Moving Camera.pdf:pdf},
title = {{City Scale Geo-Spatial Trajectory Estimation of a Moving Camera}},
year = {2012}
}
@article{Hou2017,
author = {Hou, Yi and Zhang, Hong and Zhou, Shilin and Zou, Huanxin},
file = {:home/nathan/Documents/Mendeley Desktop/Hou et al/Unknown/Hou et al.{\_}2017{\_}Efficient ConvNet Feature Extraction with Multiple RoI Pooling for Landmark-based Visual Localization of Autonomous Vehi.pdf:pdf},
keywords = {autonomous vehicle,convnet feature,convolutional,roi pooling,visual localization},
pages = {1--16},
title = {{Efficient ConvNet Feature Extraction with Multiple RoI Pooling for Landmark-based Visual Localization of Autonomous Vehicles}},
year = {2017}
}
@article{Ramalingam2011,
author = {Ramalingam, Srikumar and Bouaziz, Sofien and Sturm, Peter},
file = {:home/nathan/Documents/Mendeley Desktop/Ramalingam, Bouaziz, Sturm/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Ramalingam, Bouaziz, Sturm{\_}2011{\_}Pose Estimation Using Both Points and Lines for Geolocation.pdf:pdf},
journal = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
title = {{Pose Estimation Using Both Points and Lines for Geolocation}},
year = {2011}
}
@inproceedings{Kuang2015,
author = {Kuang, Hao and Zhu, Shiai and Saddik, Abdulmotaleb El},
booktitle = {Proceedings of the ACM International Conference on Multimedia Retrieval (ICMR)},
doi = {10.1145/2671188.2749351},
file = {:home/nathan/Documents/Mendeley Desktop/Kuang, Zhu, Saddik/Proceedings of the ACM International Conference on Multimedia Retrieval (ICMR)/Kuang, Zhu, Saddik{\_}2015{\_}Boosting Prediction of Geo-location for Web Images Through Integrating Multiple Knowledge Sources.pdf:pdf},
isbn = {9781450332743},
keywords = {geo-aware tags,image geo-location,language model},
pages = {559--562},
title = {{Boosting Prediction of Geo-location for Web Images Through Integrating Multiple Knowledge Sources}},
volume = {1},
year = {2015}
}
@article{Zhao2016,
abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU score as 85.4{\%} on PASCAL VOC 2012 and 80.2{\%} on Cityscapes.},
archivePrefix = {arXiv},
arxivId = {1612.01105},
author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
eprint = {1612.01105},
file = {:home/nathan/Documents/Mendeley Desktop/Zhao et al/arXiv preprint/Zhao et al.{\_}2016{\_}Pyramid Scene Parsing Network.pdf:pdf},
journal = {arXiv preprint},
title = {{Pyramid Scene Parsing Network}},
url = {http://arxiv.org/abs/1612.01105},
year = {2016}
}
@inproceedings{Paulin2015,
abstract = {Patch-level descriptors underlie several important computer vision tasks, such as stereo-matching or contentbased image retrieval. We introduce a deep convolutional architecture that yields patch-level descriptors, as an alternative to the popular SIFT descriptor for image retrieval. The proposed family of descriptors, called PatchCKN, adapt the recently introduced Convolutional Kernel Network (CKN), an unsupervised framework to learn convolutional architectures. We present a comparison framework to benchmark current deep convolutional approaches along with Patch-CKN for both patch and image retrieval, including our novel “RomePatches” dataset. Patch-CKN descriptors yield competitive results compared to supervised CNN alternatives on patch and image retrieval.},
author = {Paulin, Mattis and Douze, Matthijs and Harchaoui, Zaid and Mairal, Julien and Perronnin, Florent and Schmid, Cordelia},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.19},
file = {:home/nathan/Documents/Mendeley Desktop/Paulin et al/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Paulin et al.{\_}2015{\_}Local convolutional features with unsupervised training for image retrieval.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
pages = {91--99},
title = {{Local convolutional features with unsupervised training for image retrieval}},
volume = {11-18-Dece},
year = {2015}
}
@article{Gronat2013,
abstract = {The aim of this work is to localize a query photograph by finding other images depicting the same place in a large geotagged image database. This is a challenging task due to changes in viewpoint, imaging conditions and the large size of the image database. The contribution of this work is two-fold. First, we cast the place recognition problem as a classification task and use the available geotags to train a classifier for each location in the database in a similar manner to per-exemplar SVMs in object recognition. Second, as only few positive training examples are available for each location, we propose a new approach to calibrate all the per-location SVM classifiers using only the negative examples. The calibration we propose relies on a significance measure essentially equivalent to the p-values classically used in statistical hypothesis testing. Experiments are performed on a database of 25,000 geotagged street view images of Pittsburgh and demonstrate improved place recognition accuracy of the proposed approach over the previous work. View full abstract},
author = {Gronat, Petr and Sivic, Josef and Obozinski, Guillaume and Pajdla, Tomas},
doi = {10.1007/s11263-015-0878-x},
file = {:home/nathan/Documents/Mendeley Desktop/Gronat et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Gronat et al.{\_}2013{\_}Learning and Calibrating Per-Location Classifiers for Visual Place Recognition.pdf:pdf},
isbn = {1063-6919 VO -},
issn = {15731405},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
keywords = {Classifier calibration,Exemplar SVM,Geo-localization,Place recognition},
pages = {1--18},
title = {{Learning and Calibrating Per-Location Classifiers for Visual Place Recognition}},
year = {2013}
}
@inproceedings{Ramalingam2010,
abstract = {This paper investigates the problem of geo-localization in GPS challenged urban canyons using only skylines. Our proposed solution takes a sequence of upward facing omnidirectional images and coarse 3D models of cities to compute the geo-trajectory. The camera is oriented upwards to capture images of the immediate skyline, which is generally unique and serves as a fingerprint for a specific location in a city. Our goal is to estimate global position by matching skylines extracted from omni-directional images to skyline segments from coarse 3D city models. Under day-time and clear sky conditions, we propose a sky-segmentation algorithm using graph cuts for estimating the geo-location. In cases where the skyline gets affected by partial fog, night-time and occlusions from trees, we propose a shortest path algorithm that computes the location without prior sky detection. We show compelling experimental results for hundreds of images taken in New York, Boston and Tokyo under various weather and lighting conditions (daytime, foggy dawn and night-time).},
author = {Ramalingam, Srikumar and Bouaziz, Sofien and Sturm, Peter and Brand, Matthew},
booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2010.5649105},
file = {:home/nathan/Documents/Mendeley Desktop/Ramalingam et al/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Ramalingam et al.{\_}2010{\_}SKYLINE2GPS Localization in urban canyons using omni-skylines.pdf:pdf},
isbn = {9781424466757},
issn = {2153-0858},
pages = {3816--3823},
title = {{SKYLINE2GPS: Localization in urban canyons using omni-skylines}},
year = {2010}
}
@article{Pascoe2015a,
abstract = {— This paper is concerned with large-scale locali-sation at city scales with monocular cameras. Our primary motivation lies with the development of autonomous road vehicles — an application domain in which low-cost sensing is particularly important. Here we present a method for localising against a textured 3-dimensional prior mesh using a monocular camera. We first present a system for generating and texturing the prior using a LIDAR scanner and camera. We then describe how we can localise against that prior with a single camera, using an information-theoretic measure of image similarity. This process requires dealing with the distortions induced by a wide-angle camera. We present and justify an interesting approach to this issue in which we distort the prior map into the image rather than vice-versa. Finally we explain how the general purpose computation functionality of a modern GPU is particularly apt for our task, allowing us to run the system in real time. We present results showing centimetre-level localisation accuracy through a city over six kilometres.},
author = {Pascoe, Geoffrey and Maddern, Will and Stewart, Alexander D. and Newman, Paul},
doi = {10.1109/ICRA.2015.7140093},
file = {:home/nathan/Documents/Mendeley Desktop/Pascoe et al/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Pascoe et al.{\_}2015{\_}FARLAP Fast Robust Localisation using Appearance Priors.pdf:pdf},
isbn = {9781479969227},
issn = {1050-4729},
journal = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
pages = {6366--6373},
title = {{FARLAP : Fast Robust Localisation using Appearance Priors}},
year = {2015}
}
@inproceedings{Armagan2017b,
author = {Armagan, Anil and Hirzer, Martin and Roth, Peter M. and Lepetit, Vincent},
booktitle = {British Machine Vision Conference (BMVC)},
file = {:home/nathan/Documents/Mendeley Desktop/Armagan et al/British Machine Vision Conference (BMVC)/Armagan et al.{\_}2017{\_}Accurate Camera Registration in Urban Environments Using High-Level Feature Matching.pdf:pdf},
pages = {1--12},
title = {{Accurate Camera Registration in Urban Environments Using High-Level Feature Matching}},
year = {2017}
}
@article{Fernando2014,
archivePrefix = {arXiv},
arxivId = {1409.7556},
author = {Fernando, Basura and Tommasi, Tatiana and Tuytelaars, Tinne},
eprint = {1409.7556},
file = {:home/nathan/Documents/Mendeley Desktop/Fernando, Tommasi, Tuytelaars/ACM Computing Research Repository (CoRR)/Fernando, Tommasi, Tuytelaars{\_}2014{\_}Lost in the Past Recognizing Locations Over Large Time Lags.pdf:pdf},
journal = {ACM Computing Research Repository (CoRR)},
pages = {1--16},
title = {{Lost in the Past: Recognizing Locations Over Large Time Lags}},
url = {http://arxiv.org/abs/1409.7556v1},
year = {2014}
}
@article{Ni2009,
abstract = {This paper presents a novel method for location recognition, which exploits an epitomic representation to achieve both high efficiency and good generalization. A generative model based on epitomic image analysis captures the appearance and geometric structure of an environment while allowing for variations due to motion, occlusions, and non-Lambertian effects. The ability to model translation and scale invariance together with the fusion of diverse visual features yields enhanced generalization with economical training. Experiments on both existing and new labeled image databases result in recognition accuracy superior to state of the art with real-time computational performance.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de localisation d'images utilisant la une repr{\'{e}}sentation {\'{e}}pitomique d'un groupe d'image pour la classification. La localization donn{\'{e}}e est d'ordre s{\'{e}}mantique (une classe par lieu, eg la cuisine, mon bureau, le bureau du voisin, etc.). Se place dans la lign{\'{e}} des travaux de Torralba et al. 2003 qui mod{\'{e}}lisaient les localisation par GMM. Un mod{\`{e}}le epitomique est entrain{\'{e}} par location (ou un modele pour toutes les locations si peu d'images) en utilisant des descripteurs patch associ{\'{e}}s aux images : GIST, histogramme local d'intensit{\'{e}} RGB et de carte de disparit{\'{e}} si images RGBD. Pour classifier une image, ils la d{\'{e}}crivent avex les m{\^{e}}mes features que ceux utilis{\'{e}}s pour la cr{\'{e}}ation de l'{\'{e}}pitome puis ils convoluent la repr{\'{e}}sentation obtenu sur tout le mod{\`{e}}le {\'{e}}pitomique en cherchant le maximum de corr{\'{e}}lation.

Ce que je n'ai pas compris
- Epitome : vraiment ce que c'est, comment on les construit, et comment on choisit la localisation quand on a plusieurs images {\'{e}}pitomiques

Ce qui est int{\'{e}}ressant
- Utilisation de Depth feature

Critiques
- Localisation tr{\`{e}}s grossi{\`{e}}re
- Tests seulement en indoor

Computational load
Offline: 120 sec / 693 images for one epitome
Online: 5.7 sec / 600 images -{\textgreater} 116 fps

Scalability
Scale: Inside a building
Scalability potential: -},
author = {Ni, Kai and Kannan, Anitha and Criminisi, Antonio and Winn, John},
doi = {10.1109/TPAMI.2009.165},
file = {:home/nathan/Documents/Mendeley Desktop/Ni et al/IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)/Ni et al.{\_}2009{\_}Epitomic location recognition.pdf:pdf},
isbn = {9781424422432},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
keywords = {Epitomic image analysis,Location class recognition,Panoramic stitching},
number = {12},
pages = {2158--2167},
pmid = {19834138},
title = {{Epitomic location recognition}},
volume = {31},
year = {2009}
}
@phdthesis{Stumm2015,
author = {Stumm, Elena},
doi = {10.1177/0278364915570140},
file = {:home/nathan/Documents/Mendeley Desktop/Stumm/Unknown/Stumm{\_}2015{\_}Building Location Models for Visual Place Recognition.pdf:pdf},
keywords = {bayesian inference,covisibility maps,location models,visual place recognition},
title = {{Building Location Models for Visual Place Recognition}},
year = {2015}
}
@inproceedings{Kneip2011,
author = {Kneip, Laurent and Scaramuzza, Davide and Siegwart, Roland},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
organization = {IEEE},
pages = {2969--2976},
title = {{A novel parametrization of the perspective-three-point problem for a direct computation of absolute camera position and orientation}},
year = {2011}
}
@article{Zhang2017a,
author = {Zhang, Dongming and Tang, Jingya and Jin, Guoqing and Zhang, Yongdong and Tian, Qi},
doi = {10.1016/j.neucom.2017.07.025},
file = {:home/nathan/Documents/Mendeley Desktop/Zhang et al/Neurocomputing/Zhang et al.{\_}2017{\_}Region Similarity Arrangement for Large-Scale Image Retrieval.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Content Based Image Retrieval,Geometric Verification,Region Property Space,Spatial Weighting},
publisher = {Elsevier B.V.},
title = {{Region Similarity Arrangement for Large-Scale Image Retrieval}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231217312717},
year = {2017}
}
@article{Laina2016,
abstract = {This paper addresses the problem of estimating the depth map of a scene given a single RGB image. We propose a fully convolutional architecture, encompassing residual learning, to model the ambiguous mapping between monocular images and depth maps. In order to improve the output resolution, we present a novel way to efficiently learn feature map up-sampling within the network. For optimization, we introduce the reverse Huber loss that is particularly suited for the task at hand and driven by the value distributions commonly present in depth maps. Our model is composed of a single architecture that is trained end-to-end and does not rely on post-processing techniques, such as CRFs or other additional refinement steps. As a result, it runs in real-time on images or videos. In the evaluation, we show that the proposed model contains fewer parameters and requires fewer training data than the current state of the art, while outperforming all approaches on depth estimation. Code and models are publicly available.},
archivePrefix = {arXiv},
arxivId = {1606.00373},
author = {Laina, Iro and Rupprecht, Christian and Belagiannis, Vasileios and Tombari, Federico and Navab, Nassir},
doi = {10.1109/3DV.2016.32},
eprint = {1606.00373},
file = {:home/nathan/Documents/Mendeley Desktop/Laina et al/Proceedings of the International Conference on 3D Vision/Laina et al.{\_}2016{\_}Deeper depth prediction with fully convolutional residual networks.pdf:pdf},
isbn = {9781509054077},
journal = {Proceedings of the International Conference on 3D Vision},
keywords = {CNN,Depth prediction},
pages = {239--248},
title = {{Deeper depth prediction with fully convolutional residual networks}},
year = {2016}
}
@inproceedings{Larlus2017,
author = {Larlus, Diane and Gordo, Albert},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.560},
file = {:home/nathan/Documents/Mendeley Desktop/Larlus, Gordo/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Larlus, Gordo{\_}2017{\_}Beyond instance-level image retrieval Leveraging human captions to learn representations for semantic visual search.pdf:pdf},
pages = {6589--6598},
title = {{Beyond instance-level image retrieval: Leveraging human captions to learn representations for semantic visual search}},
year = {2017}
}
@article{Nister2004,
author = {Nist{\'{e}}r, David},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
number = {6},
pages = {756--770},
publisher = {IEEE},
title = {{An efficient solution to the five-point relative pose problem}},
volume = {26},
year = {2004}
}
@inproceedings{Edward2011,
abstract = {Vision-based topological maps for mobile robot localization traditionally consist of a set of images captured along a path, with a query image then compared to every individual map image. This paper introduces a new approach to topological mapping, whereby the map consists of a set of landmarks that are detected across multiple images, spanning the continuous space between nodal images. Matches are then made to landmarks, rather than to individual images, enabling a topological map of far greater density than traditionally possible, without sacrificing computational speed. Furthermore, by treating each landmark independently, a probabilistic approach to localization can be employed by taking into account the learned discriminative properties of each landmark. An optimization stage is then used to adjust the map according to speed and localization accuracy requirements. Results for global localization show a greater positive location identification rate compared to the traditional topological map, together with enabling a greater localization resolution in the denser topological map, without requiring a decrease in frame rate.},
author = {Edward, Johns and Guang-Zhong, Yang},
booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
file = {:home/nathan/Documents/Mendeley Desktop/Edward, Guang-Zhong/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Edward, Guang-Zhong{\_}2011{\_}Global Localization in a Dense Continuous Topological Map.pdf:pdf},
title = {{Global Localization in a Dense Continuous Topological Map}},
year = {2011}
}
@inproceedings{Gehrig2017,
abstract = {We propose a novel scoring concept for visual place recognition based on nearest neighbor descriptor voting and demonstrate how the algorithm naturally emerges from the problem formulation. Based on the observation that the number of votes for matching places can be evaluated using a binomial distribution model, loop closures can be detected with high precision. By casting the problem into a probabilistic framework, we not only remove the need for commonly employed heuristic parameters but also provide a powerful score to classify matching and non-matching places. We present methods for both a 2D-2D pose-graph vertex matching and a 2D-3D landmark matching based on the above scoring. The approach maintains accuracy while being efficient enough for online application through the use of compact (low dimensional) descriptors and fast nearest neighbor retrieval techniques. The proposed methods are evaluated on several challenging datasets in varied environments, showing state-of-the-art results with high precision and high recall.},
archivePrefix = {arXiv},
arxivId = {1610.03548},
author = {Gehrig, Mathias and Stumm, Elena and Hinzmann, Timo and Siegwart, Roland},
booktitle = {Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)},
eprint = {1610.03548},
file = {:home/nathan/Documents/Mendeley Desktop/Gehrig et al/Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)/Gehrig et al.{\_}2017{\_}Visual Place Recognition with Probabilistic Voting.pdf:pdf},
isbn = {9781509046324},
keywords = {Localization,Recognition,SLAM},
pages = {3192--3199},
title = {{Visual Place Recognition with Probabilistic Voting}},
url = {http://arxiv.org/abs/1610.03548},
year = {2017}
}
@inproceedings{Jacobson2017,
abstract = {Learning and recognition is a fundamental process performed in many robot operations such as mapping and localization. The majority of approaches share some common characteristics, such as attempting to extract salient features, landmarks or signatures, and growth in data storage and computational requirements as the size of the environment increases. In biological systems, spatial encoding in the brain is definitively known to be performed using a fixed-size neural encoding framework - the place, head-direction and grid cells found in the mammalian hippocampus and entorhinal cortex. Particularly paradoxically, one of the main encoding centers - the grid cells - represents the world using a highly aliased, repetitive encoding structure where one neuron represents an unbounded number of places in the world. Inspired by this system, in this paper we invert the normal approach used in forming mapping and localization algorithms, by developing a novel place recognition algorithm that seeks out and leverages repetitive, mutually complementary landmark frequencies in the world. The combinatorial encoding capacity of multiple different frequencies enables not only the ability to achieve efficient data storage, but also the potential for sub-linear storage growth in a learning and recall system. Using both ground-based and aerial camera datasets, we demonstrate the system finding and utilizing these frequencies to achieve successful place recognition, and discuss how this approach might scale to arbitrarily large global datasets and dimensions.},
archivePrefix = {arXiv},
arxivId = {1707.06393},
author = {Jacobson, Adam and Scheirer, Walter and Milford, Michael J.},
booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
eprint = {1707.06393},
file = {:home/nathan/Documents/Mendeley Desktop/Jacobson, Scheirer, Milford/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Jacobson, Scheirer, Milford{\_}2017{\_}Deja vu Scalable Place Recognition Using Mutually Supportive Feature Frequencies.pdf:pdf},
title = {{Deja vu: Scalable Place Recognition Using Mutually Supportive Feature Frequencies}},
url = {http://arxiv.org/abs/1707.06393},
year = {2017}
}
@article{Kozlova2016,
abstract = {We present a method which is a continuation of work-in-progress paper (Kozlova et al., 2014). LRO NAC DEM, orthomosaic, and the results of detailed studies of the Lunokhod-2 route were used for modeling of individual sites in order to find observation points for archival panoramas. The method suggests determination of rover coordinates from photogrammetry processing of obtained surface images together with artificially modeled synthetic images produced from high-resolution DEM and orthomosaic. We have now tested the method on models and on real Lunokhod-2 data. The method proved its effectiveness on models, however, for Lunokhod-2 panoramas further research is needed to calibrate the camera properly.},
author = {Kozlova, N. A. and Zubarev, A. E. and Patratiy, V. D. and Konopikhin, A. A. and Oberst, J.},
doi = {10.5194/isprsarchives-XLI-B4-435-2016},
file = {:home/nathan/Documents/Mendeley Desktop/Kozlova et al/The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences/Kozlova et al.{\_}2016{\_}Method of a Planetary Rover Localization Based on Synthetic Lunokhod Images.pdf:pdf},
issn = {2194-9034},
journal = {The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {artificial image modelling,lunokhod,navigation,photogrammetry,planetary geodesy,software,the moon},
number = {July},
pages = {435--439},
title = {{Method of a Planetary Rover Localization Based on Synthetic Lunokhod Images}},
url = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLI-B4/435/2016/isprs-archives-XLI-B4-435-2016.pdf},
volume = {XLI-B4},
year = {2016}
}
@inproceedings{Kuznietsov2017,
abstract = {Supervised deep learning often suffers from the lack of sufficient training data. Specifically in the context of monocular depth map prediction, it is barely possible to determine dense ground truth depth images in realistic dynamic outdoor environments. When using LiDAR sensors, for instance, noise is present in the distance measurements, the calibration between sensors cannot be perfect, and the measurements are typically much sparser than the camera images. In this paper, we propose a novel approach to depth map prediction from monocular images that learns in a semi-supervised way. While we use sparse ground-truth depth for supervised learning, we also enforce our deep network to produce photoconsistent dense depth maps in a stereo setup using a direct image alignment loss. In experiments we demonstrate superior performance in depth map prediction from single images compared to the state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1702.02706},
author = {Kuznietsov, Yevhen and St{\"{u}}ckler, J{\"{o}}rg and Leibe, Bastian},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.238},
eprint = {1702.02706},
file = {:home/nathan/Documents/Mendeley Desktop/Kuznietsov, St{\"{u}}ckler, Leibe/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Kuznietsov, St{\"{u}}ckler, Leibe{\_}2017{\_}Semi-Supervised Deep Learning for Monocular Depth Map Prediction.pdf:pdf},
title = {{Semi-Supervised Deep Learning for Monocular Depth Map Prediction}},
url = {http://arxiv.org/abs/1702.02706},
year = {2017}
}
@article{Cheng2016,
annote = {R{\'{e}}sum{\'{e}}
Comparaison des descripteurs SURF et SIFT (impl{\'{e}}mentation OpenCV) pour faire de l'odom{\'{e}}trie visuelle. Detection des descripteur en utilisant une grille pour mieux r{\'{e}}partir la densit{\'{e}} des descripteurs dans l'espace (un peu comme DenseSIFT), puis reconstruction des poses cons{\'{e}}cutives par algo des 5 pts pour les deux premi{\`{e}}res images (dont on connait la position), puis reconstruction de la pose par correspondance 2D/3D sur les points reconstruits initiallement, puis LBA pour am{\'{e}}liorer la pose. Comparaison des descripteurs seulon 4 crit{\`{e}}res : la stabilit{\'{e}}, la variance, la pr{\'{e}}cision de la loc et le cout calculatoire. Conclusion que SIFT est mieux.

Ce que je n'ai pas compris
- Ce que repr{\'{e}}sente conceptuellement la mesure de la variance.

Ce qui est int{\'{e}}ressant
- Pr{\'{e}}cison de la m{\'{e}}thode
- Local BA (tr{\`{e}}s rapide)

Critiques
- Adapt{\'{e}} seulement au probl{\`{e}}me de localisation pr{\'{e}}cises
- Temps de calcul trop long
- Dataset pas challenging

Computational load
Offline: - 
Online: {\textgreater} 2s

Scalability
Scale: -
Scalability potential: -},
author = {Cheng, Gong and Han, Junwei},
doi = {10.1016/j.isprsjprs.2016.03.014},
file = {:home/nathan/Documents/Mendeley Desktop/Cheng, Han/ISPRS Journal of Photogrammetry and Remote Sensing/Cheng, Han{\_}2016{\_}A Survey on Object Detection in Optical Remote Sensing Images.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {deep learning,machine learning,obia,object detection,object-based image analysis,optical remote sensing images,template matching,weakly supervised learning},
title = {{A Survey on Object Detection in Optical Remote Sensing Images}},
year = {2016}
}
@inproceedings{Workman2015,
abstract = {We propose to use deep convolutional neural networks to address the problem of cross-view image geolocalization, in which the geolocation of a ground-level query image is estimated by matching to georeferenced aerial images. We use state-of-the-art feature representations for ground-level images and introduce a cross-view training approach for learning a joint semantic feature representation for aerial images. We also propose a network architecture that fuses features extracted from aerial images at multiple spatial scales. To support training these networks, we introduce a massive database that contains pairs of aerial and ground-level images from across the United States. Our methods significantly out-perform the state of the art on two benchmark datasets. We also show, qualitatively, that the proposed feature representations are discriminative at both local and continental spatial scales.},
archivePrefix = {arXiv},
arxivId = {1510.03743},
author = {Workman, Scott and Souvenir, Richard and Jacobs, Nathan},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.451},
eprint = {1510.03743},
file = {:home/nathan/Documents/Mendeley Desktop/Workman, Souvenir, Jacobs/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Workman, Souvenir, Jacobs{\_}2015{\_}Wide-area image geolocalization with aerial reference imagery.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
number = {2},
pages = {3961--3969},
title = {{Wide-area image geolocalization with aerial reference imagery}},
volume = {11-18-Dece},
year = {2015}
}
@inproceedings{Chum2005,
author = {Chum, Ondřej and Matas, Jiri},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
organization = {IEEE},
pages = {220--226},
title = {{Matching with PROSAC-progressive sample consensus}},
volume = {1},
year = {2005}
}
@article{Zheng2017,
abstract = {The Bag-of-Words (BoW) model has been predominantly viewed as the state of the art in Content-Based Image Retrieval (CBIR) systems since 2003. The past 13 years has seen its advance based on the SIFT descriptor due to its advantages in dealing with image transformations. In recent years, image representation based on the Convolutional Neural Network (CNN) has attracted more attention in image retrieval, and demonstrates impressive performance. Given this time of rapid evolution, this article provides a comprehensive survey of image retrieval methods over the past decade. In particular, according to the feature extraction and quantization schemes, we classify current methods into three types, i.e., SIFT-based, one-pass CNN-based, and multi-pass CNN-based. This survey reviews milestones in BoW image retrieval, compares previous works that fall into different BoW steps, and shows that SIFT and CNN share common characteristics that can be incorporated in the BoW model. After presenting and analyzing the retrieval accuracy on several benchmark datasets, we highlight promising directions in image retrieval that demonstrate how the CNN-based BoW model can learn from the SIFT feature.},
archivePrefix = {arXiv},
arxivId = {1608.01807},
author = {Zheng, Liang and Yang, Yi and Tian, Qi},
eprint = {1608.01807},
file = {:home/nathan/Documents/Mendeley Desktop/Zheng, Yang, Tian/IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)/Zheng, Yang, Tian{\_}2017{\_}SIFT Meets CNN A Decade Survey of Instance Retrieval.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
number = {8},
title = {{SIFT Meets CNN: A Decade Survey of Instance Retrieval}},
url = {http://arxiv.org/abs/1608.01807},
volume = {14},
year = {2017}
}
@mastersthesis{Azzi2015mastersThesis,
author = {Azzi, Charbel},
school = {University of Waterloo},
title = {{Efficient Image-Based Localization Using Context}},
year = {2015}
}
@article{Chen2017c,
author = {Chen, Boheng and Li, Jie and Wei, Gang and Ma, Biyun},
doi = {10.1016/j.patcog.2017.10.039},
file = {:home/nathan/Documents/Mendeley Desktop/Chen et al/Pattern Recognition/Chen et al.{\_}2017{\_}A Novel Localized and Second Order Feature Coding Network for Image Recognition.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Deep neural network,End to End,Feature coding network,Image recognition,Localized and second order VLAD},
publisher = {Elsevier Ltd},
title = {{A Novel Localized and Second Order Feature Coding Network for Image Recognition}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320317304466},
year = {2017}
}
@inproceedings{Ren2015,
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
booktitle = {Annual Conference on Neural Information Processing Systems (NIPS)},
pages = {91--99},
title = {{Faster R-CNN: Towards real-time object detection with region proposal networks}},
year = {2015}
}
@inproceedings{Hoffman2016a,
abstract = {In this paper we propose a technique to adapt convolutional neural network (CNN) based object detectors trained on RGB images to effectively leverage depth images at test time to boost detection performance. Given labeled depth images for a handful of categories we adapt an RGB object detector for a new category such that it can now use depth images in addition to RGB images at test time to produce more accurate detections. Our approach is built upon the observation that lower layers of a CNN are largely task and category agnostic and domain specific while higher layers are largely task and category specific while being domain agnostic. We operationalize this observation by proposing a mid-level fusion of RGB and depth CNNs. Experimental evaluation on the challenging NYUD2 dataset shows that our proposed adaptation technique results in an average 21{\%} relative improvement in detection performance over an RGB-only baseline even when no depth training data is available for the particular category evaluated. We believe our proposed technique will extend advances made in computer vision to RGB-D data leading to improvements in performance at little additional annotation effort.},
annote = {Toujours pour la tache de d{\'{e}}t{\'{e}}ction d'objet, mais cette fois si quand on a en input des images RGB-D. Les auteurs prouve que l'on peut exploiter des images de profondeur au test time pour am{\'{e}}liorer la d{\'{e}}tection d'objet m{\^{e}}me si on a pas de label dans la modalit{\'{e}} depth au moment de l'entrainement. Concr{\'{e}}tement ils ont deux reseaux, un RGB et un D qu'ils merge ensuite avec des layers fc pour cat{\'{e}}goriser les objets. Ils entrainent ind{\'{e}}pendament les resaux RGB et D, sur respectivement n et n-1 cat{\'{e}}gorie d'objet {\`{a}} trouver. Ensuite fusionne les deux r{\'{e}}seaux (tronqu{\'{e}}), en r{\'{e}}cup{\'{e}}rant le Softmax du RGB. Ca permet d'am{\'{e}}liorer les r{\'{e}}sultats compar{\'{e}} {\`{a}} un r{\'{e}}seau pur RGB.},
author = {Hoffman, Judy and Gupta, Saurabh and Leong, Jian and Guadarrama, Sergio and Darrell, Trevor},
booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2016.7487708},
file = {:home/nathan/Documents/Mendeley Desktop/Hoffman et al/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Hoffman et al.{\_}2016{\_}Cross-modal adaptation for RGB-D detection.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
pages = {5032--5039},
title = {{Cross-modal adaptation for RGB-D detection}},
volume = {2016-June},
year = {2016}
}
@inproceedings{Chum2011,
author = {Chum, Ondřej and Mikul, Andrej and Perdoch, Michal and Matas, Jiri},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Chum et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Chum et al.{\_}2011{\_}Total Recall II Query Expansion Revisited.pdf:pdf},
title = {{Total Recall II : Query Expansion Revisited}},
year = {2011}
}
@inproceedings{Mason2011,
abstract = {A textured occupancy grid map is an extremely$\backslash$nversatile data structure. It can be used to render humanreadable$\backslash$nviews and for laser rangefinder localization algorithms.$\backslash$nFor camera-based localization, landmark or featurebased$\backslash$nmaps tend to be favored in current research. This may$\backslash$nbe because of a tacit assumption that working with a textured$\backslash$noccupancy grid with a camera would be impractical. We$\backslash$ndemonstrate that a textured occupancy grid can be combined$\backslash$nwith an extremely simple monocular localization algorithm to$\backslash$nproduce a viable localization solution. Our approach is simple,$\backslash$nefficient, and produces localization results comparable to laser$\backslash$nlocalization results. A consequence of this result is that a single$\backslash$nmap representation, the textured occupancy grid, can now be$\backslash$nused for humans, robots with laser rangefinders, and robots$\backslash$nwith just a single camera.},
author = {Mason, Julian and Ricco, Susanna and Parr, Ronald},
booktitle = {Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2011.5980506},
file = {:home/nathan/Documents/Mendeley Desktop/Mason, Ricco, Parr/Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)/Mason, Ricco, Parr{\_}2011{\_}Textured Occupancy Grids for Monocular Localization Without Features.pdf:pdf},
isbn = {9781612843803},
pages = {5800--5806},
title = {{Textured Occupancy Grids for Monocular Localization Without Features}},
year = {2011}
}
@inproceedings{Liang2013,
abstract = {—Image based localization is an important problem with many applications. In our previous work, we presented a two step pipeline for performing image based localization of mobile devices in outdoor environments. In the first step, a query image is matched against a georeferenced 3D image database to retrieve the " closest " image. In the second step, the pose of the query image is recovered with respect to the " closest " image using cell phone sensors. As such, a key ingredient of our outdoor image based localization is a 3D georeferenced image database. In this paper, we extend this approach to indoors by utilizing a 3D locally referenced image database generated by an ambulatory depth acquisition backpack that is originally developed for 3D modeling of indoor environments. We demonstrate retrieval rate of 94{\%} over a set of 83 query images taken in an indoor shopping center and characterize pose recovery accuracy of the same set.},
annote = {R{\'{e}}sum{\'{e}}

Pipeline de localisation bas{\'{e}} vision en int{\'{e}}rieur. Base de donn{\'{e}}es d'images g{\'{e}}olocalis{\'{e}}es cr{\'{e}}e gr{\^{a}}ce {\`{a}} un laser backpack. Determination de la position en deux {\'{e}}tapes : matching avec l'image la plus proche de la base puis recallage a partir des correspondances entre les local features des deux images.

Ce que je n'ai pas compris

- Les d{\'{e}}tailles de la proc{\'{e}}dure de matching

Ce qui est int{\'{e}}ressant

- La m{\'{e}}thode de matching {\`{a}} base de kd-tree
- Les bons r{\'{e}}sultats avec la m{\'{e}}thode d'alignement (m{\^{e}}me si pas de vrai VT)

Critiques

- Tr{\`{e}}s petite base de donn{\'{e}}es
- Matching peu challenging (indoor dans un centre commercial avec {\'{e}}clairage tr{\`{e}}s peu variant)
- Temps de clacul relativement long alors qu'on a une toute petite base de donn{\'{e}}es
- Images a haute r{\'{e}}solution

A approfondir

- FLANN matching Kd tree

Computational load
Offline: ANN database creation
Online: 10 s

Scalability
Scale: Place (indoor)
Scalability potential: Not specialy designed to},
author = {Liang, Jason Zhi and Corso, Nicholas and Turner, Eric and Zakhor, Avideh},
booktitle = {Computing for Geospatial Research and Application},
file = {:home/nathan/Documents/Mendeley Desktop/Liang et al/Computing for Geospatial Research and Application/Liang et al.{\_}2013{\_}Image Based Localization in Indoor Environments.pdf:pdf},
keywords = {3D models,3D reconstruc-tion,Alignement,Indoor,Local features,RGB request,SIFT,Visual place recognition,indoor localization,—image retrieval},
mendeley-tags = {3D models,Alignement,Indoor,Local features,RGB request,SIFT,Visual place recognition},
title = {{Image Based Localization in Indoor Environments}},
year = {2013}
}
@inproceedings{Hauagge2012,
author = {Hauagge, Daniel Cabrini and Snavely, Noah},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
organization = {IEEE},
pages = {206--213},
title = {{Image matching using local symmetry features}},
year = {2012}
}
@inproceedings{Deng2009,
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
organization = {IEEE},
pages = {248--255},
title = {{Imagenet: A large-scale hierarchical image database}},
year = {2009}
}
@inproceedings{Jegou2012a,
abstract = {The paper addresses large scale image retrieval with short vector representations. We study dimensionality reduction by Principal Component Analysis (PCA) and propose improvements to its different phases. We show and explicitly exploit relations between i) mean subtrac-tion and the negative evidence, i.e., a visual word that is mutually miss-ing in two descriptions being compared, and ii) the axis de-correlation and the co-occurrences phenomenon. Finally, we propose an effective way to alleviate the quantization artifacts through a joint dimensionality re-duction of multiple vocabularies. The proposed techniques are simple, yet significantly and consistently improve over the state of the art on compact image representations. Complementary experiments in image classification show that the methods are generally applicable.},
author = {J{\'{e}}gou, Herv{\'{e}} and Chum, Ondřej},
booktitle = {Proceedings of the IEEE European conference on computer vision (ECCV)},
file = {:home/nathan/Documents/Mendeley Desktop/J{\'{e}}gou, Chum/Proceedings of the IEEE European conference on computer vision (ECCV)/J{\'{e}}gou, Chum{\_}2012{\_}Negative evidences and co-occurrences in image retrieval the benefit of PCA and whitening.pdf:pdf},
title = {{Negative evidences and co-occurrences in image retrieval : the benefit of PCA and whitening.}},
year = {2012}
}
@article{Gehrig2016,
abstract = {We propose a novel scoring concept for visual place recognition based on nearest neighbor descriptor voting and demonstrate how the algorithm naturally emerges from the problem formulation. Based on the observation that the number of votes for matching places can be evaluated using a binomial distribution model, loop closures can be detected with high precision. By casting the problem into a probabilistic framework, we not only remove the need for commonly employed heuristic parameters but also provide a powerful score to classify matching and non-matching places. We present methods for both a 2D-2D pose-graph vertex matching and a 2D-3D landmark matching based on the above scoring. The approach maintains accuracy while being efficient enough for online application through the use of compact (low dimensional) descriptors and fast nearest neighbor retrieval techniques. The proposed methods are evaluated on several challenging datasets in varied environments, showing state-of-the-art results with high precision and high recall.},
archivePrefix = {arXiv},
arxivId = {1610.03548},
author = {Gehrig, Mathias and Stumm, Elena and Hinzmann, Timo and Siegwart, Roland},
eprint = {1610.03548},
file = {:home/nathan/Documents/Mendeley Desktop/Gehrig et al/arXiv preprint/Gehrig et al.{\_}2016{\_}Visual Place Recognition with Probabilistic Vertex Voting.pdf:pdf},
journal = {arXiv preprint},
title = {{Visual Place Recognition with Probabilistic Vertex Voting}},
url = {http://arxiv.org/abs/1610.03548},
year = {2016}
}
@article{Nam2017,
author = {Nam, Tae and Shim, Jae and Cho, Young},
doi = {10.3390/s17122730},
file = {:home/nathan/Documents/Mendeley Desktop/Nam, Shim, Cho/Sensors/Nam, Shim, Cho{\_}2017{\_}A 2.5D Map-Based Mobile Robot Localization via Cooperation of Aerial and Ground Robots.pdf:pdf},
issn = {1424-8220},
journal = {Sensors},
keywords = {aerial robot,cooperation,ground robot,indoor,localization,low cost sensor,slam},
number = {12},
pages = {2730},
title = {{A 2.5D Map-Based Mobile Robot Localization via Cooperation of Aerial and Ground Robots}},
url = {http://www.mdpi.com/1424-8220/17/12/2730},
volume = {17},
year = {2017}
}
@article{Martinez-Carranza2015,
author = {Mart{\'{i}}nez-Carranza, Jos{\'{e}} and Bostock, Richard and Willcox, Simon and Cowling, Ian and Mayol-Cuevas, Walterio},
doi = {10.1080/01691864.2015.1094409},
file = {:home/nathan/Documents/Mendeley Desktop/Mart{\'{i}}nez-Carranza et al/Advanced Robotics/Mart{\'{i}}nez-Carranza et al.{\_}2015{\_}Indoor MAV auto-retrieval using fast 6D relocalisation.pdf:pdf},
issn = {0169-1864},
journal = {Advanced Robotics},
pages = {1--12},
title = {{Indoor MAV auto-retrieval using fast 6D relocalisation}},
url = {http://www.tandfonline.com/doi/full/10.1080/01691864.2015.1094409},
volume = {30},
year = {2015}
}
@inproceedings{Walch2016a,
abstract = {In this work we propose a new CNN+LSTM architecture for camera pose regression for indoor and outdoor scenes. CNNs allow us to learn suitable feature representations for localization that are robust against motion blur and illumination changes. We make use of LSTM units on the CNN output in spatial coordinates in order to capture contextual information. This substantially enlarges the receptive field of each pixel leading to drastic improvements in localization performance. We provide extensive quantitative comparison of CNN-based vs SIFT-based localization methods, showing the weaknesses and strengths of each. Furthermore, we present a new large-scale indoor dataset with accurate ground truth from a laser scanner. Experimental results on both indoor and outdoor public datasets show our method outperforms existing deep architectures, and can localize images in hard conditions, e.g., in the presence of mostly textureless surfaces.},
annote = {R{\'{e}}sum{\'{e}}
Am{\'{e}}lioration du CNN PoseNet (Kendall2015) avec l'ajout en sortie du reseau une unit{\'{e}} Long-Short Term Memory (comme Weyand2016) pour renforcer la consistence spatiale. Ils introduisent {\'{e}}galement un nouveau jeu de donn{\'{e}}e (en indoor) et ils mettent en avant le fait qu'ils aient compare leur m{\'{e}}thode avec des m{\'{e}}thodes directs basees SfM (Pami 2016 de Sattler). 

Ce que je n'ai pas compris
- Comment ca marche le LSTM

Ce qui est int{\'{e}}ressant
- LSTM
- Etat de l'art complet
- Dataset

Critiques
- Pas beaucoup mieux que PoseNet

Computational load
Offline: -
Online: pas d'info

Scalability
Scale: Places
Scalability potential: -},
archivePrefix = {arXiv},
arxivId = {1611.07890},
author = {Walch, Florian and Hazirbas, Caner and Leal-Taix{\'{e}}, Laura and Sattler, Torsten and Hilsenbeck, Sebastian and Cremers, Daniel},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
eprint = {1611.07890},
file = {:home/nathan/Documents/Mendeley Desktop/Walch et al/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Walch et al.{\_}2017{\_}Image-based Localization with Spatial LSTMs.pdf:pdf},
title = {{Image-based Localization with Spatial LSTMs}},
url = {http://arxiv.org/abs/1611.07890},
year = {2017}
}
@inproceedings{Baatz2012,
abstract = {Given a picture taken somewhere in the world, automatic geo-localization of that image is a task that would be extremely useful e. g. for historical and forensic sciences, documentation purposes, organization of the world's photo material and also intelligence applications. While tremendous progress has been made over the last years in visual location recognition within a single city, localization in natural environments is much more difficult, since vegetation, illumination, seasonal changes make appearance-only approaches impractical. In this work, we target mountainous terrain and use digital elevation models to extract representations for fast visual database lookup. We propose an automated approach for very large scale visual localization that can efficiently exploit visual information (contours) and geometric constraints (consistent orientation) at the same time. We validate the system on the scale of a whole country (Switzerland, 40 000km(2)) using a new dataset of more than 200 landscape query pictures with ground truth.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de localisation global d'une image dans un massif montagneux. Les auteurs se basent sur la connaissance d'une carte d'{\'{e}}l{\'{e}}vation (DEM) et la connaissance de la ligne d'horizon dans l'image. La premi{\`{e}}re {\'{e}}tape consiste {\`{a}} segmenter l'image pour obtenir la ligne d'horizon (par depth estimation with dehazeing et apprentissage), puis {\`{a}} coder cette courbe sous forme de visual word like qui encode une information d'angle relatif dans l'image origine, puis {\`{a}} int{\'{e}}roger une base de donn{\'{e}}e cr{\'{e}}{\'{e}}e gr{\^{a}}ce au mod{\`{e}}le d'{\'{e}}l{\'{e}}vation (matching grace au vecteur issu de la quantification en visual words ET d'une verification d'organisation spatial relative gr{\^{a}}ce {\`{a}} l'angle encod{\'{e}} dans les visual words). Ils appliquent enfin une v{\'{e}}rification g{\'{e}}ometrique sur le profil d'horizon par ICP.

Ce qui est int{\'{e}}ressant
- La verification g{\'{e}}om{\'{e}}trique au moment de comparer la query {\`{a}} la base de donn{\'{e}}e
- Le depth from dehaze

Critiques
- Application tr{\`{e}}s sp{\'{e}}cialis{\'{e}}e

A approfondir
- Depth from dehaze

Computational load
Offline: Pas mentionn{\'{e}}
Online: 10s

Scalability
Scale: District but with specific panormas 
Scalability potential: Like BoW methods, quantization artifact},
author = {Baatz, Georges and Saurer, Olivier and K{\"{o}}ser, Kevin and Pollefeys, Marc},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
file = {:home/nathan/Documents/Mendeley Desktop/Baatz et al/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Baatz et al.{\_}2012{\_}Large Scale Visual Geo-Localization of Images in Mountainous Terrain.pdf:pdf},
isbn = {978-3-642-33708-6},
pages = {517--530},
title = {{Large Scale Visual Geo-Localization of Images in Mountainous Terrain}},
volume = {7573},
year = {2012}
}
@inproceedings{Irschara2009,
abstract = {Efficient view registration with respect to a given 3D re- construction has many applications like inside-out tracking in indoor and outdoor environments, and geo-locating im- ages from large photo collections. We present a fast loca- tion recognition technique based on structure from motion point clouds. Vocabulary tree-based indexing of features directly returns relevant fragments of 3D models instead of documents from the images database. Additionally, we pro- pose a compressed 3D scene representation which improves recognition rates while simultaneously reducing the compu- tation time and the memory consumption. The design of our method is based on algorithms that efficiently utilize mod- ern graphics processing units to deliver real-time perfor- mance for view registration. We demonstrate the approach by matching hand-held outdoor videos to known 3D urban models, and by registering images from online photo collec- tions to the corresponding landmarks. 1.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de localisation rapide et precise d'une image {\`{a}} partir d'un "document 3D" (3D modele). Les auteurs se place dans le cadre ou l'on poss{\`{e}}de une reconstruction de l'environnement par SfM (on poss{\`{e}}de donc des descripteurs SIFT pour chaque point 3D). Des vues sythetis{\'{e}}es coupl{\'{e}}es avec les vues prises pour la reconstruction 3D sont selectionn{\'{e}}es pour couvrire tous l'espace du mod{\`{e}}le. Un vocabulary tree est ensuite utilis{\'{e}} pour matcher la query dans la base de donn{\'{e}}e. On utilise enfin un algo de 3 points pour retrouver la position exacte de la cam{\'{e}}ra.

Ce que je n'ai pas compris
- L'algo de compression des documents 3D

Ce qui est int{\'{e}}ressant
- La rapidit{\'{e}}
- La compression des vues sythetiques 
- Le crit{\`{e}}re de matching sur les inlier du RANSAC (Nombre de points SIFT * r{\'{e}}partition spatiale)

Critiques
- Dataset tr{\`{e}}s peu challenging, et d{\`{e}}s qu'on s'{\'{e}}carte des points prise lors de la SfM, les r{\'{e}}sultats sont faibles

Computational load
Offline: Vocabulary tree creation
Online: {\~{}}1 sec

Scalability
Scale: Place
Scalability potential: Quantization issues},
author = {Irschara, Arnold and Zach, Christopher and Frahm, Jan-michael and Bischof, Horst},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Irschara et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Irschara et al.{\_}2009{\_}From Structure-from-Motion Point Clouds to Fast Location Recognition.pdf:pdf},
keywords = {3D models,Alignement,Local features,Outdoor,SIFT,SfM,View Synthesis,Visual place recognition,Visual words},
mendeley-tags = {3D models,Alignement,Local features,Outdoor,SIFT,SfM,View Synthesis,Visual place recognition,Visual words},
title = {{From Structure-from-Motion Point Clouds to Fast Location Recognition}},
year = {2009}
}
@article{Lu2017,
author = {Lu, Guoyu and Yan, Yan and Sebe, Nicu and Kambhamettu, Chandra},
doi = {10.1016/j.cviu.2017.05.003},
file = {:home/nathan/Documents/Mendeley Desktop/Lu et al/Computer Vision and Image Understanding (CVIU)/Lu et al.{\_}2017{\_}Indoor Localization via Multi-View Images and Videos.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding (CVIU)},
keywords = {Multi-task Learning,Self-Similarity Matrix,Vision-based Localization},
publisher = {Elsevier Inc.},
title = {{Indoor Localization via Multi-View Images and Videos}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314217300802},
year = {2017}
}
@article{Arth2015,
abstract = {We present a method for large-scale geo-localization and global tracking of mobile devices in urban outdoor environments. In contrast to existing methods, we instantaneously initialize and globally register a SLAM map by localizing the first keyframe with respect to widely available untextured 2.5D maps. Given a single image frame and a coarse sensor pose prior, our localization method estimates the absolute camera orientation from straight line segments and the translation by aligning the city map model with a semantic segmentation of the image. We use the resulting 6DOF pose, together with information inferred from the city map model, to reliably initialize and extend a 3D SLAM map in a global coordinate system, applying a model-supported SLAM mapping approach. We show the robustness and accuracy of our localization approach on a challenging dataset, and demonstrate unconstrained global SLAM mapping and tracking of arbitrary camera motion on several sequences.},
annote = {From Duplicate 1 (Instant Outdoor Localization and SLAM Initialization from 2.5D Maps - Arth, Clemens; Pirchheim, Christian; Ventura, Jonathan; Schmalstieg, Dieter; Lepetit, Vincent)

R{\'{e}}sum{\'{e}}

M{\'{e}}thode de recalage pr{\`{e}}cis d'une image RGB localis{\'{e}}e grossi{\`{e}}rement gr{\^{a}}ce {\`{a}} un mod{\`{e}}le 2.5D g{\'{e}}olocalis{\'{e}}. Utilisation d'openstreet map pour obtenir la g{\'{e}}om{\'{e}}trie approximatice des batiments pouvant {\^{e}}tre compris dans l'image. On proc{\`{e}}de d'abord {\`{a}} une correction de l'orientation relative de l'image pour qu'elle soit align{\'{e}}e avec la normal au sol (en utilisant la g{\'{e}}om{\'{e}}trie des vanishings points). On extrait ensuite des segments dans l'image pour localiser les arr{\^{e}}tes des batiments et recaler l'image sur le mod{\`{e}}le 2.5D r{\'{e}}cup{\'{e}}r{\'{e}} pr{\'{e}}c{\'{e}}dement. On valide le recalage en segmentant s{\'{e}}mantiquement l'image (SVM) pour s'assurer qu'elle correspond au mod{\`{e}}le physique. Cette m{\'{e}}thode permet ensuite d'initialiser un SLAM en outdoor.

Ce que je n'ai pas compris

- G{\'{e}}ometrie des vanishing points
- Exponentiation SO(3)

Ce qui est int{\'{e}}ressant

- Le couplage d'infos s{\'{e}}mantique avec de l'information g{\'{e}}om{\'{e}}trique
- OpenStreetMap
- La dimension cross-data

Critiques

- 30 sec de calcul pour recaler l'image sur le mod{\`{e}}le
- Necessite d'avoir une postion {\`{a}} priori

A approfondir

- BSP tree

Computational load
Offline: - 
Online: 30s

Scalability
Scale: Known place
Scalability potential: -

From Duplicate 2 (Instant Outdoor Localization and SLAM Initialization from 2.5D Maps - Arth, Clemens; Pirchheim, Christian; Ventura, Jonathan; Schmalstieg, Dieter; Lepetit, Vincent)

R{\{}{\'{e}}{\}}sum{\{}{\'{e}}{\}}

M{\{}{\'{e}}{\}}thode de recalage pr{\{}{\`{e}}{\}}cis d'une image RGB localis{\{}{\'{e}}{\}}e grossi{\{}{\`{e}}{\}}rement gr{\{}{\^{a}}{\}}ce {\{}{\`{a}}{\}} un mod{\{}{\`{e}}{\}}le 2.5D g{\{}{\'{e}}{\}}olocalis{\{}{\'{e}}{\}}. Utilisation d'openstreet map pour obtenir la g{\{}{\'{e}}{\}}om{\{}{\'{e}}{\}}trie approximatice des batiments pouvant {\{}{\^{e}}{\}}tre compris dans l'image. On proc{\{}{\`{e}}{\}}de d'abord {\{}{\`{a}}{\}} une correction de l'orientation relative de l'image pour qu'elle soit align{\{}{\'{e}}{\}}e avec la normal au sol (en utilisant la g{\{}{\'{e}}{\}}om{\{}{\'{e}}{\}}trie des vanishings points). On extrait ensuite des segments dans l'image pour localiser les arr{\{}{\^{e}}{\}}tes des batiments et recaler l'image sur le mod{\{}{\`{e}}{\}}le 2.5D r{\{}{\'{e}}{\}}cup{\{}{\'{e}}{\}}r{\{}{\'{e}}{\}} pr{\{}{\'{e}}{\}}c{\{}{\'{e}}{\}}dement. On valide le recalage en segmentant s{\{}{\'{e}}{\}}mantiquement l'image (SVM) pour s'assurer qu'elle correspond au mod{\{}{\`{e}}{\}}le physique. Cette m{\{}{\'{e}}{\}}thode permet ensuite d'initialiser un SLAM en outdoor.

Ce que je n'ai pas compris

- G{\{}{\'{e}}{\}}ometrie des vanishing points
- Exponentiation SO(3)

Ce qui est int{\{}{\'{e}}{\}}ressant

- Le couplage d'infos s{\{}{\'{e}}{\}}mantique avec de l'information g{\{}{\'{e}}{\}}om{\{}{\'{e}}{\}}trique
- OpenStreetMap
- La dimension cross-data

Critiques

- 30 sec de calcul pour recaler l'image sur le mod{\{}{\`{e}}{\}}le
- Necessite d'avoir une postion {\{}{\`{a}}{\}} priori

A approfondir

- BSP tree},
author = {Arth, Clemens and Pirchheim, Christian and Ventura, Jonathan and Schmalstieg, Dieter and Lepetit, Vincent},
doi = {10.1109/TVCG.2015.2459772},
file = {:home/nathan/Documents/Mendeley Desktop/Arth et al/IEEE Transactions on Visualization and Computer Graphics (ToVCG)/Arth et al.{\_}2015{\_}Instant Outdoor Localization and SLAM Initialization from 2.5D Maps.pdf:pdf},
isbn = {1077-2626},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics (ToVCG)},
keywords = {2D map,Alignement,Multiple visual domaines,Outdoor,RGB request,Robotic,SLAM,SVM,Segmentation,geo-localization,image registration,outdoor augmented reality},
mendeley-tags = {Alignement,Multiple visual domaines,Outdoor,RGB request,Robotic,SVM,Segmentation},
number = {11},
pages = {1309--1318},
pmid = {26340773},
title = {{Instant Outdoor Localization and SLAM Initialization from 2.5D Maps}},
volume = {21},
year = {2015}
}
@article{Zheng2016,
abstract = {The objective of this paper is the effective transfer of the Convolutional Neural Network (CNN) feature in image search and classification. Systematically, we study three facts in CNN transfer. 1) We demonstrate the advantage of using images with a properly large size as input to CNN instead of the conventionally resized one. 2) We benchmark the performance of different CNN layers improved by average/max pooling on the feature maps. Our observation suggests that the Conv5 feature yields very competitive accuracy under such pooling step. 3) We find that the simple combination of pooled features extracted across various CNN layers is effective in collecting evidences from both low and high level descriptors. Following these good practices, we are capable of improving the state of the art on a number of benchmarks to a large margin.},
archivePrefix = {arXiv},
arxivId = {1604.00133},
author = {Zheng, Liang and Zhao, Yali and Wang, Shengjin and Wang, Jingdong and Tian, Qi},
eprint = {1604.00133},
file = {:home/nathan/Documents/Mendeley Desktop/Zheng et al/arXiv/Zheng et al.{\_}2016{\_}Good Practice in CNN Feature Transfer.pdf:pdf},
journal = {arXiv},
pages = {9},
title = {{Good Practice in CNN Feature Transfer}},
url = {http://arxiv.org/abs/1604.00133},
year = {2016}
}
@inproceedings{Arandjelovic2014a,
abstract = {Successful large scale object instance retrieval systems are typically based on accurate matching of local descriptors, such as SIFT. However, these local descriptors are often not sufficiently distinctive to prevent false correspondences, as they only consider the gradient appearance of the local patch, without being able to “see the big picture”. We describe a method, SemanticSIFT, which takes account of local image semantic content (such as grass and sky) in matching, and thereby eliminates many false matches. We show that this enhanced descriptor can be employed in standard large scale inverted file systems with the following benefits: improved precision (as false retrievals are suppressed); an almost two-fold speedup in retrieval speed (as posting lists are shorter on average); and, depending on the target application, a 20{\%} decrease in memory requirements (since unrequired ‘semantic' words can be removed). Furthermore, we also introduce a fast, and near state of the art, semantic segmentation algorithm. Quantitative and qualitative results on standard benchmark datasets (Oxford Buildings 5 k and 105 k) demonstrate the effectiveness of our approach.},
author = {Arandjelovi{\'{c}}, Relja and Zisserman, Andrew},
booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
doi = {10.1007/978-3-319-16865-4_12},
file = {:home/nathan/Documents/Mendeley Desktop/Arandjelovi{\'{c}}, Zisserman/Proceedings of the Asian Conference on Computer Vision (ACCV)/Arandjelovi{\'{c}}, Zisserman{\_}2014{\_}Visual vocabulary with a semantic twist.pdf:pdf},
isbn = {9783319168647},
issn = {16113349},
pages = {178--195},
title = {{Visual vocabulary with a semantic twist}},
volume = {9003},
year = {2014}
}
@inproceedings{Chen2017a,
author = {Chen, Zetao and Maffra, Fabiola and Sa, Inkyu and Chli, Margarita},
booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.3929/ethz-a-010782581},
file = {:home/nathan/Documents/Mendeley Desktop/Chen et al/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Chen et al.{\_}2017{\_}Only Look Once, Mining Distinctive Landmarks from ConvNet for Visual Place Recognition.pdf:pdf},
isbn = {8610828378018},
title = {{Only Look Once, Mining Distinctive Landmarks from ConvNet for Visual Place Recognition}},
year = {2017}
}
@inproceedings{Cham2010,
abstract = {A framework is presented for estimating the pose of a camera based on images extracted from a single omnidirectional image of an urban scene, given a 2D map with building outlines with no 3D geometric information nor appearance data. The framework attempts to identify vertical corner edges of buildings in the query image, which we term VCLH, as well as the neighboring plane normals, through vanishing point analysis. A bottom-up process further groups VCLH into elemental planes and subsequently into 3D structural fragments modulo a similarity transformation. A geometric hashing lookup allows us to rapidly establish multiple candidate correspondences between the structural fragments and the 2D map building contours. A voting-based camera pose estimation method is then employed to recover the correspondences admitting a camera pose solution with high consensus. In a dataset that is even challenging for humans, the system returned a top-30 ranking for correct matches out of 3600 camera pose hypotheses (0.83{\%} selectivity) for 50.9{\%} of queries.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de localisaton bas{\'{e}}e image utilisant une image omnidirectionelle et une base de r{\'{e}}f{\'{e}}rence sous forme de plan 2D de l'emprise au sol des batiments. L'image panoramique est initialement divis{\'{e}}e en quatre sous image classique, les lignes verticales et horizontales sont extraites par filtrage de cany et estimation des points de fuite (vanishing point), puis chaque ligne est d{\'{e}}crite comme un objet (VCLH) associ{\'{e}} aux normals de ses plans adjacents (pour pouvoir la consid{\'{e}}rer comme un angle de batiment, matchable avec le mod{\`{e}}le 2D). Les auteurs proc{\`{e}}dent ensuite {\`{a}} une segmentation de l'espace des emprises de batiment et match les sous segments extraits des images sous forme de pose volting scheme. La pose final de la cam{\'{e}}ra est obtenu par consensus sur les 4 poses de cam{\'{e}}ra obtenu pour chacunes des 4 images.

Ce que je n'ai pas compris
- les d{\'{e}}tails de l'extraction et de la description des lignes
- comment reconstruire des repr{\'{e}}sentation de plus de 2 lignes (vu qu'on est pas sens{\'{e}} avoir l'{\'{e}}chelle) ?

Ce qui est int{\'{e}}ressant
- Permet de r{\'{e}}duire l'{\'{e}}chelle de recherche sans information colorim{\'{e}}trique sur la base de donn{\'{e}}e

Critiques
- R{\'{e}}sultats tr{\`{e}}s moyen
- Temps de calcul relativement long

A approfondir
- Vanishing point
- 2 1/2 D sketch representation

Computational load
Offline: -
Online: 8 - 1 s

Scalability
Scale: Multiple places (440x440m²)
Scalability potential: Possible avec une meilleur description},
author = {Cham, Tat Jen and Ciptadi, Arridhana and Tan, Wei Chian and Pham, Minh Tri and Chia, Liang Tien},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2010.5540191},
file = {:home/nathan/Documents/Mendeley Desktop/Cham et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Cham et al.{\_}2010{\_}Estimating camera pose from a single urban ground-view omnidirectional image and a 2D building outline map.pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
pages = {366--373},
title = {{Estimating camera pose from a single urban ground-view omnidirectional image and a 2D building outline map}},
year = {2010}
}
@article{Wang2017a,
archivePrefix = {arXiv},
arxivId = {1710.00156},
author = {Wang, Chen and Zhang, Handuo and Nguyen, Thien-minh and Xie, Lihua},
eprint = {1710.00156},
file = {:home/nathan/Documents/Mendeley Desktop/Wang et al/arXiv preprint/Wang et al.{\_}2017{\_}Ultra-Wideband Aided Fast Localization and Mapping System.pdf:pdf},
isbn = {9781538626818},
journal = {arXiv preprint},
pages = {1602--1609},
title = {{Ultra-Wideband Aided Fast Localization and Mapping System}},
year = {2017}
}
@inproceedings{Xu2014,
abstract = {Current GPS-based devices have difficulty localizing in cases where the GPS signal is unavailable or insufficiently accurate. This paper presents an algorithm for localizing a vehicle on an arbitrary road network using vision, road curvature estimates, or a combination of both. The method uses an extension of topometric localization, which is a hybrid between topological and metric localization. The extension enables localization on a network of roads rather than just a single, non-branching route. The algorithm, which does not rely on GPS, is able to localize reliably in situations where GPS-based devices fail, including “urban canyons” in downtown areas and along ambiguous routes with parallel roads. We demonstrate the algorithm experimentally on several road networks in urban, suburban, and highway scenarios. We also evaluate the road curvature descriptor and show that it is effective when imagery is sparsely available.},
author = {Xu, Danfei and Badino, Hern{\'{a}}n and Huber, Daniel},
booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2014.6943043},
file = {:home/nathan/Documents/Mendeley Desktop/Xu, Badino, Huber/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Xu, Badino, Huber{\_}2014{\_}Topometric localization on a road network.pdf:pdf},
isbn = {9781479969340},
issn = {21530866},
pages = {3448--3455},
title = {{Topometric localization on a road network}},
year = {2014}
}
@inproceedings{Moulon2012,
author = {Moulon, Pierre and Monasse, Pascal and Marlet, Renaud},
booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
organization = {Springer},
pages = {257--270},
title = {{Adaptive structure from motion with a contrario model estimation}},
year = {2012}
}
@article{Bansal2011,
abstract = {We study the feasibility of solving the challenging problem of geolocalizing ground level images in urban areas with respect to a database of images captured from the air such as satellite and oblique aerial images. We observe that comprehensive aerial image databases are widely available while complete coverage of urban areas from the ground is at best spotty. As a result, localization of ground level imagery with respect to aerial collections is a technically important and practically significant problem. We exploit two key insights: (1) satellite image to oblique aerial image correspondences are used to extract building facades, and (2) building facades are matched between oblique aerial and ground images for geo-localization. Key contributions include: (1) A novel method for extracting building facades using building outlines; (2) Correspondence of building facades between oblique aerial and ground images without direct matching; and (3) Position and orientation estimation of ground images. We show results of ground image localization in a dense urban area.},
author = {Bansal, Mayank and Sawhney, Harpreet S. and Cheng, Hui and Daniilidis, Kostas},
doi = {10.1145/2072298.2071954},
file = {:home/nathan/Documents/Mendeley Desktop/Bansal et al/Proceedings of the ACM International Conference on Multimedia (MM)/Bansal et al.{\_}2011{\_}Geo-localization of street views with aerial image databases.pdf:pdf},
isbn = {9781450306164},
journal = {Proceedings of the ACM International Conference on Multimedia (MM)},
pages = {1125},
title = {{Geo-localization of street views with aerial image databases}},
url = {http://dl.acm.org/citation.cfm?doid=2072298.2071954},
year = {2011}
}
@article{Neubert2015,
abstract = {Changing environments pose a serious problem to current robotic systems aiming at long term operation under varying seasons or local weather conditions. This paper is built on our previous work where we propose to learn to predict the changes in an environment. Our key insight is that the occurring scene changes are in part systematic, repeatable and therefore predictable. The goal of our work is to support existing approaches to place recognition by learning how the visual appearance of an environment changes over time and by using this learned knowledge to predict its appearance under different environmental conditions. We describe the general idea of appearance change prediction (ACP) and investigate properties of our novel implementation based on vocabularies of superpixels (SP-ACP). Our previous work showed that the proposed approach significantly improves the performance of SeqSLAM and BRIEF-Gist for place recognition on a subset of the Nordland dataset under extremely different environmental conditions in summer and winter. This paper deepens the understanding of the proposed SP-ACP system and evaluates the influence of its parameters. We present the results of a large-scale experiment on the complete 10 h Nordland dataset and appearance change predictions between different combinations of seasons.},
author = {Neubert, Peer and S{\"{u}}nderhauf, Niko and Protzel, Peter},
doi = {10.1016/j.robot.2014.08.005},
file = {:home/nathan/Documents/Mendeley Desktop/Neubert, S{\"{u}}nderhauf, Protzel/Robotics and Autonomous Systems (RAS)/Neubert, S{\"{u}}nderhauf, Protzel{\_}2015{\_}Superpixel-based appearance change prediction for long-term navigation across seasons.pdf:pdf},
isbn = {9781479902637},
issn = {09218890},
journal = {Robotics and Autonomous Systems (RAS)},
keywords = {Appearance based localization,Appearance change prediction,Changing environments,Long term navigation,Place recognition},
number = {1},
pages = {15--27},
publisher = {Elsevier B.V.},
title = {{Superpixel-based appearance change prediction for long-term navigation across seasons}},
url = {http://dx.doi.org/10.1016/j.robot.2014.08.005},
volume = {69},
year = {2015}
}
@article{Contreras2017,
abstract = {This paper presents a study on the use of Convolutional Neural Networks for camera relocalisation and its application to map compression. We follow state of the art visual relocalisation results and evaluate response to different data inputs -- namely, depth, grayscale, RGB, spatial position and combinations of these. We use a CNN map representation and introduce the notion of CNN map compression by using a smaller CNN architecture. We evaluate our proposal in a series of publicly available datasets. This formulation allows us to improve relocalisation accuracy by increasing the number of training trajectories while maintaining a constant-size CNN.},
annote = {R{\'{e}}sum{\'{e}}
Etude du CNN PoseNet dans le but d'avoir une map compress{\'{e}} que l'on peut am{\'{e}}liorer sans ajouter de donn{\'{e}}es (la taille doit rester la m{\^{e}}me). Les auteurs utilisent une CNN plus petit (CNN-F, 8 layers compar{\'{e}} {\`{a}} 23 layers pour le PoseNet original) et compare diff{\'{e}}rents types d'entr{\'{e}}es : RGB, D, et 3D points, ainsi que leur combinaison. Ils en concluent que c'est RGB qui marche le mieux. Ils d{\'{e}}montrent {\'{e}}galement que plus on entraine le reseau avec des trajectoires vari{\'{e}}, plus on augment la pr{\'{e}}cision du r{\'{e}}seau ; ils ont donc bien une map de taille fixe que l'on peut am{\'{e}}liorer avec des exp{\'{e}}riences successive.

Ce que je n'ai pas compris
- Comment ils font passer le nuage de point ?

Ce qui est int{\'{e}}ressant
- Ils ont des r{\'{e}}sultats {\'{e}}quivalent {\`{a}} PoseNet sur un dataset alors que leur reseau est 3 fois plus petit

Critiques
- Pas de vrai nouveaut{\'{e}} ni de nouvelle contribution
- Pour mettre {\`{a}} jour la map on doit r{\'{e}}entrainer le CNN, ce qui est couteux

Computational load
Offline: CNN fine tuning
Online: gratuit

Scalability
Scale: room, city c'est vachement moins bon
Scalability potential: ?},
archivePrefix = {arXiv},
arxivId = {1703.00845},
author = {Contreras, Luis and Mayol-Cuevas, Walterio},
eprint = {1703.00845},
file = {:home/nathan/Documents/Mendeley Desktop/Contreras, Mayol-Cuevas/arXiv preprint/Contreras, Mayol-Cuevas{\_}2017{\_}Towards CNN Map Compression for camera relocalisation.pdf:pdf},
journal = {arXiv preprint},
title = {{Towards CNN Map Compression for camera relocalisation}},
url = {http://arxiv.org/abs/1703.00845},
year = {2017}
}
@inproceedings{Lin2013,
abstract = {The recent availability of large amounts of geotagged imagery has inspired a number of data driven solutions to the image geolocalization problem. Existing approaches predict the location of a query image by matching it to a database of georeferenced photographs. While there are many geotagged images available on photo sharing and street view sites, most are clustered around landmarks and urban areas. The vast majority of the Earth's land area has no ground level reference photos available, which limits the applicability of all existing image geolocalization methods. On the other hand, there is no shortage of visual and geographic data that densely covers the Earth - we examine overhead imagery and land cover survey data - but the relationship between this data and ground level query photographs is complex. In this paper, we introduce a cross-view feature translation approach to greatly extend the reach of image geolocalization methods. We can often localize a query even if it has no corresponding ground level images in the database. A key idea is to learn the relationship between ground level appearance and overhead appearance and land cover attributes from sparsely available geotagged ground-level images. We perform experiments over a 1600 km2 region containing a variety of scenes and land cover types. For each query, our algorithm produces a probability density over the region of interest. View full abstract},
author = {Lin, Tsung-Yi and Belongie, Serge and Hays, James},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2013.120},
file = {:home/nathan/Documents/Mendeley Desktop/Lin, Belongie, Hays/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Lin, Belongie, Hays{\_}2013{\_}Cross-view image geolocalization.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
pages = {891--898},
title = {{Cross-view image geolocalization}},
year = {2013}
}
@inproceedings{Stumm2015a,
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de place recognition pour les taches de loop closure et de kinapped robot en robotique. Utilisation du formalisme des BoW (SURF detector) pour cr{\'{e}}er un dico, mais en maintenant en parallele un graph de covisibilit{\'{e}} (c.a.d un graph qui a les visual word comme noeud et des edges ente les noeuds s'ils ont {\'{e}}t{\'{e}} d{\'{e}}t{\'{e}}ct{\'{e}} dans la m{\^{e}}me image). Utilisation d'une am{\'{e}}lioration du graph kernel pour faire le matching entre le graph de covisibilit{\'{e}} en requ{\^{e}}te et ceux pr{\'{e}}sent dans la database. Mise en parallele du weighting scheme tf-idf et de leur approche. Calcul probabiliste qui permet d'avoir d'obtenir un indicateur de confiance sur le match

Ce que je n'ai pas compris
- On maintient un graph par image ?
- Les proba

Ce qui est int{\'{e}}ressant
- Bon r{\'{e}}sultats compar{\'{e}} a FAB-MAP

Computational load
Offline: ?
Online: ?

Scalability
Scale: {\~{}}1-2km de parcours
Scalability potential: No},
author = {Stumm, Elena and Mei, Christopher and Lacroix, Simon},
booktitle = {Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)},
doi = {10.1177/0278364915570140},
file = {:home/nathan/Documents/Mendeley Desktop/Stumm, Mei, Lacroix/Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)/Stumm, Mei, Lacroix{\_}2015{\_}Location graphs for visual place recognition.pdf:pdf},
keywords = {bayesian inference,covisibility maps,location models,visual place recognition},
number = {May},
title = {{Location graphs for visual place recognition}},
year = {2015}
}
@article{Cascianelli2017,
author = {Cascianelli, Silvia and Costante, Gabriele and Bellocchio, Enrico and Valigi, Paolo and Fravolini, Mario L. and Ciarfuglia, Thomas A.},
doi = {10.1016/j.robot.2017.03.004},
file = {:home/nathan/Documents/Mendeley Desktop/Cascianelli et al/Robotics and Autonomous Systems (RAS)/Cascianelli et al.{\_}2017{\_}Robust visual semi-semantic loop closure detection by a covisibility graph and CNN features.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems (RAS)},
publisher = {Elsevier B.V.},
title = {{Robust visual semi-semantic loop closure detection by a covisibility graph and CNN features}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0921889016304900},
year = {2017}
}
@article{Bevilacqua2017,
abstract = {This paper presents a novel strategy to generate, from 3-D lidar measures, dense depth and reflectance images coherent with given color images. It also estimates for each pixel of the input images a visibility attribute. 3-D lidar measures carry multiple information, e.g. relative distances to the sensor (from which we can compute depths) and reflectances. When projecting a lidar point cloud onto a reference image plane, we generally obtain sparse images, due to undersampling. Moreover, lidar and image sensor positions typically differ during acquisition; therefore points belonging to objects that are hidden from the image view point might appear in the lidar images. The proposed algorithm estimates the complete depth and reflectance images, while concurrently excluding those hidden points. It consists in solving a joint (depth and reflectance) variational image inpainting problem, with an extra variable to concurrently estimate handling the selection of visible points. As regularizers, two coupled total variation terms are included to match, two by two, the depth, reflectance, and color image gradients. We compare our algorithm with other image-guided depth upsampling methods, and show that, when dealing with real data, it produces better inpainted images, by solving the visibility issue.},
author = {Bevilacqua, Marco and Aujol, Jean Fran{\c{c}}ois and Biasutti, Pierre and Br{\'{e}}dif, Mathieu and Bugeau, Aur{\'{e}}lie},
doi = {10.1016/j.isprsjprs.2017.01.005},
file = {:home/nathan/Documents/Mendeley Desktop/Bevilacqua et al/ISPRS Journal of Photogrammetry and Remote Sensing/Bevilacqua et al.{\_}2017{\_}Joint inpainting of depth and reflectance with visibility estimation.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Depth maps,Inpainting,Lidar,Point cloud,Reflectance,Total variation,Visibility},
pages = {16--32},
publisher = {International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
title = {{Joint inpainting of depth and reflectance with visibility estimation}},
url = {http://dx.doi.org/10.1016/j.isprsjprs.2017.01.005},
volume = {125},
year = {2017}
}
@article{Cummins2008,
abstract = {This paper describes a probabilistic approach to the problem of recognizing places based on their appearance. The system we present is not limited to localization, but can determine that a new observation comes from a previously unseen place, and so augment its map. Effectively this is a SLAM system in the space of appearance. Our probabilistic approach allows us to explicitly account for perceptual aliasing in the environment--identical but indistinctive observations receive a low probability of having come from the same place. We achieve this by learning a generative model of place appearance. By partitioning the learning problem into two parts, new place models can be learned online from only a single observation of a place. The algorithm complexity is linear in the number of places in the map, and is particularly suitable for online loop closure detection in mobile robotics.},
annote = {R{\'{e}}sum{\'{e}} 

M{\'{e}}thode de loop closure visuelle pour SLAM. Utilisation d'un arbre de Chow Liu pour mod{\'{e}}liser une distribution a partir de BoW. Utilisation de la probabilit{\'{e}} mod{\'{e}}liser par cette distribution pour d{\'{e}}finir si oui ou non le lieu observ{\'{e}} a d{\'{e}}j{\`{a}} {\'{e}}t{\'{e}} visit{\'{e}}. Description des images par SURF.

Ce que je n'ai pas compris

- Les proba et le Chow Lieu tree

Ce qui est int{\'{e}}ressant

- M{\'{e}}trique probabiliste pour {\'{e}}valuer le matching
- Temps de calcul faible

Critiques

- Tr{\`{e}}s orient{\'{e}} robotique SLAM
- Pas adapt{\'{e}} {\`{a}} de la grande regression de base de donn{\'{e}}es

A approfondir

- Chow Liu + Monte Carlo

Computational load
Offline: 1 ms/place
Online: 1 ms/place (max 5.9s)

Scalability
Scale: Place
Scalability potential: Faible},
author = {Cummins, Mark and Newman, Paul},
doi = {10.1177/0278364908090961},
file = {:home/nathan/Documents/Mendeley Desktop/Cummins, Newman/The International Journal of Robotics Research (IJRR)/Cummins, Newman{\_}2008{\_}FAB-MAP Probabilistic Localization and Mapping in the Space of Appearance.pdf:pdf},
isbn = {0278364908090},
issn = {0278-3649},
journal = {The International Journal of Robotics Research (IJRR)},
keywords = {Local features,RGB request,SLAM,SURF,Visual words,ap-,pearance based navigation,place recognition,topological slam},
mendeley-tags = {Local features,RGB request,SLAM,SURF,Visual words},
pages = {647--665},
title = {{FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance}},
volume = {27},
year = {2008}
}
@article{Pascoe2015,
author = {Pascoe, Geoffrey and Maddern, Will and Newman, Paul},
file = {:home/nathan/Documents/Mendeley Desktop/Pascoe, Maddern, Newman/British Machine Vision Conference (BMVC)/Pascoe, Maddern, Newman{\_}2015{\_}Robust Direct Visual Localisation using Normalised Information Distance.pdf:pdf},
journal = {British Machine Vision Conference (BMVC)},
pages = {1--13},
title = {{Robust Direct Visual Localisation using Normalised Information Distance}},
year = {2015}
}
@inproceedings{Arandjelovic2014,
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode d'image retrieval appliqu{\'{e}} {\`{a}} la place recognition. Am{\'{e}}lioration de la pipeline BoW + hamming embedding + burstiness par l'ajout d'un poid jugeant de la distinctivness d'une feature en fonction de la densit{\'{e}} de ses voisins dans l'espace des descripteurs (poid sur les mots du dico). Les auteurs insistent sur le fait que c'est rapide {\`{a}} calculer et que ca ne prend pas beaucoup de place en stoquage, voir qu'on peut r{\'{e}}duire le dico en enlevant 7 pourcents des features les moins discriminant sans baisse de performances. Ajout de tests de post-traimtent pour le rerank des candidats : un classique par geometry verification et un autre qui choisi un seul candidat correspondant {\`{a}} un certain lieu {\`{a}} chaque fois, pour augmenter la diversit{\'{e}} des images (augment le recall at N pour N{\textgreater}1)

Ce que je n'ai pas compris
- D{\'{e}}taille des calculs avec les sous-ensemble du vecteur d'hamming

Ce qui est int{\'{e}}ressant
- Se fait seulement en pr{\'{e}}-traitement
- Article claire, avec bonne {\'{e}}tat de l'art sur les m{\'{e}}thodes de BoW

Critiques
- Un peu trop orient{\'{e}} sur gain en temps de calcul

Computational load
Offline: d{\'{e}}tailli{\'{e}} dans l'article (plutot bon)
Online: idem que BoW (pas d'arbre)

Scalability
Scale: City
Scalability potential: Yes},
author = {Arandjelovi{\'{c}}, Relja and Zisserman, Andrew},
booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
file = {:home/nathan/Documents/Mendeley Desktop/Arandjelovi{\'{c}}, Zisserman/Proceedings of the Asian Conference on Computer Vision (ACCV)/Arandjelovi{\'{c}}, Zisserman{\_}2014{\_}DisLocation Scalable descriptor.pdf:pdf},
title = {{DisLocation : Scalable descriptor}},
year = {2014}
}
@article{Naseer2016,
abstract = {Lifelong autonomous operation has gained much attention in the field of mobile robotics in recent years. In the context of robot navigation based on vision, lifelong applications include scenarios with substantial perceptual changes due to changes in season, illumination and weather. In this paper, we present an approach to localize a mobile robot, equipped with a low frequency camera, with respect to an image sequence recorded in a different season. Our approach employs a discrete Bayes filter with a sensor model based on whole image descriptors. We compute a similarity matrix over all image descriptors and leverage the sequential nature of typical image streams with a flexible transition scheme in the Bayes filter framework. Since we compute a probability distribution over the entire state space, our approach can handle complex trajectories that may include same season loop-closures as well as fragmented sub-sequences. Furthermore, we show that decorrelating the similarity matrix results in an improved localization performance. Through an extensive experimental evaluation on challenging datasets we demonstrate that our approach outperforms state-of-the-art techniques.},
author = {Naseer, Tayyab and Suger, Benjamin and Ruhnke, Michael and Burgard, Wolfram},
doi = {10.1016/j.robot.2016.11.008},
file = {:home/nathan/Documents/Mendeley Desktop/Naseer et al/Robotics and Autonomous Systems (RAS)/Naseer et al.{\_}2016{\_}Vision-based Markov localization for long-term autonomy.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems (RAS)},
keywords = {lifelong visual localization,long-term autonomy,markov,perceptual changes,robot vision},
publisher = {Elsevier B.V.},
title = {{Vision-based Markov localization for long-term autonomy}},
url = {http://dx.doi.org/10.1016/j.robot.2016.11.008},
year = {2016}
}
@inproceedings{Gordo2016,
annote = {R{\'{e}}sum{\'{e}}
CNN entrain{\'{e}} pour faire du landmark retrieval en extrayant un descripteur global de l'image. Se diff{\'{e}}rencie par le fait que le r{\'{e}}seau est entrain{\'{e}} sp{\'{e}}cifiquement pour la tache d'image retrieval en utilisant un r{\'{e}}seau siamois (3 r{\'{e}}seaux en parall{\`{e}}le : un avec la query, un autre avec une image ressemblante et un dernier avec un hard negative example, les trois r{\'{e}}seaux sont li{\'{e}} par leur poid et entrain{\'{e}} {\`{a}} l'aide d'une triplet loss function). En amont de se r{\'{e}}seau on retrouve un autre r{\'{e}}seau qui permet d'extraite de l'image requ{\^{e}}te des zones d'int{\'{e}}rets (RPN, Region Proposal Network). Les auteurs insite aussi sur le fait qu"ils ont besoin d'une grosse base de donn{\'{e}}e "propre", donc il propose une m{\'{e}}thode pour suprimer les images non significatives et ressentrer les ROI sur des bonnes zones d'int{\'{e}}r{\^{e}}ts. Ensuite matching des descripteurs par produit scalaire.
Pour la m{\'{e}}thode d'aggregation, voir -{\textgreater} "Particular object retrieval with integral max-pooling of CNN activations"

Ce que je n'ai pas compris
- Comment on compare des similitudes par produit scalaire ? (peut {\^{e}}tre grace {\`{a}} l'architecture du r{\'{e}}seau ?)

Ce qui est int{\'{e}}ressant
- Les r{\'{e}}sultats/temps de calcul online
- Le nettoyage des bases d'entrainement

Critiques
- Plutot orient{\'{e}} landmark detection
- Marche un tout petit peu moins bien que les m{\'{e}}thodes orient{\'{e}}es descripteurs locaux et consistence g{\'{e}}om{\'{e}}trique

A approfondir


Computational load
Offline: 6 days
Online: {\textless}1ms (but they said 5Hz with a 5k€ GPU to extract the descriptor, so ?)

Scalability
Scale: City
Scalability potential: High},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Gordo, Albert and Alm{\'{a}}zan, Jon and Revaud, Jerome and Larlus, Diane},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-319-46448-0},
eprint = {1311.2901},
file = {:home/nathan/Documents/Mendeley Desktop/Gordo et al/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Gordo et al.{\_}2016{\_}Deep Image Retrieval Learning Global Representations for Image Search.pdf:pdf},
isbn = {978-3-319-46447-3},
issn = {0302-9743},
keywords = {deep learning,instance-level retrieval},
pages = {241--257},
title = {{Deep Image Retrieval: Learning Global Representations for Image Search}},
url = {http://link.springer.com/10.1007/978-3-319-46448-0},
volume = {9905},
year = {2016}
}
@inproceedings{Corke2013,
abstract = {In outdoor environments shadows are common. These typically strong visual features cause considerable change in the appearance of a place, and therefore confound visionbased localisation approaches. In this paper we describe how to convert a colour image of the scene to a greyscale invariant image where pixel values are a function of underlying material property not lighting. We summarise the theory of shadow invariant images and discuss the modelling and calibration issues which are important for non-ideal off-the-shelf colour cameras. We evaluate the technique with a commonly used robotic camera and an autonomous car operating in an outdoor environment, and show that it can outperform the use of ordinary greyscale images for the task of visual localisation.},
annote = {R{\'{e}}sum{\'{e}}
Application d'une m{\'{e}}thode de supression des ombres sur une image pour la localisation bas{\'{e}}e vision. Les auteurs transforment l'image d'origine (RGB) en une image {\`{a}} deux canaux en projetant la couleur du pixel dans un espace qui prend en compte la r{\'{e}}flection des mat{\'{e}}riaux et supprime l'ombre. Les auteurs adaptent une m{\'{e}}thode existante {\`{a}} une cam{\'{e}}ra r{\'{e}}elle et corrigent les d{\'{e}}fauts en r{\'{e}}duisant l'overlapping des r{\'{e}}ponses spectrales du capteur. Pour la localisation les auteurs utilisent une m{\'{e}}thode de correlation ZNCC (zero-mean normalised cross correlation).

Ce qui est int{\'{e}}ressant
- Les auteurs disent que {\c{c}}a am{\'{e}}liore et qu'on peut toujours calculer des points d'int{\'{e}}rets
- Rapide {\`{a}} calculer

Critiques
- On doit connaitre les spec du capteur

A approfondir
- M{\'{e}}thode des autres auteurs (ref [2])

Computational load
Offline: -
Online: 40 ms (1 mpex image)

Scalability
Scale: -
Scalability potential: -},
author = {Corke, Peter and Paul, Rohan and Churchill, Winston and Newman, Paul},
booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2013.6696648},
file = {:home/nathan/Documents/Mendeley Desktop/Corke et al/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Corke et al.{\_}2013{\_}Dealing with shadows Capturing intrinsic scene appearance for image-based outdoor localisation.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
pages = {2085--2092},
title = {{Dealing with shadows: Capturing intrinsic scene appearance for image-based outdoor localisation}},
year = {2013}
}
@inproceedings{Zhou2016a,
author = {Zhou, Dan and Li, Xue and Zhang, Yu-Jin},
booktitle = {Proceedings of the IEEE International Conference on Image Processing (ICIP)},
file = {:home/nathan/Documents/Mendeley Desktop/Zhou, Li, Zhang/Proceedings of the IEEE International Conference on Image Processing (ICIP)/Zhou, Li, Zhang{\_}2016{\_}A Novel CNN-Based Match Kernel for Image Retrieval.pdf:pdf},
title = {{A Novel CNN-Based Match Kernel for Image Retrieval}},
year = {2016}
}
@article{Arandjelovic2013,
abstract = {The objective of this paper is large scale object instance retrieval, given a query image. A starting point of such systems is feature detection and description, for example using SIFT. The focus of this paper, however, is towards very large scale retrieval where, due to storage requirements, very compact image descriptors are required and no information about the original SIFT descriptors can be accessed directly at run time. We start from VLAD, the state-of-the art compact descriptor introduced by Jegou et al. for this purpose, and make three novel contributions: first, we show that a simple change to the normalization method significantly improves retrieval performance, second, we show that vocabulary adaptation can substantially alleviate problems caused when images are added to the dataset after initial vocabulary learning. These two methods set a new state-of-the-art over all benchmarks investigated here for both mid-dimensional (20k-D to 30k-D) and small (128-D) descriptors. Our third contribution is a multiple spatial VLAD representation, MultiVLAD, that allows the retrieval and localization of objects that only extend over a small part of an image (again without requiring use of the original image SIFT descriptors). View full abstract},
author = {Arandjelovi{\'{c}}, Relja and Zisserman, Andrew},
doi = {10.1109/CVPR.2013.207},
file = {:home/nathan/Documents/Mendeley Desktop/Arandjelovi{\'{c}}, Zisserman/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Arandjelovi{\'{c}}, Zisserman{\_}2013{\_}All About VLAD.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {1578--1585},
title = {{All About VLAD}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619051{\%}5Cnpapers3://publication/doi/10.1109/CVPR.2013.207{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619051},
year = {2013}
}
@inproceedings{Lezama2014,
author = {Lezama, Jos{\'{e}} and {Von Gioi}, Rafael and Randall, Gregory and Morel, Jean-Michel},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {509--515},
title = {{Finding vanishing points via point alignments in image primal and dual domains}},
year = {2014}
}
@article{Kang2017,
author = {Kang, Cuicui and Liao, Shengcai and Li, Zhen and Cao, Zigang and Xiong, Gang},
file = {:home/nathan/Documents/Mendeley Desktop/Kang et al/Unknown/Kang et al.{\_}2017{\_}Learning Deep Semantic Embeddings for Cross-Modal Retrieval.pdf:pdf},
keywords = {cross-modal retrieval,deep learning,semantic embedding learning},
number = {2014},
pages = {1--16},
title = {{Learning Deep Semantic Embeddings for Cross-Modal Retrieval}},
year = {2017}
}
@article{Wan2014a,
abstract = {Learning effective feature representations and similarity measures are crucial to the retrieval performance of a content-based image retrieval (CBIR) system. Despite extensive research efforts for decades, it remains one of the most challenging open problems that considerably hinders the successes of real-world CBIR sys- tems. The key challenge has been attributed to the well-known “se- mantic gap” issue that exists between low-level image pixels cap- tured by machines and high-level semantic concepts perceived by human. Among various techniques, machine learning has been ac- tively investigated as a possible direction to bridge the semantic gap in the long term. Inspired by recent successes of deep learning tech- niques for computer vision and other applications, in this paper, we attempt to address an open problem: if deep learning is a hope for bridging the semantic gap in CBIR and howmuch improvements in CBIR tasks can be achieved by exploring the state-of-the-art deep learning techniques for learning feature representations and simi- larity measures. Specifically, we investigate a framework of deep learningwith application toCBIRtaskswith an extensive set of em- pirical studies by examining a state-of-the-art deep learningmethod (Convolutional Neural Networks) for CBIR tasks under varied set- tings. From our empirical studies, we find some encouraging re- sults and summarize some important insights for future research. Categories},
author = {Wan, Ji and Wang, Dayong and Hoi, Steven Chu Hong and Wu, Pengcheng and Zhu, Jianke and Zhang, Yongdong and Li, Jintao},
doi = {10.1145/2647868.2654948},
file = {:home/nathan/Documents/Mendeley Desktop/Wan et al/Proceedings of the ACM International Conference on Multimedia (MM)/Wan et al.{\_}2014{\_}Deep Learning for Content-Based Image Retrieval.pdf:pdf},
isbn = {9781450330633},
journal = {Proceedings of the ACM International Conference on Multimedia (MM)},
pages = {157--166},
title = {{Deep Learning for Content-Based Image Retrieval}},
url = {http://dl.acm.org/citation.cfm?doid=2647868.2654948},
year = {2014}
}
@article{Li2017,
author = {Li, Ruihao and Liu, Qiang and Gui, Jianjun and Gu, Dongbing and Hu, Huosheng},
file = {:home/nathan/Documents/Mendeley Desktop/Li et al/IEEE Transactions on Automation Science and Engineering/Li et al.{\_}2017{\_}Indoor Relocalization in Challenging Environments With Dual-Stream Convolutional Neural Networks.pdf:pdf},
journal = {IEEE Transactions on Automation Science and Engineering},
pages = {1--12},
title = {{Indoor Relocalization in Challenging Environments With Dual-Stream Convolutional Neural Networks}},
year = {2017}
}
@article{Mishkin2017,
abstract = {We present an accurate method for estimation of the affine shape of local features. The method is trained in a novel way, exploiting the recently proposed HardNet triplet loss. The loss function is driven by patch descriptor differences, avoiding problems with symmetries. Moreover, such training process does not require precisely geometrically aligned patches. The affine shape is represented in a way amenable to learning by stochastic gradient descent. When plugged into a state-of-the-art wide baseline matching algorithm, the performance on standard datasets improves in both the number of challenging pairs matched and the number of inliers. Finally, AffNet with combination of Hessian detector and HardNet descriptor improves bag-of-visual-words based state of the art on Oxford5k and Paris6k by large margin, 4.5 and 4.2 mAP points respectively. The source code and trained networks are available at https://github.com/ducha-aiki/affnet},
archivePrefix = {arXiv},
arxivId = {1711.06704},
author = {Mishkin, Dmytro and Radenovic, Filip and Matas, Jiri},
eprint = {1711.06704},
file = {:home/nathan/Documents/Mendeley Desktop/Mishkin, Radenovic, Matas/Unknown/Mishkin, Radenovic, Matas{\_}2017{\_}Learning Discriminative Affine Regions via Discriminability.pdf:pdf},
title = {{Learning Discriminative Affine Regions via Discriminability}},
url = {http://arxiv.org/abs/1711.06704},
year = {2017}
}
@article{Fuse2015,
abstract = {Recently, development of high performance CPU, cameras and other sensors on mobile devices have been used for wide variety of applications. Most of the applications require self-localization of the mobile device. Since the self-localization is based on GPS, gyro sensor, acceleration meter and magnetic field sensor (called as POS) of low accuracy, the applications are limited. On the other hand, self-localization method using images have been developed, and the accuracy of the method is increasing. This paper develops the self-localization method by integrating sensors, such as POS and cameras, on mobile devices simultaneously. The proposed method mainly consists of two parts: one is the accuracy improvement of POS data filtering, and another is development of self-localization method by integrating POS and camera. The POS data filtering combines all POS data by using Kalman filter in order to improve the accuracy of exterior orientation factors. The exterior orientation factors with POS filtering are used as initial value of ones in image-based self-localization method. The image-based self-localization method consists of feature points extraction and tracking, relative orientation, coordinates estimation of the feature points, and orientation factors updates of the mobile device. The proposed method is applied to POS data and images taken in urban area. Through experiments with real data, the accuracy improvement by POS data filtering is confirmed. The proposed self-localization method with POS and camera make the accuracy more sophisticated by comparing with only POS data filtering.},
author = {Fuse, T. and Matsumoto, K.},
doi = {10.5194/isprsarchives-XL-4-W5-87-2015},
file = {:home/nathan/Documents/Mendeley Desktop/Fuse, Matsumoto/The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences/Fuse, Matsumoto{\_}2015{\_}Self-Localization Method By Integrating Sensors.pdf:pdf},
issn = {2194-9034},
journal = {The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {augmented reality,mobile devices,navigation,self-localization,sensor fusion,visualization},
number = {May},
pages = {87--92},
title = {{Self-Localization Method By Integrating Sensors}},
url = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XL-4-W5/87/2015/},
volume = {XL-4/W5},
year = {2015}
}
@inproceedings{Carlevaris-Bianco2014,
abstract = {In many robotic applications, especially long-term outdoor deployments,$\backslash$nthe success or failure of feature-based image registration is largely$\backslash$ndetermined by changes in lighting. This paper reports on a method$\backslash$nto learn visual feature point descriptors that are more robust to$\backslash$nchanges in scene lighting than standard hand-designed features. We$\backslash$ndemonstrate that, by tracking feature points in time-lapse videos,$\backslash$none can easily generate training data that captures how the visual$\backslash$nappearance of interest points changes with lighting over time. This$\backslash$ntraining data is used to learn feature descriptors that map the image$\backslash$npatches associated with feature points to a lower-dimensional feature$\backslash$nspace where Euclidean distance provides good discrimination between$\backslash$nmatching and non-matching image patches. Results showing that the$\backslash$nlearned descriptors increase the ability to register images under$\backslash$nvarying lighting conditions are presented for a challenging indoor-outdoor$\backslash$ndataset spanning 27 mapping sessions over a period of 15 months,$\backslash$ncontaining a wide variety of lighting changes.},
annote = {Descripteur CNN locaux pour le mathcing avec des illuminations diff{\'{e}}rentes},
author = {Carlevaris-Bianco, Nicholas and Eustice, Ryan M.},
booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2014.6942941},
file = {:home/nathan/Documents/Mendeley Desktop/Carlevaris-Bianco, Eustice/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Carlevaris-Bianco, Eustice{\_}2014{\_}Learning visual feature descriptors for dynamic lighting conditions.pdf:pdf},
isbn = {9781479969340},
issn = {21530866},
pages = {2769--2776},
title = {{Learning visual feature descriptors for dynamic lighting conditions}},
year = {2014}
}
@article{Chen2013,
abstract = {The goal of object retrieval is to rank a set of images by the similarity of their contents to those of a query image. However, it is difficult to measure image content similarity due to visual changes caused by varying viewpoint and environment. In this paper, we propose a simple, efficient method to more effectively measure content similarity from image measurements. Our method is based on the ranking information available from existing retrieval systems. We observe that images within the set which, when used as queries, yield similar ranking lists are likely to be relevant to each other and vice versa. In our method, ranking consistency is used as a verification method to efficiently refine an existing ranking list, in much the same fashion that spatial verification is employed. The efficiency of our method is achieved by a list-wise min-Hash scheme, which allows rapid calculation of an approximate similarity ranking. Experimental results demonstrate the effectiveness of the proposed framework and its applications. ?? 2013 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Chen, Yanzhi and Li, Xi and Dick, Anthony and Hill, Rhys},
doi = {10.1016/j.patcog.2013.09.011},
eprint = {arXiv:1011.1669v3},
file = {:home/nathan/Documents/Mendeley Desktop/Chen et al/Pattern Recognition/Chen et al.{\_}2013{\_}Ranking consistency for image matching and object retrieval.pdf:pdf},
isbn = {9788578110796},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Image matching,List-wise min-Hash,Object retrieval,Ranking consistency},
number = {3},
pages = {1349--1360},
pmid = {25246403},
title = {{Ranking consistency for image matching and object retrieval}},
volume = {47},
year = {2013}
}
@article{He2017,
author = {He, Jianfeng and Ma, Bingpeng and Wang, Shuhui and Liu, Yugui and Huang, Qingming},
doi = {10.1016/j.neucom.2017.10.032},
file = {:home/nathan/Documents/Mendeley Desktop/He et al/Neurocomputing/He et al.{\_}2017{\_}Multi-label Double-layer Learning for Cross-Modal Retrieval.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Cross-modal retrieval,Multi-label,Multimedia,Parti,cross-modal retrieval},
pages = {1--10},
publisher = {Elsevier B.V.},
title = {{Multi-label Double-layer Learning for Cross-Modal Retrieval}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231217316831},
volume = {0},
year = {2017}
}
@article{Kanai2015,
abstract = {Effective and accurate localization method in three-dimensional indoor environments is a key requirement for indoor navigation and lifelong robotic assistance. So far, Monte Carlo Localization (MCL) has given one of the promising solutions for the indoor localization methods. Previous work of MCL has been mostly limited to 2D motion estimation in a planar map, and a few 3D MCL approaches have been recently proposed. However, their localization accuracy and efficiency still remain at an unsatisfactory level (a few hundreds millimetre error at up to a few FPS) or is not fully verified with the precise ground truth. Therefore, the purpose of this study is to improve an accuracy and efficiency of 6DOF motion estimation in 3D MCL for indoor localization. Firstly, a terrestrial laser scanner is used for creating a precise 3D mesh model as an environment map, and a professional-level depth camera is installed as an outer sensor. GPU scene simulation is also introduced to upgrade the speed of prediction phase in MCL. Moreover, for further improvement, GPGPU programming is implemented to realize further speed up of the likelihood estimation phase, and anisotropic particle propagation is introduced into MCL based on the observations from an inertia sensor. Improvements in the localization accuracy and efficiency are verified by the comparison with a previous MCL method. As a result, it was confirmed that GPGPU-based algorithm was effective in increasing the computational efficiency to 10-50 FPS when the number of particles remain below a few hundreds. On the other hand, inertia sensor-based algorithm reduced the localization error to a median of 47mm even with less number of particles. The results showed that our proposed 3D MCL method outperforms the previous one in accuracy and efficiency.},
author = {Kanai, S. and Hatakeyama, R. and Date, H.},
doi = {10.5194/isprsarchives-XL-4-W5-61-2015},
file = {:home/nathan/Documents/Mendeley Desktop/Kanai, Hatakeyama, Date/The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences/Kanai, Hatakeyama, Date{\_}2015{\_}Improvement of 3D Monte Carlo Localization Using a Depth Camera and Terrestrial Laser Scanner.pdf:pdf},
issn = {2194-9034},
journal = {The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {depth camera,gpgpu,imu,monte carlo localization,scene simulation,terrestrial laser scanner},
number = {May},
pages = {61--66},
title = {{Improvement of 3D Monte Carlo Localization Using a Depth Camera and Terrestrial Laser Scanner}},
url = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XL-4-W5/61/2015/},
volume = {XL-4/W5},
year = {2015}
}
@article{Budikova2017,
author = {Budikova, Petra and Batko, Michal and Zezula, Pavel},
doi = {10.1007/978-3-662-53416-8},
file = {:home/nathan/Documents/Mendeley Desktop/Budikova, Batko, Zezula/Transactions on Large-Scale Data-and Knowledge-Centered Systems/Budikova, Batko, Zezula{\_}2017{\_}Fusion Strategies for Large-Scale Multi-modal Image Retrieval.pdf:pdf},
isbn = {978-3-662-53415-1},
journal = {Transactions on Large-Scale Data-and Knowledge-Centered Systems},
pages = {146--184},
title = {{Fusion Strategies for Large-Scale Multi-modal Image Retrieval}},
url = {http://link.springer.com/10.1007/978-3-662-53416-8},
volume = {9860},
year = {2017}
}
@article{Razavian2014,
abstract = {Recent results indicate that the generic descriptors ex- tracted from the convolutional neural networks are very powerful [ 10 , 29 , 48 ]. This paper adds to the mount- ing evidence that this is indeed the case. We report on a series of experiments conducted for different recogni- tion tasks using the publicly available code and model of the OverFeat network which was trained to perform ob- ject classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image represen- tation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the orig- inal task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L 2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representa- tions are further modified using simple augmentation tech- niques e.g. jittering. The results strongly suggest that fea- tures obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
archivePrefix = {arXiv},
arxivId = {1403.6382},
author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan and Sharif, Ali and Hossein, Razavian and Josephine, Azizpour and Stefan, Sullivan and Royal, K T H},
doi = {10.1109/CVPRW.2014.131},
eprint = {1403.6382},
file = {:home/nathan/Documents/Mendeley Desktop/Razavian et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)/Razavian et al.{\_}2014{\_}CNN Features off-the-shelf an Astounding Baseline for Recognition.pdf:pdf},
isbn = {9781479943081},
issn = {21607516},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
keywords = {CNN features,feature description,overfeat},
pages = {512--519},
pmid = {87882338},
title = {{CNN Features off-the-shelf : an Astounding Baseline for Recognition}},
url = {http://arxiv.org/abs/1403.6382},
year = {2014}
}
@article{Scovanner2007,
abstract = {In this paper we introduce a 3-dimensional (3D) SIFT descriptor for video or 3D imagery such as MRI data. We also show how this new descriptor is able to better represent the 3D nature of video data in the application of action recognition. This paper will show how 3D SIFT is able to outperform previously used description methods in an elegant and efficient manner. We use a bag of words approach to represent videos, and present a method to discover relationships between spatio-temporal words in order to better describe the video data.},
author = {Scovanner, Paul and Ali, Saad and Shah, Mubarak},
doi = {10.1145/1291233.1291311},
file = {:home/nathan/Documents/Mendeley Desktop/Scovanner, Ali, Shah/Proceedings of the ACM International Conference on Multimedia (MM)/Scovanner, Ali, Shah{\_}2007{\_}A 3-dimensional sift descriptor and its application to action recognition.pdf:pdf},
isbn = {9781595937025},
issn = {14764687},
journal = {Proceedings of the ACM International Conference on Multimedia (MM)},
number = {c},
pages = {357},
pmid = {17994073},
title = {{A 3-dimensional sift descriptor and its application to action recognition}},
url = {http://dl.acm.org/citation.cfm?id=1291311{\%}5Cnhttp://portal.acm.org/citation.cfm?doid=1291233.1291311},
year = {2007}
}
@article{Paparoditis2012,
author = {Paparoditis, Nicolas and Papelard, Jean-Pierre and Cannelle, Bertrand and Devaux, Alexandre and Soheilian, Bahman and David, Nicolas and Houzay, Erwann},
journal = {Revue fran{\c{c}}aise de photogramm{\'{e}}trie et de t{\'{e}}l{\'{e}}d{\'{e}}tection},
number = {1},
pages = {69--79},
title = {{Stereopolis II: A multi-purpose and multi-sensor 3D mobile mapping system for street visualisation and 3D metrology}},
volume = {200},
year = {2012}
}
@inproceedings{Armagan2017a,
author = {Armagan, Anil and Hirzer, Martin and Lepetit, Vincent},
booktitle = {Joint Urban Remote Sensing Event (JURSE)},
file = {:home/nathan/Documents/Mendeley Desktop/Armagan, Hirzer, Lepetit/Joint Urban Remote Sensing Event (JURSE)/Armagan, Hirzer, Lepetit{\_}2017{\_}Semantic Segmentation for 3D Localization in Urban Environments.pdf:pdf},
isbn = {9781509058082},
pages = {3--6},
title = {{Semantic Segmentation for 3D Localization in Urban Environments}},
year = {2017}
}
@inproceedings{Feng2016,
abstract = {Dynamic relocalization of 6D camera pose from single reference image is a costly and challenging task that re-quires delicate hand-eye calibration and precision position-ing platform to do 3D mechanical rotation and translation. In this paper, we show that high-quality camera relocaliza-tion can be achieved in a much less expensive way. Based on inexpensive platform with unreliable absolute reposi-tioning accuracy (ARA), we propose a hand-eye calibra-tion free strategy to actively relocate camera into the same 6D pose that produces the input reference image, by se-quentially correcting 3D relative rotation and translation. We theoretically prove that, by this strategy, both rotational and translational relative pose can be effectively reduced to zero, with bounded unknown hand-eye pose displacement. To conquer 3D rotation and translation ambiguity, this the-oretical strategy is further revised to a practical relocaliza-tion algorithm with faster convergence rate and more reli-ability by jointly adjusting 3D relative rotation and trans-lation. Extensive experiments validate the effectiveness and superior accuracy of the proposed approach on laboratory tests and challenging real-world applications.},
annote = {R{\'{e}}sum{\'{e}}

Algorithme de recallage dynamique pr{\'{e}}cis sans calibration d'une image par rapport {\`{a}} une image de r{\'{e}}f{\'{e}}rence bas{\'{e}} sur des m{\'{e}}thodes de type local feature. Preuve de convergence de la m{\'{e}}thode dans un cas d'utilisation pr{\'{e}}cis. Plut{\^{o}}t orient{\'{e}} robotique.

Ce que je n'ai pas compris

- La preuve de convergence
- Pas de calibration possible ? ({\`{a}} moins d'avoir la position pr{\'{e}}cise de la cam{\'{e}}ra au moment de la prise de vue initiale)
- Notation SO(3) comme espace math{\'{e}}matique li{\'{e}} aux rotations

Ce qui est int{\'{e}}ressant

- M{\'{e}}thode "light {\&} rapide" (quand m{\^{e}}me utilisation de matching de points d'interets...)
- Possibilit{\'{e}} de faire des panoramas
- D{\'{e}}tection de changements tr{\`{e}}s faible entre les images

Critiques

- Les images doivent {\^{e}}tre tr{\`{e}}s similaire (prise par une m{\^{e}}me cam{\'{e}}ra) 
- Plus adapt{\'{e}} au monde de la robotique avec la possibilit{\'{e}} de faire bouger pr{\'{e}}cis{\'{e}}ment l'image {\`{a}} recaller
- Info RGB seules
- R{\'{e}}sultats th{\'{e}}orique des comparaisons un peu fauss{\'{e}} {\'{e}}tant donn{\'{e}} qu'il n'y a pas vraiment de syst{\`{e}}mes similaires

A approfondir

Les autres m{\'{e}}thodes utilis{\'{e}} pour comparer leurs r{\'{e}}sulats [7] {\&} [33]

Computational load
Offline: -
Online: -

Scalability
Scale: -
Scalability potential: -},
author = {Feng, Wei and Tian, Fei-Peng and Zhang, Qian and Sun, Jizhou},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Feng et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Feng et al.{\_}2016{\_}6D Dynamic Camera Relocalization from Single Reference Image.pdf:pdf},
keywords = {5 points algorithm,Alignement,Indoor,Local features,Outdoor,RGB request,Robotic,SIFT},
mendeley-tags = {5 points algorithm,Alignement,Indoor,Local features,Outdoor,RGB request,Robotic,SIFT},
title = {{6D Dynamic Camera Relocalization from Single Reference Image}},
year = {2016}
}
@inproceedings{Wu2013,
author = {Wu, Changchang},
booktitle = {International Conference on 3D Vision (3DV)},
organization = {IEEE},
pages = {127--134},
title = {{Towards linear-time incremental structure from motion}},
year = {2013}
}
@inproceedings{Tzeng2015,
abstract = {Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias. Fine-tuning deep models in a new domain can require a significant amount of labeled data, which for many applications is simply not available. We propose a new CNN architecture to exploit unlabeled and sparsely labeled target domain data. Our approach simulta-neously optimizes for domain invariance to facilitate domain transfer and uses a soft label distribution matching loss to transfer information between tasks. Our proposed adapta-tion method offers empirical performance which exceeds previously published results on two standard benchmark vi-sual domain adaptation tasks, evaluated across supervised and semi-supervised adaptation settings.},
archivePrefix = {arXiv},
arxivId = {arXiv:1510.02192v1},
author = {Tzeng, Eric and Hoffman, Judy and Darrell, Trevor and Saenko, Kate and Lowell, Umass},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
eprint = {arXiv:1510.02192v1},
file = {:home/nathan/Documents/Mendeley Desktop/Tzeng et al/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Tzeng et al.{\_}2015{\_}Simultaneous Deep Transfer Across Domains and Tasks.pdf:pdf},
title = {{Simultaneous Deep Transfer Across Domains and Tasks}},
url = {https://arxiv.org/pdf/1510.02192.pdf},
year = {2015}
}
@article{Li2016a,
abstract = {In this work, we present a Convolutional Neural Network(CNN) with depth images as its inputs to solve the relocalization problem of a moving platform in night-time indoor environment. The developed algorithm can estimate the camera pose in an end-to-end manner with 0.40m and 7.49° errors in real time during night. It does not require any geometric computation as it directly uses a CNN for 6 DOFs pose regression. The architecture and its encoding methods of depth images are discussed. The proposed method is also evaluated on benchmark datasets collected from a motion capture system in our lab.},
author = {Li, Ruihao and Liu, Qiang and Gui, Jianjun and Gu, Dongbing and Hu, Huosheng},
doi = {10.1109/IConAC.2016.7604929},
file = {:home/nathan/Documents/Mendeley Desktop/Li et al/Proceedings of the IEEE International Conference on Automation and Computing (ICAC)/Li et al.{\_}2016{\_}Night-time indoor relocalization using depth image with Convolutional Neural Networks.pdf:pdf},
isbn = {9781862181311},
journal = {Proceedings of the IEEE International Conference on Automation and Computing (ICAC)},
pages = {261--266},
title = {{Night-time indoor relocalization using depth image with Convolutional Neural Networks}},
year = {2016}
}
@article{Liu2017,
author = {Liu, Peizhong and Guo, Jing-Ming and Wu, Chi-Yi and Cai, Danlin},
doi = {10.1109/TIP.2017.2736343},
file = {:home/nathan/Documents/Mendeley Desktop/Liu et al/IEEE Transactions on Image Processing (ToIP)/Liu et al.{\_}2017{\_}Fusion of Deep Learning and Compressed Domain features for Content Based Image Retrieval.pdf:pdf},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing (ToIP)},
keywords = {block truncation coding,content-based image retrieval,convolutional-neural network,deep learning},
number = {c},
pages = {1--1},
title = {{Fusion of Deep Learning and Compressed Domain features for Content Based Image Retrieval}},
url = {http://ieeexplore.ieee.org/document/8019823/},
volume = {7149},
year = {2017}
}
@inproceedings{Paul2010,
abstract = {This paper describes a probabilistic framework for appearance based navigation and mapping using spatial and visual appearance data. Like much recent work on appearance based navigation we adopt a bag-of-words approach in which positive or negative observations of visual words in a scene are used to discriminate between already visited and new places. In this paper we add an important extra dimension to the approach. We explicitly model the spatial distribution of visual words as a random graph in which nodes are visual words and edges are distributions over distances. Care is taken to ensure that the spatial model is able to capture the multi-modal distributions of inter-word spacing and account for sensor errors both in word detection and distances. Crucially, these inter-word distances are viewpoint invariant and collectively constitute strong place signatures and hence the impact of using both spatial and visual appearance is marked. We provide results illustrating a tremendous increase in precision-recall area compared to a state-of-the-art visual appearance only systems.},
author = {Paul, Rohan and Newman, Paul},
booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ROBOT.2010.5509587},
file = {:home/nathan/Documents/Mendeley Desktop/Paul, Newman/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Paul, Newman{\_}2010{\_}FAB-MAP 3D Topological mapping with spatial and visual appearance.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
pages = {2649--2656},
title = {{FAB-MAP 3D: Topological mapping with spatial and visual appearance}},
year = {2010}
}
@inproceedings{Zhou2016,
abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable local-ization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1{\%} top-5 error for object localization on ILSVRC 2014, which is re-markably close to the 34.2{\%} top-5 error achieved by a fully supervised CNN approach. We demonstrate that our net-work is able to localize the discriminative image regions on a variety of tasks despite not being trained for them.},
annote = {From Duplicate 1 (Learning Deep Features for Discriminative Localization - Zhou, Bolei; Khosla, Aditya; Lapedriza, Agata; Oliva, Aude; Torralba, Antonio)

R{\'{e}}sum{\'{e}}

M{\'{e}}thode permettant a partir d'un CNN entrain{\'{e}} pour la classification d'objet de localiser dans l'image l'objet qui est classifi{\'{e}}, sans donn{\'{e}}e a priori de sa postion.

Ce que je n'ai pas compris

- global max pooling vs global average pooling
- IoU criterion
- hard negative mining
- qu'est ce qu'ils appellent un "fully connected layer"


Ce qui est int{\'{e}}ressant

- c'est de l'apprentissage non supervi{\'{e}}

Critiques

- c'est pas vraiment adapt{\'{e}} {\`{a}} notre probl{\'{e}}matique {\'{e}}tant donn{\'{e}}e que c'est de la localisation dans une image, et non de la localisation "spatial"
- pas de donn{\'{e}}es sur les temps de calculs et d'apprentissage

A approfondir

- comment adapter cet m{\'{e}}thode {\`{a}} notre probl{\'{e}}matique
- utiliser le fait que l'on peut visualiser ce qui interesse le r{\'{e}}seau dans une image pour s'en resservire pour "compresser" les donn{\'{e}}es par exemple, ou par se focaliser sur des zones qui vont {\^{e}}tre plus discriminantes pour d'autre tache de localistion (cf. PoseNet)

Computational load
Offline:
Online:

Scalability
Scale:
Scalability potential:

From Duplicate 2 (Learning Deep Features for Discriminative Localization - Zhou, Bolei; Khosla, Aditya; Lapedriza, Agata; Oliva, Aude; Torralba, Antonio)

R{\{}{\'{e}}{\}}sum{\{}{\'{e}}{\}}

M{\{}{\'{e}}{\}}thode permettant a partir d'un CNN entrain{\{}{\'{e}}{\}} pour la classification d'objet de localiser dans l'image l'objet qui est classifi{\{}{\'{e}}{\}}, sans donn{\{}{\'{e}}{\}}e a priori de sa postion.

Ce que je n'ai pas compris

- global max pooling vs global average pooling
- IoU criterion
- hard negative mining
- qu'est ce qu'ils appellent un "fully connected layer"


Ce qui est int{\{}{\'{e}}{\}}ressant

- c'est de l'apprentissage non supervi{\{}{\'{e}}{\}}

Critiques

- c'est pas vraiment adapt{\{}{\'{e}}{\}} {\{}{\`{a}}{\}} notre probl{\{}{\'{e}}{\}}matique {\{}{\'{e}}{\}}tant donn{\{}{\'{e}}{\}}e que c'est de la localisation dans une image, et non de la localisation "spatial"
- pas de donn{\{}{\'{e}}{\}}es sur les temps de calculs et d'apprentissage

A approfondir

- comment adapter cet m{\{}{\'{e}}{\}}thode {\{}{\`{a}}{\}} notre probl{\{}{\'{e}}{\}}matique
- utiliser le fait que l'on peut visualiser ce qui interesse le r{\{}{\'{e}}{\}}seau dans une image pour s'en resservire pour "compresser" les donn{\{}{\'{e}}{\}}es par exemple, ou par se focaliser sur des zones qui vont {\{}{\^{e}}{\}}tre plus discriminantes pour d'autre tache de localistion (cf. PoseNet)},
author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {1512.04150},
file = {:home/nathan/Documents/Mendeley Desktop/Zhou et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Zhou et al.{\_}2016{\_}Learning Deep Features for Discriminative Localization.pdf:pdf},
keywords = {CNN,Data-driven,Machine learning,RGB request,SVM,Unsupervised learning},
mendeley-tags = {CNN,Data-driven,Machine learning,RGB request,SVM,Unsupervised learning},
title = {{Learning Deep Features for Discriminative Localization}},
year = {2016}
}
@article{Zhou2017b,
author = {Zhou, Lei and Zhu, Siyu and Shen, Tianwei and Wang, Jinglu},
file = {:home/nathan/Documents/Mendeley Desktop/Zhou et al/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Zhou et al.{\_}2017{\_}Progressive Large Scale-Invariant Image Matching In Scale Space.pdf:pdf},
journal = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
pages = {2362--2371},
title = {{Progressive Large Scale-Invariant Image Matching In Scale Space}},
year = {2017}
}
@inproceedings{Chu2014,
abstract = {A framework is presented for refining GPS location and estimate the camera orientation using a single urban building image, a 2D city map with building outlines, given a noisy GPS location. We propose to use tilt-invariant vertical building corner edges extracted from the building image. A location-orientation hypothesis, which we call an LOH, is a proposed map location from which an image of building corners would occur at the observed positions of corner edges in the photo. The noisy GPS location is refined and orientation is estimated using the computed LOHs. Experiments show the framework improves GPS accuracy significantly, generally produces reliable orientation estimation, and is computationally efficient.},
author = {Chu, Hang and Gallagher, Andrew and Chen, Tsuhan},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2014.31},
file = {:home/nathan/Documents/Mendeley Desktop/Chu, Gallagher, Chen/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)/Chu, Gallagher, Chen{\_}2014{\_}GPS refinement and camera orientation estimation from a single image and a 2D map.pdf:pdf},
isbn = {9781479943098},
issn = {21607516},
pages = {171--178},
title = {{GPS refinement and camera orientation estimation from a single image and a 2D map}},
year = {2014}
}
@inproceedings{Niitani2017,
abstract = {Despite significant progress of deep learning in the field of computer vision, there has not been a software library that covers these methods in a unifying manner. We introduce ChainerCV, a software library that is intended to fill this gap. ChainerCV supports numerous neural network models as well as software components needed to conduct research in computer vision. These implementations emphasize simplicity, flexibility and good software engineering practices. The library is designed to perform on par with the results reported in published papers and its tools can be used as a baseline for future research in computer vision. Our implementation includes sophisticated models like Faster R-CNN and SSD, and covers tasks such as object detection and semantic segmentation.},
archivePrefix = {arXiv},
arxivId = {1708.08169},
author = {Niitani, Yusuke and Ogawa, Toru and Saito, Shunta and Saito, Masaki},
booktitle = {Proceedings of the ACM International Conference on Multimedia (MM)},
doi = {10.1145/3123266.3129395},
eprint = {1708.08169},
file = {:home/nathan/Documents/Mendeley Desktop/Niitani et al/Proceedings of the ACM International Conference on Multimedia (MM)/Niitani et al.{\_}2017{\_}ChainerCV a Library for Deep Learning in Computer Vision.pdf:pdf},
isbn = {9781450349062},
keywords = {2017,acm reference format,and masaki saito,chain-,computer vision,deep learning,machine learning,open source,shunta saito,toru ogawa,yusuke niitani},
pages = {2--5},
title = {{ChainerCV: a Library for Deep Learning in Computer Vision}},
url = {http://arxiv.org/abs/1708.08169},
year = {2017}
}
@inproceedings{Tolias2016,
abstract = {Recently, image representation built upon Convolutional Neural Network (CNN) has been shown to provide effective descriptors for image search, outperforming pre-CNN features as short-vector representations. Yet such models are not compatible with geometry-aware re-ranking methods and still outperformed, on some particular object retrieval benchmarks, by traditional image search systems relying on precise descriptor matching, geometric re-ranking, or query expansion. This work revisits both retrieval stages, namely initial search and re-ranking, by employing the same primitive information derived from the CNN. We build compact feature vectors that encode several image regions without the need to feed multiple inputs to the network. Furthermore, we extend integral images to handle max-pooling on convolutional layer activations, allowing us to efficiently localize matching objects. The resulting bounding box is finally used for image re-ranking. As a result, this paper significantly improves existing CNN-based recognition pipeline: We report for the first time results competing with traditional methods on the challenging Oxford5k and Paris6k datasets.},
archivePrefix = {arXiv},
arxivId = {1511.05879},
author = {Tolias, Giorgos and Sicre, Ronan and J{\'{e}}gou, Herv{\'{e}}},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
eprint = {1511.05879},
file = {:home/nathan/Documents/Mendeley Desktop/Tolias, Sicre, J{\'{e}}gou/Proceedings of the International Conference on Learning Representations (ICLR)/Tolias, Sicre, J{\'{e}}gou{\_}2016{\_}Particular object retrieval with integral max-pooling of CNN activations.pdf:pdf},
pages = {1--11},
title = {{Particular object retrieval with integral max-pooling of CNN activations}},
url = {http://arxiv.org/abs/1511.05879},
year = {2016}
}
@article{Laskar2017,
abstract = {We propose a new deep learning based approach for camera relocalization. Our approach localizes a given query image by using a convolutional neural network (CNN) for first retrieving similar database images and then predicting the relative pose between the query and the database images, whose poses are known. The camera location for the query image is obtained via triangulation from two relative translation estimates using a RANSAC based approach. Each relative pose estimate provides a hypothesis for the camera orientation and they are fused in a second RANSAC scheme. The neural network is trained for relative pose estimation in an end-to-end manner using training image pairs. In contrast to previous work, our approach does not require scene-specific training of the network, which improves scalability, and it can also be applied to scenes which are not available during the training of the network. As another main contribution, we release a challenging indoor localisation dataset covering 5 different scenes registered to a common coordinate frame. We evaluate our approach using both our own dataset and the standard 7 Scenes benchmark. The results show that the proposed approach generalizes well to previously unseen scenes and compares favourably to other recent CNN-based methods.},
archivePrefix = {arXiv},
arxivId = {1707.09733},
author = {Laskar, Zakaria and Melekhov, Iaroslav and Kalia, Surya and Kannala, Juho},
eprint = {1707.09733},
file = {:home/nathan/Documents/Mendeley Desktop/Laskar et al/arXiv preprint/Laskar et al.{\_}2017{\_}Camera Relocalization by Predicting Pairwise Relative Poses Using Convolutional Neural Network.pdf:pdf},
journal = {arXiv preprint},
title = {{Camera Relocalization by Predicting Pairwise Relative Poses Using Convolutional Neural Network}},
url = {http://arxiv.org/abs/1707.09733},
year = {2017}
}
@article{Li2010,
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de visual localization qui inverse la solution classique en faisant matcher des points 3D d'un mod{\`{e}}le SfM aux features d'un query image (P2F au lieu de F2P). La premi{\`{e}}re {\'{e}}tape consiste {\`{a}} une r{\'{e}}duction du mod{\`{e}}le 3D en selectionnant seulement les points qui apparaissent le plus souvent dans les images et en couvrant l'int{\'{e}}gralit{\'{e}} des points du mod{\`{e}}le (probl{\`{e}}me de minimisation NP hard). Pour le matching avec la query, les auteurs utilisent d'abord quelques points "seeds" dissimin{\'{e}}s sur tout le mod{\`{e}}le pour obtenir des zones envisageable de prise de vue de la query puis la comparaison avec les autres points du mod{\`{e}}le par le biai d'une liste de prioretisation se basant sur la proximit{\'{e}} des points d{\'{e}}j{\`{a}} correctement match{\'{e}}s et leur "popularit{\'{e}}". Regression de la pose final par p6p et DLT.

Ce qui est int{\'{e}}ressant
- Meilleur r{\'{e}}sultats que F2P approche
- Temps de calcul pas trop long vu qu'on s'arrete apr{\`{e}}s un certain nombre de bonnes coresspondances
- Aspect de prioretisation des features les un par rapport aux autres

Computational load
Offline: Gread y algorithm NP hard problem minimisation + SfM

Online: {\~{}}1 sec ({\textless}3s for reject time)

Scalability
Scale: Multiple places
Scalability potential: ?},
author = {Li, Yunpeng and Snavely, Noah and Huttenlocher, Daniel P.},
doi = {10.1007/978-3-642-15552-9_57},
file = {:home/nathan/Documents/Mendeley Desktop/Li, Snavely, Huttenlocher/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Li, Snavely, Huttenlocher{\_}2010{\_}Location Recognition using Prioritized Feature Matching.pdf:pdf},
isbn = {978-3-642-15551-2},
issn = {03029743},
journal = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
keywords = {image matching,image registration,location recognition,structure},
pages = {791--804},
title = {{Location Recognition using Prioritized Feature Matching}},
year = {2010}
}
@article{Maddern2016,
author = {Maddern, Will and Pascoe, Geoffrey and Linegar, Chris and Newman, Paul},
journal = {The International Journal of Robotics Research (IJRR)},
pages = {0278364916679498},
publisher = {SAGE Publications},
title = {{1 year, 1000 km: The Oxford RobotCar dataset}},
year = {2016}
}
@article{Gupta2014,
abstract = {In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3{\%}, which is a 56{\%} relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24{\%} relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics.},
annote = {HHA inventors

They do fine tuning on HHA and fine tuning on RGB separataly (they found work similar than RGBD fine tuning). 
They compare against 4 channel RGBD, HHA vs Disparity and w$\backslash$ and w$\backslash$out sythetic data (+1 pt with syth data)},
archivePrefix = {arXiv},
arxivId = {arXiv:1407.5736v1},
author = {Gupta, Saurabh and Girshick, Ross and Arbel{\'{a}}ez, Pablo and Malik, Jitendra},
doi = {10.1007/978-3-319-10584-0_23},
eprint = {arXiv:1407.5736v1},
file = {:home/nathan/Documents/Mendeley Desktop/Gupta et al/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Gupta et al.{\_}2014{\_}Learning rich features from RGB-D images for object detection and segmentation.pdf:pdf},
isbn = {9783319105833},
issn = {16113349},
journal = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
keywords = {RGB-D perception,object detection,object segmentation},
number = {PART 7},
pages = {345--360},
pmid = {604804},
title = {{Learning rich features from RGB-D images for object detection and segmentation}},
volume = {8695 LNCS},
year = {2014}
}
@article{Bai2017,
author = {Bai, Dongdong and Wang, Chaoqun and Zhang, Bo and Yi, Xiaodong and Yang, Xuejun},
doi = {10.1016/j.cag.2017.07.019},
file = {:home/nathan/Documents/Mendeley Desktop/Bai et al/Computers {\&} Graphics/Bai et al.{\_}2017{\_}Sequence searching with CNN features for robust and fast visual place recognition.pdf:pdf},
issn = {00978493},
journal = {Computers {\&} Graphics},
keywords = {Visual place recognition,CNN,SeqSLAM,SeqCNNSLAM,A-,visual place recognition},
publisher = {Elsevier Ltd},
title = {{Sequence searching with CNN features for robust and fast visual place recognition}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0097849317301127},
year = {2017}
}
@inproceedings{Bhowmik2017,
author = {Bhowmik, Neelanjan and Weng, Li and Gouet-Brunet, Val{\'{e}}rie and Soheilian, Bahman},
booktitle = {Joint Urban Remote Sensing Event (JURSE)},
title = {{Cross-domain Image Localization by Adaptive Feature Fusion}},
year = {2017}
}
@inproceedings{Sunderhauf2015,
abstract = {This paper provides a comprehensive investigation of the utility of ConvNet features for robotic place recogni- tion under changing conditions and viewpoint. We focus on analyzing the performance of features extracted from different layers of a Convolutional Network and examine their robustness against changes in viewpoint and environmental conditions.We gain the insight that these features are well suited to be used in place recognition and that the middle layers are very robust against appearance changes while the higher layers become increasingly invariant to viewpoint changes. Experiments are conducted on a number of different challenging datasets. We furthermore demonstrate how the hierarchical structure of the ConvNet features can be exploited by partitioning the search space based on the semantic information that is contained in these layers.},
annote = {R{\'{e}}sum{\'{e}}
Papier analysant les CNN pour la t{\^{a}}che de place recognition. Les auteurs extraient des descripteurs globaux des images {\`{a}} diff{\'{e}}rentes couches du r{\'{e}}seaux et comparent les performances sur plusieurs datasets (changement d'apparences et de points de vue). Ils en concluent que les couches profondes (conv3) sont plus discriminantes pour le changement d'apparence alors que les couches de plus hauts niveaux (fc6) permettent de rendre le descripteur invariant aux changements de points de vue. Trois autre apport en fin d'article : compression du vecteur descripteur par hashing pour faire du temp r{\'{e}}el, utilisation combin{\'{e}} des layers fc6 pour partionner l'espace de recherche (SVM) puis conv3 pour matcher la bonne image et mise en avant qu'un CNN entrain{\'{e}} pour la place categorization fonctionne mieux qu'un CNN entrain{\'{e}} pour object recognition.

Ce que je n'ai pas compris
- Pourquoi on utilise la cosinue distance ? -{\textgreater} diminuer les temps de comparaison par 2

Ce qui est int{\'{e}}ressant
- Les temps de calculs {\&} performances
- Le hashing pour la compression

Critiques
- Pas de "true negative" dans leur dataset, donc courbre recall/precision un peu fausse
- Comparaison avec state of the art pas bonne
- Dataset pas tres challenging en terme de view point changes

Computational load
Offline: Pretrained CNN -{\textgreater} 0
Online: 5hz (1k€ GPU)

Scalability
Scale: 100k images
Scalability potential: Real},
archivePrefix = {arXiv},
arxivId = {1501.04158v1},
author = {S{\"{u}}nderhauf, Niko and Shirazi, Sareh and Dayoub, Feras and Upcroft, Ben and Milford, Michael J.},
booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2015.7353986},
eprint = {1501.04158v1},
file = {:home/nathan/Documents/Mendeley Desktop/S{\"{u}}nderhauf et al/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/S{\"{u}}nderhauf et al.{\_}2015{\_}On the performance of ConvNet features for place recognition.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
keywords = {CNN,Challenging matching,Computer vision,Feature extraction,Global features,Outdoor,RGB request,Real-time systems,Robotic,Robustness,SLAM,SVM,Semantics,Unsupervised learning,Visual place recognition,Visualization},
mendeley-tags = {CNN,Challenging matching,Global features,Outdoor,RGB request,Robotic,SLAM,SVM,Unsupervised learning,Visual place recognition},
pages = {4297--4304},
title = {{On the performance of ConvNet features for place recognition}},
volume = {2015-Decem},
year = {2015}
}
@article{Arandjelovic2012,
abstract = {The objective of this work is object retrieval in large scale image datasets, where the object is specified by an image query and retrieval should be immediate at run time in the manner of Video Google [28]. We make the following three contributions: (i) a new method to compare SIFT descriptors (RootSIFT) which yields superior performance without increasing process- ing or storage requirements; (ii) a novel method for query expansion where a richer model for the query is learnt discriminatively in a form suited to immediate retrieval through efficient use of the inverted index; (iii) an improve- ment of the image augmentationmethod proposed by Turcot and Lowe [29], where only the augmenting features which are spatially consistent with the augmented image are kept. We evaluate these three methods over a number of stan- dard benchmark datasets (Oxford Buildings 5k and 105k, and Paris 6k) and demonstrate substantial improvements in retrieval performance whilst maintaining immediate re- trieval speeds. Combining these complementary meth- ods achieves a new state-of-the-art performance on these datasets.},
annote = {R{\'{e}}sum{\'{e}}

Ce papier pr{\'{e}}sente trois points d'am{\'{e}}lioration dans le domaine object retrival in large scale image datasets lorsqu'on utilise le framework classique Bag of Words. Les donn{\'{e}}es utilis{\'{e}}es sont celle de batiment compris dans des photos de ville. 
La premi{\`{e}}re am{\'{e}}lioration concerne le descripteur local SIFT et en particulier la m{\'{e}}thode de matching qui lui est associ{\'{e}} (en utilisant la racine carr{\'{e}} du descripteur on obtient de meilleurs r{\'{e}}sultats lors des matchings). 
La seconde am{\'{e}}liration concerne la query expension : m{\'{e}}thode qui consiste {\`{a}} augmenter le nombre des mots visuels associ{\'{e}} {\`{a}} une requet en agglom{\'{e}}rant des mots visuels issus d'image proche.
La troisi{\`{e}}me am{\'{e}}lioration vient modifier la mani{\`{e}}re d'utiliser un matching graph pour diminuer le nombre faux positifs.

Ce que je n'ai pas compris

- Pas sur d'avoir bien compris la m{\'{e}}thode des bags of words
- Comment on construit un graph de matching ? (comment ils font leur verification d'hommographie ?)
- Qu'est ce que le soft alignement ?

Ce qui est int{\'{e}}ressant

- RootSIFT
- Les deux autres m{\'{e}}thodes qui n'ajoute pas trop de temps de calcul online
- La rapidit{\'{e}} de la m{\'{e}}thode

Critiques

- Pas vraiment orient{\'{e}} place recognition, aucune notion de distance (on reconnait des "lieux")
- Dataset pas tr{\`{e}}s grand

A approfondir

- Matching graph
- Soft alignement

Computational load
Offline: Classic BoW
Online: Un tout petit peu plus que les approches BoW classique

Scalability
Scale: City
Scalability potential: Growing et probleme de perte en discrimination avec l'augmentation de la base de donn{\'{e}}es avec les BoW},
author = {Arandjelovi{\'{c}}, Relja and Zisserman, Andrew},
doi = {10.1109/CVPR.2012.6248018},
file = {:home/nathan/Documents/Mendeley Desktop/Arandjelovi{\'{c}}, Zisserman/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Arandjelovi{\'{c}}, Zisserman{\_}2012{\_}Three things everyone should know to improve object retrieval.pdf:pdf},
isbn = {9781467312288},
issn = {9781467312288},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
keywords = {Challenging matching,Local features,Outdoor,SIFT,SVM,Visual place recognition,Visual words},
mendeley-tags = {Challenging matching,Local features,Outdoor,SIFT,SVM,Visual place recognition,Visual words},
number = {April},
pages = {2911--2918},
title = {{Three things everyone should know to improve object retrieval}},
year = {2012}
}
@inproceedings{Yi2016,
abstract = {We show how to train a Convolutional Neural Network to assign a canonical orientation to feature points given an image patch centered on the feature point. Our method improves feature point matching upon the state-of-the art and can be used in conjunction with any existing rotation sensitive descriptors. To avoid the tedious and almost impossible task of finding a target orientation to learn, we propose to use Siamese networks which implicitly find the optimal orientations during training. We also propose a new type of activation function for Neural Networks that generalizes the popular ReLU, maxout, and PReLU activation functions. This novel activation performs better for our task. We validate the effectiveness of our method extensively with four existing datasets, including two non-planar datasets, as well as our own dataset. We show that we outperform the state-of-the-art without the need of retraining for each dataset.},
archivePrefix = {arXiv},
arxivId = {1511.04273},
author = {Yi, Kwang Moo and Verdie, Yannick and Fua, Pascal and Lepetit, Vincent},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.19},
eprint = {1511.04273},
file = {:home/nathan/Documents/Mendeley Desktop/Yi et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Yi et al.{\_}2016{\_}Learning to Assign Orientations to Feature Points.pdf:pdf},
pages = {107--116},
title = {{Learning to Assign Orientations to Feature Points}},
url = {http://arxiv.org/abs/1511.04273},
year = {2016}
}
@article{Mahendran2017,
archivePrefix = {arXiv},
arxivId = {1708.05628},
author = {Mahendran, Siddharth and Ali, Haider},
doi = {10.1109/CVPRW.2017.73},
eprint = {1708.05628},
file = {:home/nathan/Documents/Mendeley Desktop/Mahendran, Ali/Unknown/Mahendran, Ali{\_}2017{\_}3D Pose Regression using Convolutional Neural Networks.pdf:pdf},
pages = {2--3},
title = {{3D Pose Regression using Convolutional Neural Networks}},
volume = {2},
year = {2017}
}
@inproceedings{Liu2012,
author = {Liu, Zhe and Marlet, Renaud},
booktitle = {British Machine Vision Conference (BMVC)},
pages = {11--16},
title = {{Virtual line descriptor and semi-local matching method for reliable feature correspondence}},
year = {2012}
}
@inproceedings{Massiceti2016,
abstract = {This work addresses the task of camera localization in a known 3D scene, given a single input RGB image. State-of-the-art approaches accomplish this with two steps. Firstly, regressing for every pixel in the image its so-called 3D scene coordinate and, subsequently, using those coordinates to estimate the final 6D camera pose via RANSAC. To solve the first step, Random Forests (RFs) are typically used. On the other hand, Neural Networks (NNs) currently reign in many dense regression problems, but are not test-time efficient. We ask the question: Which of the two is the best choice for camera localization? To address this, we make two method contributions: (1) a test-time efficient NN architecture which we term a ForestNet that is derived and initialized from a RF, and (2) a new fully-differentiable robust averaging technique for regression ensembles which can be trained end-to-end with a NN architecture. Our experimental findings show that traditional NN architectures are superior to test-time efficient RFs and ForestNets in terms of scene coordinate regression, however, this does not translate to final 6D camera pose accuracy where ForestNets and RFs perform slightly better. To summarize, our best method, a ForestNet with a robust average, improves over the state-of-the-art for camera localization on the 7-Scenes dataset. While this work focuses on scene coordinate regression for camera localization, our innovations may also be applied to other continuous regression tasks.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode s'inspirant des travaux de Shotton et al. 2013 qui remplace la random forests par des NN (neural networks) et utilise seulement les cannaux RGB de l'image (la profondeur est seulemnt utilis{\'{e}} pour la phase d'entrainement). Les poids sont initialis{\'{e}}s ne s'appuyant sur les param{\`{e}}tres de la RF puis fine-tuned sur un sous-ensemble des donn{\'{e}}es. Les auteurs comparent {\'{e}}galement leur r{\'{e}}sultats avec un CNN qui apprend a regresser la pose du pixel avec en entr{\'{e}} un pixel (enfin je crois, c'est pas vraiment expliqu{\'{e}}). 

Ce que je n'ai pas compris
- Pas lu le d{\'{e}}tail de la conversion des RF en NN

Ce qui est int{\'{e}}ressant
- R{\'{e}}sultat un peu meilleur
- Juste du RGB en input

Critiques
- Gros temps de calcul et de m{\'{e}}moire (vu qu'on a une "foret" de NN), ils disent qu'on peut repasser sur des RF mais du coup je vois pas l'interet (a moins de transf{\'{e}}rer ce qu'on a appri avec le fine-tuning)

Computational load
Offline: ?
Online: {\~{}}7s par image

Scalability
Scale: Room
Scalability potential: -},
archivePrefix = {arXiv},
arxivId = {1609.05797},
author = {Massiceti, Daniela and Krull, Alexander and Brachmann, Eric and Rother, Carsten and Torr, Philip H. S.},
booktitle = {Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)},
eprint = {1609.05797},
file = {:home/nathan/Documents/Mendeley Desktop/Massiceti et al/Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)/Massiceti et al.{\_}2017{\_}Random Forests versus Neural Networks - What's Best for Camera Relocalization.pdf:pdf},
title = {{Random Forests versus Neural Networks - What's Best for Camera Relocalization?}},
url = {http://arxiv.org/abs/1609.05797},
year = {2017}
}
@article{Jia2016,
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode directe utilisant un CNN pour la tache de localization bas{\'{e}} vision. Les principales contributions sont qu'ils comparent plusieurs architectures pour {\'{e}}valuer la meilleur, et qu'ils g{\'{e}}n{\'{e}}rent beaucoup de donn{\'{e}}e d'entraintement de couple (image, 6DoF) {\`{a}} partir d'un nuage de point dense g{\'{e}}n{\'{e}}r{\'{e}} par SfM (log. VisualSfM) et d'un logiciel de rendu graphique (Unity) pour synthetiser des vues intermedaires avec des effets visuels obtenus par shader pour simuler des conditions m{\'{e}}t{\'{e}}orologique diverse).

Ce qui est int{\'{e}}ressant
- La sythese de vue
- Etat de l'art qui couvre bien toutes les m{\'{e}}thodes

Critiques
- Pas de comparaison sur des dataset existant
- Article mal ecrit

Computational load
Offline: ?
Online: O(1)

Scalability
Scale: 6000 m²
Scalability potential: {\~{}}},
archivePrefix = {arXiv},
arxivId = {1611.02776},
author = {Jia, Daoyuan and Su, Yongchi and Li, Chunping},
eprint = {1611.02776},
file = {:home/nathan/Documents/Mendeley Desktop/Jia, Su, Li/arXiv preprint/Jia, Su, Li{\_}2016{\_}Deep Convolutional Neural Network for 6-DOF Image Localization.pdf:pdf},
journal = {arXiv preprint},
number = {413113},
pages = {1790--1798},
title = {{Deep Convolutional Neural Network for 6-DOF Image Localization}},
url = {http://papers.nips.cc/paper/5485-deep-convolutional-neural-network-for-image-deconvolution},
year = {2016}
}
@inproceedings{Donoser2014,
abstract = {The prevalent approach to image-based localization is matching interest points detected in the query image to a sparse 3D point cloud representing the known world. The obtained correspondences are then used to recover a precise camera pose. The state-of-the-art in this field often ignores the availability of a set of 2D descriptors per 3D point, for example by representing each 3D point by only its centroid. In this paper we demonstrate that these sets contain useful information that can be exploited by formulating matching as a discriminative classification problem. Since memory demands and computational complexity are crucial in such a setup, we base our algorithm on the efficient and effective random fern principle. We propose an extension which projects features to fern-specific embedding spaces, which yields improved matching rates in short runtime. Experiments first show that our novel formulation provides improved matching performance in comparison to the standard nearest neighbor approach and that we outperform related randomization methods in our localization scenario.},
annote = {R{\'{e}}sum{\'{e}}
Nouvelle m{\'{e}}thode de matching de feature se basant sur un mod{\`{e}}le de classification d{\'{e}}riv{\'{e}} des random forest : les random fern. Les auteurs s'interessent {\`{a}} la probl{\'{e}}matique de la localisation bas{\'{e}} vision dans le cas ou l'on dispose d'un mod{\`{e}}le 3D (SfM) et d'une information flou sur la position de l'agent pour diminuer la zone de recherche dans le mod{\`{e}}le 3D. Il propose un m{\'{e}}thode d{\'{e}}riv{\'{e}}e des random ferm coupl{\'{e}}e {\`{a}} une r{\'{e}}duction de dimension par CCA (Canonical Correlatoin Analysis) pour classifier les keys points. L'apprentissage est fait en prenant en compte tous les descripteurs 2D associ{\'{e}}s {\`{a}} un point 3D. Ensuite PROSAC et pnp pour retrouver la postioin de la camera.

Ce que je n'ai pas compris
- Random ferms
- Comment ils traitent le fait que certain point 3D ont beaucoup plus de descripteurs 2D que d'autre ?

Ce qui est int{\'{e}}ressant
- Rapidi{\'{e}} du matching d{\`{e}}s lors ou on a faire {\`{a}} plus de quelques centaines de keypoint
- Matching meilleur qu'avec du brut force

Critiques
- Pas trop de d{\'{e}}taille sur les temps de calcul
- L'espace est subdivis{\'{e}} et ils utilisent une connaissance {\`{a}} priori de la postion de la cam{\'{e}}ra

A approfondir
- Random ferms
- PROSAC

Computational load
Offline: -
Online: + rapide que BF

Scalability
Scale: On sait pas trop
Scalability potential: Idem},
author = {Donoser, Michael and Schmalstieg, Dieter},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2014.73},
file = {:home/nathan/Documents/Mendeley Desktop/Donoser, Schmalstieg/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Donoser, Schmalstieg{\_}2014{\_}Discriminative feature-to-point matching in image-based localization.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
keywords = {Camera Pose Estimation,Classification,Image-based Localization},
pages = {516--523},
title = {{Discriminative feature-to-point matching in image-based localization}},
year = {2014}
}
@inproceedings{Bansal2014,
abstract = {We propose a purely geometric correspondence-free ap-proach to urban geo-localization using 3D point-ray fea-tures extracted from the Digital Elevation Map of an urban environment. We derive a novel formulation for estimat-ing the camera pose locus using 3D-to-2D correspondence of a single point and a single direction alone. We show how this allows us to compute putative correspondences between building corners in the DEM and the query im-age by exhaustively combining pairs of point-ray features. Then, we employ the two-point method to estimate both the camera pose and compute correspondences between build-ings in the DEM and the query image. Finally, we show that the computed camera poses can be efficiently ranked by a simple skyline projection step using building edges from the DEM. Our experimental evaluation illustrates the promise of a purely geometric approach to the urban geo-localization problem.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de LBV en milieu urbain utilisant des images au niveau de la rue et un MNT (DEM en anglais) comme base de donn{\'{e}}es. Les auteurs consid{\`{e}}re un probl{\'{e}}me de mise en corresspondance de points d{\'{e}}signant des coins de batiments puis de calcul de pose par algorithme de deux points. Pour rendre cette probl{\'{e}}matique viable (rapport au temps de calcul) les auteurs ne consid{\'{e}}rent pas seulement des points dans la requ{\^{e}}te mais de PointRay d{\'{e}}signant un coin de batiment et la direction dans laquelle le batiment se prolonge. L'extraction des PointRay dans l'image se fait pas filtre de canny, d{\'{e}}tection de segement et s{\'{e}}gmentation du ciel.

Ce que je n'ai pas compris
- Je n'ai pas lu la partie technique

Ce qui est int{\'{e}}ressant
- Method ne se bassant pas du tout sur comparaison d'image mais uniquement features geometriques.

Critiques
- Pas de comparaison avec l'etat de l'art
- Pas de temps de calcul sur l'extraction des features image

Computational load
Offline: - 
Online: complexit{\'{e}} en O(m{\^{}}2n) avec m le nombre de PointRay et n le nombre de somment dans le MNT

Scalability
Scale: City
Scalability potential: lineraly growing with size of MNT},
author = {Bansal, Mayank and Daniilidis, Kostas},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Bansal, Daniilidis/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Bansal, Daniilidis{\_}2014{\_}Geometric Urban Geo-Localization.pdf:pdf},
title = {{Geometric Urban Geo-Localization}},
year = {2014}
}
@inproceedings{Zeisl2015,
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de image based localization fortement insipr{\'{e}}e des travaux de Svarm et al. 2014-2016, et se focalisant plus sur le rejet des outliers que sur l'obtention en amont de bon matching. Les auteurs utilisent une pipeline classique de matching 2D/3D (SIFT + kdtree quantization) avec une relaxation au niveau de la selection des matchs (ils consid{\'{e}}rent le matching d'un descripteur de la query avec plusieurs descripteurs du mod{\`{e}}le 3D). Pour ensuite faire du rejet rapide d'outliers, ils font l'hypoth{\`{e}}se qu'ils connaissent 2 DDL de la position de la cam{\'{e}}ra (deux angles par rapport {\`{a}} la vertical et une approximation de l'altitude), et emploie la m{\^{e}}me technique que Svarm et al. {\`{a}} la diff{\'{e}}rence qu'ils projettent l'insertitude de position dans le r{\'{e}}p{\`{e}}re global du mod{\`{e}}le plut{\^{o}}t que dans celui de la cam{\'{e}}ra. Les auteurs rajoutent {\'{e}}galement d'autres filtres qui interviennent au moment du matching et qui prennent en consid{\'{e}}ration des informations d'{\'{e}}chelle et d'orientation des descripteurs, de visibilit{\'{e}} des keypoints dans le mod{\`{e}}le 3D et de connaissance a priori de la position (GPS).

Ce que je n'ai pas compris
- Les m{\^{e}}mes choses que pour le papier de Svarm et al. , les aspects math{\'{e}}matiques du rejet rapide

Ce qui est int{\'{e}}ressant
- Les petits filtres ajout{\'{e}}s lors du matching

Critiques
- Pas de r{\'{e}}elle am{\'{e}}lioration
- Le temps de clacul qui reste un peu long, et le manqur d'information sur le temps de calcul avec RANSAC {\&} BA

Computational load
Offline: - 
Online: 4s sans RANSAC et BA

Scalability
Scale: City
Scalability potential: Real},
author = {Zeisl, Bernhard and Sattler, Torsten and Pollefeys, Marc},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
file = {:home/nathan/Documents/Mendeley Desktop/Zeisl, Sattler, Pollefeys/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Zeisl, Sattler, Pollefeys{\_}2015{\_}Camera Pose Voting for Large-Scale Image-Based Localization.pdf:pdf},
pages = {2704--2712},
title = {{Camera Pose Voting for Large-Scale Image-Based Localization}},
year = {2015}
}
@article{Brejcha2017a,
author = {Brejcha, Jan and Cadik, Martin},
doi = {10.1016/j.imavis.2017.05.009},
file = {:home/nathan/Documents/Mendeley Desktop/Brejcha, Cadik/Image and Vision Computing/Brejcha, Cadik{\_}2017{\_}GeoPose3K Mountain Landscape Dataset for Camera Pose Estimation in Outdoor Environments.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
publisher = {Elsevier B.V.},
title = {{GeoPose3K: Mountain Landscape Dataset for Camera Pose Estimation in Outdoor Environments}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0262885617300963},
year = {2017}
}
@article{Lynen2015,
abstract = {Accurately estimating a robot's pose relative to a global scene model and precisely tracking the pose in real-time is a fundamental problem for navigation and obstacle avoidance tasks. Due to the computational complexity of localization against a large map and the memory consumed by the model, state-of- the-art approaches are either limited to small workspaces or rely on a server-side system to query the global model while tracking the pose locally. The latter approaches face the problem of smoothly integrating the server's pose estimates into the trajectory computed locally to avoid temporal discontinuities. In this paper, we demonstrate that large-scale, real-time pose estimation and tracking can be performed on mobile platforms with limited resources without the use of an external server. This is achieved by employing map and descriptor compression schemes as well as efficient search algorithms from computer vision. We derive a formulation for integrating the global pose information into a local state estimator that produces much smoother trajectories than current approaches. Through detailed experiments, we evaluate each of our design choices individually and document its impact on the overall system performance, demonstrating that our approach outperforms state-of-the-art algorithms},
author = {Lynen, Simon and Sattler, Torsten and Bosse, Michael and Hesch, Joel A. and Pollefeys, Marc and Siegwart, Roland},
doi = {10.15607/RSS.2015.XI.037},
file = {:home/nathan/Documents/Mendeley Desktop/Lynen et al/Robotics Science and Systems (RSS)/Lynen et al.{\_}2015{\_}Get out of my lab Large-scale, real-time visual-intertial localization.pdf:pdf},
isbn = {978-0-9923747-1-6},
journal = {Robotics Science and Systems (RSS)},
keywords = {inertial,localisation,vision},
pages = {37--46},
title = {{Get out of my lab: Large-scale, real-time visual-intertial localization}},
year = {2015}
}
@article{Krajnik2017a,
abstract = {We present an evaluation of standard image features in the context of long-term visual teach-and-repeat navigation of mobile robots, where the environment exhibits significant changes in appearance caused by seasonal weather variations and daily illumination changes. We argue that for long-term autonomous navigation, the viewpoint-, scale- and rotation- invariance of the standard feature extractors is less important than their robustness to the mid- and long-term environment appearance changes. Therefore, we focus our evaluation on the robustness of image registration to variable lighting and naturally-occurring seasonal changes. We combine detection and description components of different image extractors and evaluate their performance on five datasets collected by mobile vehicles in three different outdoor environments over the course of one year. Moreover, we propose a trainable feature descriptor based on a combination of evolutionary algorithms and Binary Robust Independent Elementary Features, which we call GRIEF (Generated BRIEF). In terms of robustness to seasonal changes, the most promising results were achieved by the SpG/CNN and the STAR/GRIEF feature, which was slightly less robust, but faster to calculate.},
annote = {Etude approfondie des couples detecteur/descripteur la tache de mise en correspondance de points dans un environement changeant. + New descripteur GRIEF},
author = {Krajn{\'{i}}k, Tom{\'{a}}{\v{s}} and Cristoforis, Pablo and Kusumam, Keerthy and Neubert, Peer and Duckett, Tom},
doi = {10.1016/j.robot.2016.11.011},
file = {:home/nathan/Documents/Mendeley Desktop/Krajn{\'{i}}k et al/Robotics and Autonomous Systems (RAS)/Krajn{\'{i}}k et al.{\_}2017{\_}Image features for visual teach-and-repeat navigation in changing environments.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems (RAS)},
keywords = {Long-term autonomy,Mobile robotics,Visual navigation},
number = {November},
pages = {127--141},
title = {{Image features for visual teach-and-repeat navigation in changing environments}},
volume = {88},
year = {2017}
}
@article{Matas2004,
author = {Matas, Jiri and Chum, Ondřej and Urban, Martin and Pajdla, Tomas},
journal = {Image and Vision Computing},
number = {10},
pages = {761--767},
publisher = {Elsevier},
title = {{Robust wide-baseline stereo from maximally stable extremal regions}},
volume = {22},
year = {2004}
}
@misc{VC1962,
annote = {US Patent 3,069,654},
author = {VC and Hough, Paul},
publisher = {Google Patents},
title = {{Method and means for recognizing complex patterns}},
year = {1962}
}
@article{Muller2017,
author = {M{\"{u}}ller, M. S. and Urban, S. and Jutzi, B.},
file = {:home/nathan/Documents/Mendeley Desktop/M{\"{u}}ller, Urban, Jutzi/Unknown/M{\"{u}}ller, Urban, Jutzi{\_}2017{\_}Squeezeposenet Image Based Pose Regression with Small Convolutional Neural Networks for Real Time UAS Navigat.pdf:pdf},
keywords = {convolutional neural networks,image-based,navigation,pose estimation,uas,uav},
title = {{Squeezeposenet: Image Based Pose Regression with Small Convolutional Neural Networks for Real Time UAS Navigation}},
year = {2017}
}
@article{canny1986computational,
author = {Canny, John},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
number = {6},
pages = {679--698},
publisher = {IEEE},
title = {{A computational approach to edge detection}},
year = {1986}
}
@inproceedings{Eigen2014,
abstract = {Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.},
archivePrefix = {arXiv},
arxivId = {1406.2283},
author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
booktitle = {Annual Conference on Neural Information Processing Systems (NIPS)},
doi = {10.1007/978-3-540-28650-9_5},
eprint = {1406.2283},
file = {:home/nathan/Documents/Mendeley Desktop/Eigen, Puhrsch, Fergus/Annual Conference on Neural Information Processing Systems (NIPS)/Eigen, Puhrsch, Fergus{\_}2014{\_}Depth Map Prediction from a Single Image using a Multi-Scale Deep Network.pdf:pdf},
isbn = {10495258},
issn = {10495258},
pages = {1--9},
pmid = {23459267},
title = {{Depth Map Prediction from a Single Image using a Multi-Scale Deep Network}},
url = {http://arxiv.org/abs/1406.2283},
year = {2014}
}
@article{Valgren2010,
abstract = {In this paper, we address the problem of outdoor, appearance-based topological localization, particularly over long periods of time where seasonal changes alter the appearance of the environment. We investigate a straightforward method that relies on local image features to compare single-image pairs. We first look into which of the dominating image feature algorithms, SIFT or the more recent SURF, that is most suitable for this task. We then fine-tune our localization algorithm in terms of accuracy, and also introduce the epipolar constraint to further improve the result. The final localization algorithm is applied on multiple data sets, each consisting of a large number of panoramic images, which have been acquired over a period of nine months with large seasonal changes. The final localization rate in the single-image matching, cross-seasonal case is between 80{\%} to 95{\%}. ?? 2009 Elsevier B.V. All rights reserved.},
annote = {Comparaison SIFT{\&} SURF pour la localization d'images entre diff{\'{e}}rentes saisons. Ils montrent que SURF l{\'{e}}gerement sup{\'{e}}rieur (en particulier U-SURF vu qu'ils utilisent des images pano dont on connait l'orientation. Ils soulignent quand m{\^{e}}me le fait qu'ils ont beoin des contraintes {\'{e}}pipolaire pour que {\c{c}}a marche bien, sinon trop de faux matching.},
archivePrefix = {arXiv},
arxivId = {www.scholar.google.com},
author = {Valgren, Christoffer and Lilienthal, Achim J.},
doi = {10.1016/j.robot.2009.09.010},
eprint = {www.scholar.google.com},
file = {:home/nathan/Documents/Mendeley Desktop/Valgren, Lilienthal/Robotics and Autonomous Systems (RAS)/Valgren, Lilienthal{\_}2010{\_}SIFT, SURF {\&} seasons Appearance-based long-term localization in outdoor environments.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems (RAS)},
keywords = {Localization,Outdoor environments,Scene recognition},
number = {2},
pages = {149--156},
title = {{SIFT, SURF {\&} seasons: Appearance-based long-term localization in outdoor environments}},
url = {http://dx.doi.org/10.1016/j.robot.2009.09.010},
volume = {58},
year = {2010}
}
@inproceedings{Mousavian2015,
abstract = {The problem of image based localization has a long history both in robotics and computer vision and shares many similarities with image based retrieval problem. Existing techniques use either local features or (semi)-global image signatures in the context of topological mapping or loop closure detection. Difficulties of the location recognition problem are often affected by large appearance and viewpoint variation between the query view and reference dataset and presence of non-discriminative features due to vegetation, sky and road. In this work we show that semantic segmentation labeling of man-made structures can inform the traditional bag-of-visual words models to obtain proper feature weighting and improve the overall location recognition accuracy. We also demonstrate additional capability of identifying individual buildings and estimating their extent in images, providing the essential building block for semantic localization. Towards this end we introduce a new challenging outdoors urban dataset exhibiting large variations in appearance and viewpoint.},
author = {Mousavian, Arsalan and Koseck{\'{a}}, Jana and Lien, Jyh Ming},
booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2015.7139877},
file = {:home/nathan/Documents/Mendeley Desktop/Mousavian, Koseck{\'{a}}, Lien/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Mousavian, Koseck{\'{a}}, Lien{\_}2015{\_}Semantically guided location recognition for outdoors scenes.pdf:pdf},
isbn = {978-1-4799-6923-4},
issn = {10504729},
number = {June},
pages = {4882--4889},
title = {{Semantically guided location recognition for outdoors scenes}},
volume = {2015-June},
year = {2015}
}
@inproceedings{Melekhov,
author = {Melekhov, Iaroslav and Kannala, Juho and Rahtu, Esa},
booktitle = {Proceedings of the International Conference on Pattern Recognition (ICPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Melekhov/Unknown/Melekhov{\_}Unknown{\_}Siamese Network Features for Image Matching.pdf:pdf},
title = {{Siamese Network Features for Image Matching}},
year = {2016}
}
@inproceedings{Mai2017,
abstract = {Spatial-semantic query b) Image search with spatial-semantic constraints a) Image search with semantic constraints only Person Surfboard Water Text-based query Figure 1: Spatial-semantic image search. (a) Searching with content-only queries such as text keywords, while effective in retrieving relevant content, is unable to incorporate detailed spatial intents. (b) Spatial-semantic image search allows users to interact with the 2-D canvas to express their search intent both spatially and semantically. Abstract The performance of image retrieval has been improved tremendously in recent years through the use of deep fea-ture representations. Most existing methods, however, aim to retrieve images that are visually similar or semantically relevant to the query, irrespective of spatial configuration. In this paper, we develop a spatial-semantic image search technology that enables users to search for images with both semantic and spatial constraints by manipulating con-cept text-boxes on a 2D query canvas. We train a convolu-tional neural network to synthesize appropriate visual fea-tures that captures the spatial-semantic constraints from the user canvas query. We directly optimize the retrieval perfor-mance of the visual features when training our deep neural network. These visual features then are used to retrieve im-ages that are both spatially and semantically relevant to the user query. The experiments on large-scale datasets such as MS-COCO and Visual Genome show that our method outperforms other baseline and state-of-the-art methods in spatial-semantic image search.},
author = {Mai, Long and Jin, Hailin and Lin, Zhe and Fang, Chen and Brandt, Jonathan and Liu, Feng},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.125},
file = {:home/nathan/Documents/Mendeley Desktop/Mai et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Mai et al.{\_}2017{\_}Spatial-Semantic Image Search by Visual Feature Synthesis.pdf:pdf},
pages = {1121--1130},
title = {{Spatial-Semantic Image Search by Visual Feature Synthesis}},
volume = {1},
year = {2017}
}
@inproceedings{Jegou2008,
abstract = {This technical report presents and extends a recent paper we have proposed for large scale image search. State-of-the-art methods build on the bag-of- features image representation. We first analyze bag-of-features in the framework of approximate nearest neighbor search. This shows the sub-optimality of such a representation for matching descriptors and leads us to derive a more precise representation based on 1) Hamming embedding (HE) and 2) weak geometric consistency constraints (WGC). HE provides binary signatures that refine the matching based on visual words. WGC filters matching descriptors that are not consistent in terms of angle and scale. HE and WGC are integrated within an inverted file and are efficiently exploited for all images, even in the case of very large datasets. Experiments performed on a dataset of one million of images show a significant improvement due to the binary signature and the weak geometric consistency constraints, as well as their efficiency. Estimation of the full geometric transformation, i.e., a re-ranking step on a short list of images, is complementary to our weak geometric consistency constraints and allows to further improve the accuracy.},
annote = {R{\'{e}}sum{\'{e}}
Introduction de Hamming embedding pour la quantization de descriteur dans le cadre d'image retrieval. On se place dans un sch{\'{e}}ma classique de BoF et les auteurs soulignent le fait que suivant la nombre de clusters on a soit un appariment trop peu pr{\'{e}}cis soit un appariment sensible au bruit. Ils introduisent alors une quantization avec peu de descripteur coupl{\'{e}} a une signature binaire associ{\'{e}} au descripteur pour rafiner le matching de fa{\c{c}}on rapide. Deuxi{\`{e}}me contribution en utilisant l'angle et l'{\'{e}}chelle associ{\'{e}} au descripteur au moment du matching pour {\^{e}}tre plus discriminant.

Ce que je n'ai pas compris
- Calcul du descripteur binaire

Ce qui est int{\'{e}}ressant
- Verficiation "g{\'{e}}om{\'{e}}trique" at quey time


Computational load
Offline: In the paper
Online: idem

Scalability
Scale: -
Scalability potential: -},
author = {J{\'{e}}gou, Herv{\'{e}} and Douze, Matthijs and Schmid, Cordelia},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-540-88682-2_24},
file = {:home/nathan/Documents/Mendeley Desktop/J{\'{e}}gou, Douze, Schmid/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/J{\'{e}}gou, Douze, Schmid{\_}2008{\_}Hamming Embedding and Weak Geometry Consistency for Large Scale Image Search.pdf:pdf},
isbn = {9783540886815},
issn = {03029743},
number = {October},
pages = {304--317},
title = {{Hamming Embedding and Weak Geometry Consistency for Large Scale Image Search}},
year = {2008}
}
@article{Jain2017,
abstract = {Robust visual place recognition (VPR) requires scene representations that are invariant to various environmental challenges such as seasonal changes and variations due to ambient lighting conditions during day and night. Moreover, a practical VPR system necessitates compact representations of environmental features. To satisfy these requirements, in this paper we suggest a modification to the existing pipeline of VPR systems to incorporate supervised hashing. The modified system learns (in a supervised setting) compact binary codes from image feature descriptors. These binary codes imbibe robustness to the visual variations exposed to it during the training phase, thereby, making the system adaptive to severe environmental changes. Also, incorporating supervised hashing makes VPR computationally more efficient and easy to implement on simple hardware. This is because binary embeddings can be learned over simple-to-compute features and the distance computation is also in the low-dimensional hamming space of binary codes. We have performed experiments on several challenging data sets covering seasonal, illumination and viewpoint variations. We also compare two widely used supervised hashing methods of CCAITQ and MLH and show that this new pipeline out-performs or closely matches the state-of-the-art deep learning VPR methods that are based on high-dimensional features extracted from pre-trained deep convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {1709.08103},
author = {Jain, Unnat and Namboodiri, Vinay P. and Pandey, Gaurav},
eprint = {1709.08103},
file = {:home/nathan/Documents/Mendeley Desktop/Jain, Namboodiri, Pandey/arXiv preprint/Jain, Namboodiri, Pandey{\_}2017{\_}Compact Environment-Invariant Codes for Robust Visual Place Recognition.pdf:pdf},
journal = {arXiv preprint},
keywords = {-visual place recognition,a,convolutional neural network,dynamic time warping,hash-,ing,mild to,no viewpoint change,nordland spring-summer,severe appearance change,similarity learning,summer-winter data sets},
title = {{Compact Environment-Invariant Codes for Robust Visual Place Recognition}},
url = {http://arxiv.org/abs/1709.08103},
year = {2017}
}
@article{Fan2017,
author = {Fan, Chen and Chen, Zetao and Jacobson, Adam and Hu, Xiaoping and Milford, Michael J.},
doi = {10.1016/j.robot.2017.07.015},
file = {:home/nathan/Documents/Mendeley Desktop/Fan et al/Robotics and Autonomous Systems (RAS)/Fan et al.{\_}2017{\_}Biologically-inspired visual place recognition with adaptive multiple scales.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems (RAS)},
keywords = {visual place recognition},
pages = {224--237},
publisher = {Elsevier B.V.},
title = {{Biologically-inspired visual place recognition with adaptive multiple scales}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0921889016307990},
volume = {96},
year = {2017}
}
@article{Dong2016,
abstract = {This paper addresses the problem of large-scale image retrieval. We propose a two-layer fusion method which takes advantage of global and local cues and ranks database images from coarse to fine (C2F). Departing from the previous methods fusing multiple image descriptors simultaneously, C2F is featured by a layered procedure composed by filtering and refining. In particular, C2F consists of three components. 1) Distractor filtering. With holistic representations, noise images are filtered out from the database, so the number of candidate images to be used for comparison with the query can be greatly reduced. 2) Adaptive weighting. For a certain query, the similarity of candidate images can be estimated by holistic similarity scores in complementary to the local ones. 3) Candidate refining. Accurate retrieval is conducted via local features, combining the pre-computed adaptive weights. Experiments are presented on two benchmarks, $\backslash$emph{\{}i.e.,{\}} Holidays and Ukbench datasets. We show that our method outperforms recent fusion methods in terms of storage consumption and computation complexity, and that the accuracy is competitive to the state-of-the-arts.},
archivePrefix = {arXiv},
arxivId = {1607.00719},
author = {Dong, Le and Kong, Gaipeng and Dong, Wenpu and Zheng, Liang and Tian, Qi},
eprint = {1607.00719},
file = {:home/nathan/Documents/Mendeley Desktop/Dong et al/arXiv preprint/Dong et al.{\_}2016{\_}Coarse2Fine Two-Layer Fusion For Image Retrieval.pdf:pdf},
journal = {arXiv preprint},
number = {8},
pages = {1--9},
title = {{Coarse2Fine: Two-Layer Fusion For Image Retrieval}},
url = {http://arxiv.org/abs/1607.00719},
volume = {14},
year = {2016}
}
@article{Murillo2013,
abstract = {Vision-based topological localization and mapping for autonomous robotic systems have received increased research interest in recent years. The need to map larger environments requires models at different levels of abstraction and additional abilities to deal with large amounts of data efficiently. Most successful approaches for appearance-based localization and mapping with large datasets typically represent locations using local image features. We study the feasibility of performing these tasks in urban environments using global descriptors instead and taking advantage of the increasingly common panoramic datasets. This paper describes how to represent a panorama using the global gist descriptor, while maintaining desirable invariance properties for location recognition and loop detection. We propose different gist similarity measures and algorithms for appearance-based localization and an online loop-closure detection method, where the probability of loop closure is determined in a Bayesian filtering framework using the proposed image representation. The extensive experimental validation in this paper shows that their performance in urban environments is comparable with local-feature-based approaches when using wide field-of-view images.},
author = {Murillo, Ana C. and Singh, Gautam and Koseck{\'{a}}, Jana and Guerrero, Josechu J.},
doi = {10.1109/TRO.2012.2220211},
file = {:home/nathan/Documents/Mendeley Desktop/Murillo et al/IEEE Transactions on Robotics (ToR)/Murillo et al.{\_}2013{\_}Localization in urban environments using a panoramic gist descriptor.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics (ToR)},
keywords = {Appearance-based localization,computer vision,gist descriptor,omnidirectional images,recognition},
number = {1},
pages = {146--160},
title = {{Localization in urban environments using a panoramic gist descriptor}},
volume = {29},
year = {2013}
}
@article{Martinez-Gomez2016,
abstract = {The semantic localization problem in robotics consists in determining the place where a robot is located by means of semantic categories. The problem is usually addressed as a supervised classification process, where input data correspond to robot perceptions while classes to semantic categories, like kitchen or corridor. In this paper we propose a framework, implemented in the PCL library, which provides a set of valuable tools to easily develop and evaluate semantic localization systems. The implementation includes the generation of 3D global descriptors following a Bag-of-Words approach. This allows the generation of fixed-dimensionality descriptors from any type of keypoint detector and feature extractor combinations. The framework has been designed, structured and implemented to be easily extended with different keypoint detectors, feature extractors as well as classification models. The proposed framework has also been used to evaluate the performance of a set of already implemented descriptors, when used as input for a specific semantic localization system. The obtained results are discussed paying special attention to the internal parameters of the BoW descriptor generation process. Moreover, we also review the combination of some keypoint detectors with different 3D descriptor generation techniques.},
archivePrefix = {arXiv},
arxivId = {1601.08158},
author = {Mart{\'{i}}nez-G{\'{o}}mez, Jesus and Morell, Vicente and Cazorla, Miguel and Garc{\'{i}}a-Varea, Ismael},
doi = {10.1016/j.robot.2015.09.006},
eprint = {1601.08158},
file = {:home/nathan/Documents/Mendeley Desktop/Mart{\'{i}}nez-G{\'{o}}mez et al/Robotics and Autonomous Systems (RAS)/Mart{\'{i}}nez-G{\'{o}}mez et al.{\_}2016{\_}Semantic localization in the PCL library.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems (RAS)},
keywords = {3D features,Classification,PCL,Semantic localization},
pages = {641--648},
title = {{Semantic localization in the PCL library}},
volume = {75},
year = {2016}
}
@inproceedings{Li2016b,
abstract = {Semantic labeling of RGB-D scenes is crucial to many intelligent applications including perceptual robotics. It generates pixelwise and fine-grained label maps from simultaneously sensed photometric (RGB) and depth channels. This paper addresses this problem by i) developing a novel Long Short-Term Memorized Context Fusion (LSTM-CF) Model that captures and fuses contextual information from multiple channels of photometric and depth data, and ii) incorporating this model into deep convolutional neural networks (CNNs) for end-to-end training. Specifically, contexts in photometric and depth channels are, respectively, captured by stacking several convolutional layers and a long short-term memory layer; the memory layer encodes both short-range and long-range spatial dependencies in an image along the vertical direction. Another long short-term memorized fusion layer is set up to integrate the contexts along the vertical direction from different channels, and perform bi-directional propagation of the fused vertical contexts along the horizontal direction to obtain true 2D global contexts. At last, the fused contextual representation is concatenated with the convolutional features extracted from the photometric channels in order to improve the accuracy of fine-scale semantic labeling. Our proposed model has set a new state of the art, i.e., 48.1{\%} and 49.4{\%} average class accuracy over 37 categories (2.2{\%} and 5.4{\%} improvement) on the large-scale SUNRGBD dataset and the NYUDv2dataset, respectively.},
archivePrefix = {arXiv},
arxivId = {1604.05000},
author = {Li, Zhen and Gan, Yukang and Liang, Xiaodan and Yu, Yizhou and Cheng, Hui and Lin, Liang},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-319-46475-6_34},
eprint = {1604.05000},
file = {:home/nathan/Documents/Mendeley Desktop/Li et al/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Li et al.{\_}2016{\_}LSTM-CF Unifying Context Modeling and Fusion with LSTMs for RGB-D Scene Labeling.pdf:pdf},
isbn = {9783319464749},
issn = {16113349},
keywords = {Depth and photometric data fusion,Image context modeling,Long short-term memory,RGB-D scene labeling},
pages = {541--557},
pmid = {10463930},
title = {{LSTM-CF: Unifying Context Modeling and Fusion with LSTMs for RGB-D Scene Labeling}},
volume = {9906 LNCS},
year = {2016}
}
@article{Christie2016,
abstract = {Localization in a global map is critical to success in many autonomous robot missions. This is particularly challenging for multi-robot operations in unknown and adverse environments. Here, we are concerned with providing a small unmanned ground vehicle (UGV) the ability to localize itself within a 2.5D aerial map generated from imagery captured by a low-flying unmanned aerial vehicle (UAV). We consider the scenario where GPS is unavailable and appearance-based scene changes may have occurred between the UAV's flight and the start of the UGV's mission. We present a GPS-free solution to this localization problem that is robust to appearance shifts by exploiting high-level, semantic representations of image and depth data. Using data gathered at an urban test site, we empirically demonstrate that our technique yields results within five meters of a GPS-based approach.},
archivePrefix = {arXiv},
arxivId = {1609.04794},
author = {Christie, Gordon and Warnell, Garrett and Kochersberger, Kevin},
eprint = {1609.04794},
file = {:home/nathan/Documents/Mendeley Desktop/Christie, Warnell, Kochersberger/arXiv preprint/Christie, Warnell, Kochersberger{\_}2016{\_}Semantics for UGV Registration in GPS-denied Environments.pdf:pdf},
journal = {arXiv preprint},
title = {{Semantics for UGV Registration in GPS-denied Environments}},
url = {http://arxiv.org/abs/1609.04794},
year = {2016}
}
@inproceedings{Harris1988,
author = {Harris, Chris and Stephens, Mike},
booktitle = {Alvey vision conference},
organization = {Citeseer},
pages = {50},
title = {{A combined corner and edge detector.}},
volume = {15},
year = {1988}
}
@article{Qi2016a,
abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
archivePrefix = {arXiv},
arxivId = {1612.00593},
author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
eprint = {1612.00593},
file = {:home/nathan/Documents/Mendeley Desktop/Qi et al/arXiv preprint/Qi et al.{\_}2016{\_}PointNet Deep Learning on Point Sets for 3D Classification and Segmentation.pdf:pdf},
journal = {arXiv preprint},
title = {{PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation}},
url = {http://arxiv.org/abs/1612.00593},
year = {2016}
}
@inproceedings{Sattler2012,
abstract = {We propose a powerful pipeline for determining the pose of a query image relative to a point cloud reconstruction of a large scene consisting of more than one million 3D points. The key component of our approach is an ecient and e ective search method to establish matches between image features and scene points needed for pose estimation. Our main contribution is a framework for actively searching for additional matches, based on both 2D-to-3D and 3D-to-2D search. A uni ed formulation of search in both directions allows us to exploit the distinct advantages of both strategies, while avoiding their weaknesses. Due to active search, the resulting pipeline is able to close the gap in registration performance observed between ecient search methods and approaches that are allowed to run for multiple seconds, without sacri cing runtime eciency. Our method achieves the best registration performance published so far on three standard benchmark datasets, with run-times comparable or superior to the fastest state-of-the-art methods.},
author = {Sattler, Torsten and Leibe, Bastian and Kobbelt, Leif},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-642-33718-5_54},
file = {:home/nathan/Documents/Mendeley Desktop/Sattler, Leibe, Kobbelt/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Sattler, Leibe, Kobbelt{\_}2012{\_}Improving image-based localization by active correspondence search.pdf:pdf},
isbn = {9783642337178},
issn = {03029743},
number = {PART 1},
pages = {752--765},
title = {{Improving image-based localization by active correspondence search}},
volume = {7572 LNCS},
year = {2012}
}
@article{Ieee2017,
author = {Ieee, Senior Member and Ieee, Senior Member and Ieee, Fellow},
file = {:home/nathan/Documents/Mendeley Desktop/Ieee, Ieee, Ieee/Proceedings of the IEEE/Ieee, Ieee, Ieee{\_}2017{\_}On Combining Social Media and Spatial Technology for POI Cognition and Image Localization.pdf:pdf},
journal = {Proceedings of the IEEE},
keywords = {geolocalization,global positioning system,gps,i,i n t roduc,image retrieval,multimedia,place of interests,placing,poi,recommender system,social media,spatial technology,t ion,task},
title = {{On Combining Social Media and Spatial Technology for POI Cognition and Image Localization}},
year = {2017}
}
@inproceedings{Pinard2017,
author = {Pinard, Cl{\'{e}}ment and Chevalley, Laure and Manzanera, Antoine and Filliat, David},
booktitle = {Proceedings of the European Conference on Mobile Robotics},
file = {:home/nathan/Documents/Mendeley Desktop/Pinard et al/Proceedings of the European Conference on Mobile Robotics/Pinard et al.{\_}2017{\_}Multi range Real-time depth inference from a monocular stabilized footage using a Fully Convolutional Neural Network.pdf:pdf},
title = {{Multi range Real-time depth inference from a monocular stabilized footage using a Fully Convolutional Neural Network}},
year = {2017}
}
@inproceedings{Panphattarasap2016,
author = {Panphattarasap, Pilailuck and Calway, Andrew},
booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
file = {:home/nathan/Documents/Mendeley Desktop/Panphattarasap, Calway/Proceedings of the Asian Conference on Computer Vision (ACCV)/Panphattarasap, Calway{\_}2016{\_}Visual place recognition using landmark distribution descriptors.pdf:pdf},
title = {{Visual place recognition using landmark distribution descriptors}},
year = {2016}
}
@article{Liu2016a,
abstract = {The traditional environment maps built by mobile robots include both metric ones and topological ones. These maps are navigation-oriented and not adequate for service robots to interact with or serve human users who normally rely on the conceptual knowledge or semantic contents of the environment. Therefore, the construction of semantic maps becomes necessary for building an effective human-robot interface for service robots. This paper reviews recent research and development in the field of visual-based semantic mapping. The main focus is placed on how to extract semantic information from visual data in terms of feature extraction, object/place recognition and semantic representation methods.},
author = {Liu, Qiang and Li, Ruihao and Hu, Huosheng and Gu, Dongbing},
doi = {10.3390/robotics5010008},
file = {:home/nathan/Documents/Mendeley Desktop/Liu et al/Robotics/Liu et al.{\_}2016{\_}Extracting Semantic Information from Visual Data A Survey.pdf:pdf},
issn = {2218-6581},
journal = {Robotics},
keywords = {feature extraction,object recognition,place recognition,semantic map,semantic representation,visual data},
number = {1},
pages = {8},
title = {{Extracting Semantic Information from Visual Data: A Survey}},
url = {http://www.mdpi.com/2218-6581/5/1/8/htm},
volume = {5},
year = {2016}
}
@article{Radenovic2017,
abstract = {Image descriptors based on activations of Convolutional Neural Networks (CNNs) have become dominant in image retrieval due to their discriminative power, compactness of the representation, and the efficiency of search. Training of CNNs, either from scratch or fine-tuning, requires a large amount of annotated data, where high quality of the annotation is often crucial. In this work, we propose to fine-tune CNNs for image retrieval on a large collection of unordered images in a fully automatic manner. Reconstructed 3D models, obtained by the state-of-the-art retrieval and structure-from-motion methods, guide the selection of the training data. We show that both hard positive and hard negative examples, selected by exploiting the geometry and the camera positions available from the 3D models, enhance the performance in particular object retrieval. CNN descriptor whitening discriminatively learned from the same training data outperforms the commonly used PCA whitening. We propose a novel trainable Generalized-Mean (GeM) pooling layer that generalizes max and average pooling and show that it boosts retrieval performance. Applying the proposed method on VGG network achieves state-of-the-art performance on standard benchmarks: Oxford Buildings, Paris, and Holidays datasets.},
archivePrefix = {arXiv},
arxivId = {1711.02512},
author = {Radenovi{\'{c}}, Filip and Tolias, Giorgos and Chum, Ondřej},
eprint = {1711.02512},
file = {:home/nathan/Documents/Mendeley Desktop/Radenovi{\'{c}}, Tolias, Chum/arXiv/Radenovi{\'{c}}, Tolias, Chum{\_}2017{\_}Fine-tuning CNN Image Retrieval with No Human Annotation.pdf:pdf},
journal = {arXiv},
pages = {1--13},
title = {{Fine-tuning CNN Image Retrieval with No Human Annotation}},
url = {http://arxiv.org/abs/1711.02512},
year = {2017}
}
@inproceedings{Merzic2017,
abstract = {— A variety of end-user devices involving keypoint-based mapping systems are about to hit the market e.g. as part of smartphones, cars, robotic platforms, or virtual and augmented reality applications. Thus, the generated map data requires automated evaluation procedures that do not require experienced personnel or ground truth knowledge of the underlying environment. A particularly important question enabling commercial applications is whether a given map is of sufficient quality for localization. This paper proposes a framework for predicting localiza-tion performance in the context of visual landmark-based mapping. Specifically, we propose an algorithm for predicting performance of vision-based localization systems from different poses within the map. To achieve this, a metric is defined that assigns a score to a given query pose based on the underlying map structure. The algorithm is evaluated on two challenging datasets involving indoor data generated using a handheld device and outdoor data from an autonomous fixed-wing unmanned aerial vehicle (UAV). Using these, we are able to show that the score provided by our method is highly correlated to the true localization performance. Furthermore, we demonstrate how the predicted map quality can be used within a belief based path planning framework in order to provide reliable trajectories through high-quality areas of the map.},
author = {Merzi{\'{c}}, Hamza and Stumm, Elena and Dymczyk, Marcin and Siegwart, Roland and Gilitschenski, Igor},
booktitle = {Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)},
file = {:home/nathan/Documents/Mendeley Desktop/Merzi{\'{c}} et al/Proceedings of the IEEE International Conference of Robotics and Automation (ICRA)/Merzi{\'{c}} et al.{\_}2017{\_}Map Quality Evaluation for Visual Localization.pdf:pdf},
isbn = {9781509046324},
pages = {3200--3206},
title = {{Map Quality Evaluation for Visual Localization}},
year = {2017}
}
@article{Melekhov2017,
abstract = {In this paper, we propose an encoder-decoder convolutional neural network (CNN) architecture for estimating camera pose (orientation and location) from a single RGB-image. The architecture has a hourglass shape consisting of a chain of convolution and up-convolution layers followed by a regression part. The up-convolution layers are introduced to preserve the fine-grained information of the input image. Following the common practice, we train our model in end-to-end manner utilizing transfer learning from large scale classification data. The experiments demonstrate the performance of the approach on data exhibiting different lighting conditions, reflections, and motion blur. The results indicate a clear improvement over the previous state-of-the-art even when compared to methods that utilize sequence of test frames instead of a single frame.},
archivePrefix = {arXiv},
arxivId = {1703.07971},
author = {Melekhov, Iaroslav and Ylioinas, Juha and Kannala, Juho and Rahtu, Esa},
eprint = {1703.07971},
file = {:home/nathan/Documents/Mendeley Desktop/Melekhov et al/arXiv preprint/Melekhov et al.{\_}2017{\_}Image-based Localization using Hourglass Networks.pdf:pdf},
journal = {arXiv preprint},
title = {{Image-based Localization using Hourglass Networks}},
url = {http://arxiv.org/abs/1703.07971},
year = {2017}
}
@inproceedings{Zhou2017a,
abstract = {We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. We achieve this by simultaneously training depth and camera pose estimation networks using the task of view synthesis as the supervisory signal. The networks are thus coupled via the view synthesis objective during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performing comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performing favorably with established SLAM systems under comparable input settings.},
archivePrefix = {arXiv},
arxivId = {1704.07813},
author = {Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G.},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.700},
eprint = {1704.07813},
file = {:home/nathan/Documents/Mendeley Desktop/Zhou et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Zhou et al.{\_}2017{\_}Unsupervised Learning of Depth and Ego-Motion from Video.pdf:pdf},
title = {{Unsupervised Learning of Depth and Ego-Motion from Video}},
url = {http://arxiv.org/abs/1704.07813},
year = {2017}
}
@article{Eitel2015,
abstract = {Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset and show recognition in challenging RGB-D real-world noisy settings.},
annote = {How do they perform modality fusion

Training on RGB and D separatly, dropping sofmax and adding fc to RGB and D ouput and fine tuning both the networks.
Used Jet depth representation (finded {\textgreater} than HHA)},
archivePrefix = {arXiv},
arxivId = {1507.06821},
author = {Eitel, Andreas and Springenberg, Jost Tobias and Spinello, Luciano and Riedmiller, Martin and Burgard, Wolfram},
doi = {10.1109/IROS.2015.7353446},
eprint = {1507.06821},
file = {:home/nathan/Documents/Mendeley Desktop/Eitel et al/Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)/Eitel et al.{\_}2015{\_}Multimodal deep learning for robust RGB-D object recognition.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
pages = {681--687},
title = {{Multimodal deep learning for robust RGB-D object recognition}},
volume = {2015-Decem},
year = {2015}
}
@inproceedings{Menze2015,
author = {Menze, Moritz and Geiger, Andreas},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{Object Scene Flow for Autonomous Vehicles}},
year = {2015}
}
@inproceedings{Radenovic2016,
annote = {R{\'{e}}sum{\'{e}}
CNN permettant de d{\'{e}}crire une image pour des taches de landmark image retrieval. Les auteurs se focalisent sur le fine tuning de reseaux existant en entrainant un reseau de neurone produisant un descripteur MAC par contrastive loss en utilisant deux CNN coupl{\'{e}}s (reseaux siamois). L'entrainent est fait sur une base de tuple comprenant une query, un hard positive example, et un set de hard negative example. Les tuples sont obtenu gr{\^{a}}ce {\`{a}} des reconstruction 3D de grosses bases d'images par SfM : le positive example est choisi en prenant une image avec une position de cam{\'{e}}ra proche de la query et inversement pour le hard negative example. Les auteurs introduisent {\'{e}}galement une nouvelle m{\'{e}}thode de withening et de r{\'{e}}duction de dimension du vecteur en sortie du CNN.

Ce que je n'ai pas compris
- Pourquoi ils disent que la taille max de l'image est 362*362 lors de l'entrainement et 1024*1024 lors des tests ?

Ce qui est int{\'{e}}ressant
- Apprentissage non directement supervis{\'{e}}

Critiques
- Ca a l'air moins bon que les travaux de Gordo et al. 2016
- Pas de temps de calcul
- Plus bas{\'{e}} landmark localization

A approfondir
- Architecture R-MAC

Computational load
Offline: Non pr{\'{e}}sis{\'{e}} 
Online: Non pr{\'{e}}sis{\'{e}}, juste plus rapide que les m{\'{e}}thodes bas{\'{e}} descripteurs locaux

Scalability
Scale: City
Scalability potential: Real},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Radenovi{\'{c}}, Filip and Tolias, Giorgos and Chum, Ondřej},
booktitle = {Proceedings of the IEEE European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-319-46448-0},
eprint = {1311.2901},
file = {:home/nathan/Documents/Mendeley Desktop/Radenovi{\'{c}}, Tolias, Chum/Proceedings of the IEEE European Conference on Computer Vision (ECCV)/Radenovi{\'{c}}, Tolias, Chum{\_}2016{\_}CNN Image Retrieval Learns from BoW Unsupervised Fine-Tuning with Hard Examples.pdf:pdf},
isbn = {978-3-319-46447-3},
issn = {0302-9743},
keywords = {cnn fine-tuning,image retrieval,unsupervised learning},
pages = {3--20},
title = {{CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples}},
url = {http://link.springer.com/10.1007/978-3-319-46448-0},
volume = {9905},
year = {2016}
}
@article{Bae2010,
abstract = {Rephotographers aim to recapture an existing photograph from the same viewpoint. A historical photograph paired with a well-aligned modern rephotograph can serve as a remarkable visualization of the passage of time. However, the task of rephotography is tedious and often imprecise, because reproducing the viewpoint of the original photograph is challenging. The rephotographer must disambiguate between the six degrees of freedom of 3D translation and rotation, and the confounding similarity between the effects of camera zoom and dolly. We present a real-time estimation and visualization technique for rephotography that helps users reach a desired viewpoint during capture. The input to our technique is a reference image taken from the desired viewpoint. The user moves through the scene with a camera and follows our visualization to reach the desired viewpoint. We employ computer vision techniques to compute the relative viewpoint difference. We guide 3D movement using two 2D arrows. We demonstrate the success of our technique by rephotographing historical images and conducting user studies.},
annote = {From Duplicate 1 (Computational rephotography - Bae, Soonmin; Agarwala, Aseem; Durand, Fr{\'{e}}do)

R{\{}{\'{e}}{\}}sum{\{}{\'{e}}{\}}

M{\{}{\'{e}}{\}}thode de recallage assisit{\{}{\'{e}}{\}}e par ordinateur pour la re-photographie. Trois {\{}{\'{e}}{\}}tapes dans la m{\{}{\'{e}}{\}}thod : la premi{\{}{\`{e}}{\}}re {\{}{\'{e}}{\}}tape est une reconstruction partielle de la structure 3D du site par s{\{}{\'{e}}{\}}t{\{}{\'{e}}{\}}ro vision, indexation par l'utilisateur des points communs entre l'image actuel des lieux et l'image d'origine, puis guidage de l'utilisateur pour qu'il se positionne au bon point de vue. (j'ai pas tout lu)

Ce que je n'ai pas compris

- Comment on retrouve la profondeur

Ce qui est int{\{}{\'{e}}{\}}ressant

- La m{\{}{\'{e}}{\}}thode de matching MSAC

Critiques

- Pas trop adapt{\{}{\'{e}}{\}} {\{}{\`{a}}{\}} notre syst{\{}{\`{e}}{\}}me dans la mesure ou on a seulement une requet et pas un flux d'image
- Utilisation de l'utilisateur pour faire le challenging match entre l'image du pass{\{}{\'{e}}{\}} et celle du future

A approfondir

- MSAC

From Duplicate 2 (Computational rephotography - Bae, Soonmin; Agarwala, Aseem; Durand, Fr{\'{e}}do)

R{\'{e}}sum{\'{e}}

M{\'{e}}thode de recallage assisit{\'{e}}e par ordinateur pour la re-photographie. Trois {\'{e}}tapes dans la m{\'{e}}thod : la premi{\`{e}}re {\'{e}}tape est une reconstruction partielle de la structure 3D du site par s{\'{e}}t{\'{e}}ro vision, indexation par l'utilisateur des points communs entre l'image actuel des lieux et l'image d'origine, puis guidage de l'utilisateur pour qu'il se positionne au bon point de vue. (j'ai pas tout lu)

Ce que je n'ai pas compris

- Comment on retrouve la profondeur

Ce qui est int{\'{e}}ressant

- La m{\'{e}}thode de matching MSAC

Critiques

- Pas trop adapt{\'{e}} {\`{a}} notre syst{\`{e}}me dans la mesure ou on a seulement une requet et pas un flux d'image
- Utilisation de l'utilisateur pour faire le challenging match entre l'image du pass{\'{e}} et celle du future

A approfondir

- MSAC

Computational load
Offline:
Online:

Scalability
Scale:
Scalability potential:},
author = {Bae, Soonmin and Agarwala, Aseem and Durand, Fr{\'{e}}do},
doi = {10.1145/1805964.1805968},
file = {:home/nathan/Documents/Mendeley Desktop/Bae, Agarwala, Durand/ACM Transactions on Graphics (ToG)/Bae, Agarwala, Durand{\_}2010{\_}Computational rephotography.pdf:pdf},
isbn = {0730-0301},
issn = {07300301},
journal = {ACM Transactions on Graphics (ToG)},
keywords = {5 points algorithm,Alignement,Re-Photography,Robotic,SIFT},
mendeley-tags = {5 points algorithm,Alignement,Re-Photography,Robotic,SIFT},
number = {3},
pages = {1--15},
title = {{Computational rephotography}},
volume = {29},
year = {2010}
}
@inproceedings{Torii2015,
abstract = {We address the problem of large-scale visual place recognition for situations where the scene undergoes a major change in appearance, for example, due to illumination (day/night), change of seasons, aging, or structural modifications over time such as buildings built or destroyed. Such situations represent a major challenge for current large scale place recognition methods. This work has the following three principal contributions. First, we demonstrate that matching across large changes in the scene appearance becomes much easier when both the query image and the database image depict the scene from approximately the same viewpoint. Second, based on this observation, we develop a new place recognition approach that combines (i) an efficient synthesis of novel views with (ii) a compact indexable image representation. Third, we introduce a new challenging dataset of 1,125 camera-phone query images of Tokyo that contain major changes in illumination (day, sunset, night) as well as structural changes in the scene. We demonstrate that the proposed approach significantly outperforms other large-scale place recognition techniques on this challenging data.},
annote = {From Duplicate 1 (24/7 place recognition by view synthesis - Torii, Akihiko; Arandjelovic, Relja; Sivic, Josef; Okutomi, Masatoshi; Pajdla, Tomas)

R{\'{e}}sum{\'{e}}
M{\'{e}}thode de visual place recogniton bas{\'{e}} sur des descripteurs locaux dense (dense SIFT) et la synth{\`{e}}se de vues {\`{a}} partir de google street view. Les descripteurs denses fonctionnent quand le point de vue de l'image ne change pas beaucoup, d'ou l'interet de cr{\'{e}}er beaucoup d'images pour dans la base de donn{\'{e}}es pour qu'il y ait forc{\'{e}}ment une prise de vue similaire {\`{a}} la requete.

Ce que je n'ai pas compris
- La m{\'{e}}thode de matching des dense descriptors
- le VLAD descriptor
- la partie sur la creation des dictionnaires ou des GMMs (pourquoi on fait {\c{c}}a ? Pour moi ce n'est pas vraiment un probl{\`{e}}me de modelisation probabiliste)

Ce qui est int{\'{e}}ressant
- Le dataset
- Les r{\'{e}}sultats lors de changement de l'illumination ou d'architectures
- La m{\'{e}}thode de synth{\`{e}}se de vues a partir de panorama et d'information de profondeur basique
- L'aspect compression de donn{\'{e}}es (avec VLAD)

Critiques
- Dataset un peu petit
- Creation d'{\'{e}}norm{\'{e}}ment d'images pour une localisation sur un petite zone
- Artefacts lors de la cr{\'{e}}ation des vues synth{\'{e}}tiques
- AUCUN TEMPS DE CALCUL mentionn{\'{e}}
- 9 x plus d'images qu'une m{\'{e}}thode normale

A approfondir
- La synth{\'{e}}se de vue avec illuminiation diff{\'{e}}rentes
- La comprssion des donn{\'{e}}es
- VLAD
- RootSIFT

Computational load
Offline: Quiet heavy due to view sythesis and dense feature extraction
Online: 9 times longer than classical method I guess

Scalability
Scale: City
Scalability potential: Yes according to the autors

From Duplicate 2 (24/7 place recognition by view synthesis - Torii, Akihiko; Arandjelovic, Relja; Sivic, Josef; Okutomi, Masatoshi; Pajdla, Tomas)

R{\{}{\'{e}}{\}}sum{\{}{\'{e}}{\}}

M{\{}{\'{e}}{\}}thode de visual place recogniton bas{\{}{\'{e}}{\}} sur des descripteurs locaux dense (dense SIFT) et la synth{\{}{\`{e}}{\}}se de vues {\{}{\`{a}}{\}} partir de google street view. Les descripteurs denses fonctionnent quand le point de vue de l'image ne change pas beaucoup, d'ou l'interet de cr{\{}{\'{e}}{\}}er beaucoup d'images pour dans la base de donn{\{}{\'{e}}{\}}es pour qu'il y ait forc{\{}{\'{e}}{\}}ment une prise de vue similaire {\{}{\`{a}}{\}} la requete.

Ce que je n'ai pas compris

- La m{\{}{\'{e}}{\}}thode de matching des dense descriptors
- le VLAD descriptor
- la partie sur la creation des dictionnaires ou des GMMs (pourquoi on fait {\{}{\c{c}}{\}}a ? Pour moi ce n'est pas vraiment un probl{\{}{\`{e}}{\}}me de modelisation probabiliste)

Ce qui est int{\{}{\'{e}}{\}}ressant

- Le dataset
- Les r{\{}{\'{e}}{\}}sultats lors de changement de l'illumination ou d'architectures
- La m{\{}{\'{e}}{\}}thode de synth{\{}{\`{e}}{\}}se de vues a partir de panorama et d'information de profondeur basique
- L'aspect compression de donn{\{}{\'{e}}{\}}es (avec VLAD)

Critiques

- Dataset un peu petit
- Creation d'{\{}{\'{e}}{\}}norm{\{}{\'{e}}{\}}ment d'images pour une localisation sur un petite zone
- Artefacts lors de la cr{\{}{\'{e}}{\}}ation des vues synth{\{}{\'{e}}{\}}tiques
- AUCUN TEMPS DE CALCUL mentionn{\{}{\'{e}}{\}}
- 9 x plus d'images qu'une m{\{}{\'{e}}{\}}thode normale

A approfondir

- La synth{\{}{\'{e}}{\}}se de vue avec illuminiation diff{\{}{\'{e}}{\}}rentes
- La comprssion des donn{\{}{\'{e}}{\}}es
- VLAD
- RootSIFT},
author = {Torii, Akihiko and Arandjelovi{\'{c}}, Relja and Sivic, Josef and Okutomi, Masatoshi and Pajdla, Tomas},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Torii et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Torii et al.{\_}2015{\_}247 place recognition by view synthesis.pdf:pdf},
keywords = {Challenging matching,Database creation,Local features,Long terme place recognition,Machine learning,Outdoor,PCA,RGB request,SIFT,Supervised learning,View Synthesis,Visual place recognition},
mendeley-tags = {Challenging matching,Database creation,Local features,Long terme place recognition,Machine learning,Outdoor,PCA,RGB request,SIFT,Supervised learning,View Synthesis,Visual place recognition},
title = {24/7 place recognition by view synthesis},
year = {2015}
}
@article{Yi2017,
archivePrefix = {arXiv},
arxivId = {1711.05971},
author = {Yi, Kwang Moo and Trulls, Eduard and Ono, Yuki and Lepetit, Vincent and Salzmann, Mathieu and Fua, Pascal},
eprint = {1711.05971},
file = {:home/nathan/Documents/Mendeley Desktop/Yi et al/arXiv preprint/Yi et al.{\_}2017{\_}Learning to Find Good Correspondences.pdf:pdf},
journal = {arXiv preprint},
title = {{Learning to Find Good Correspondences}},
year = {2017}
}
@inproceedings{Lu2015,
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de localisation global d'une cam{\'{e}}ra {\`{a}} partir d'un mod{\`{e}}le 3D comme query et d'un mod{\`{e}}le 3D o{\`{u}} localiser la cam{\'{e}}ra. Les mod{\`{e}}les sont obtenus par SfM (vid{\'{e}}o de 2-3s en input pour la query). Syst{\`{e}}me en 3 {\'{e}}tapes : extractions de candidats potentiels par s{\'{e}}gmentation du mod{\`{e}}le 3D en input (extraction d'objet, puis matching par spherical function map avec la base de donn{\'{e}}e), ensuite matching des features 3D (d{\'{e}}crites par plusieurs points SIFT) en utilisant un mod{\`{e}}le MTL, et enfin estimation de la pose pr{\'{e}}cise de la cam{\'{e}}ra.
Les auteurs utilisent {\'{e}}galement leur m{\'{e}}thode pour faire de la localisation d'images issues de cam{\'{e}}ra thermique (images calibr{\'{e}}es avec des images RGB).

Ce que je n'ai pas compris
- Les MTL

Ce qui est int{\'{e}}ressant
- L'utilisation en query d'un mod{\`{e}}le 3D
- Le multi domaine (thermique/classique)
- Les r{\'{e}}sultats
- Utilisation de donn{\'{e}}es s{\'{e}}mantiques pour r{\'{e}}duire l'espace de recherche (segmentation)

Critiques
- Construction SfM etonnament performante (sur mur blanc ?)
- Cout calculatoire un peu {\'{e}}lev{\'{e}} (m{\^{e}}me s'ils disent que c'est + rapide que les autres m{\'{e}}thodes)
- Besoin d'info SIFT sur les points 3D
- Aucune info sur l'estimation de pose 3D/3D ni r{\'{e}}sultats

A approfondir
- MPEG-7 edge-orientation histogram features

Computational load
Offline: Cr{\'{e}}ation des models 3D + extraction
Online: 2s pour segmenter (opti CUDA) + 0.33s pour retrouver les inliers dans les candidats potentiels

Scalability
Scale: Large place
Scalability potential: High because of the first step of potential candidate retriveal},
author = {Lu, Guoyu and Yan, Yan and Ren, Li and Song, Jingkuan and Sebe, Nicu and Kambhamettu, Chandra},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.280},
file = {:home/nathan/Documents/Mendeley Desktop/Lu et al/Proceedings of the IEEE International Conference on Computer Vision (ICCV)/Lu et al.{\_}2015{\_}Localize Me Anywhere, Anytime A Multi-task Point-Retrieval Approach.pdf:pdf},
pages = {2434--2442},
title = {{Localize Me Anywhere, Anytime: A Multi-task Point-Retrieval Approach}},
year = {2015}
}
@inproceedings{Dalal2005,
author = {Dalal, Navneet and Triggs, Bill},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
organization = {IEEE},
pages = {886--893},
title = {{Histograms of oriented gradients for human detection}},
volume = {1},
year = {2005}
}
@article{Bonardi2017,
abstract = {Long-term place recognition in outdoor environments remains a challenge due to high appearance changes in the environment. The problem becomes even more difficult when the matching between two scenes has to be made with information coming from different visual sources, particularly with different spectral ranges. For instance, an infrared camera is helpful for night vision in combination with a visible camera. In this paper, we emphasize our work on testing usual feature point extractors under both constraints: repeatability across spectral ranges and long-term appearance. We develop a new feature extraction method dedicated to improve the repeatability across spectral ranges. We conduct an evaluation of feature robustness on long-term datasets coming from different imaging sources (optics, sensors size and spectral ranges) with a Bag-of-Words approach. The tests we perform demonstrate that our method brings a significant improvement on the image retrieval issue in a visual place recognition context, particularly when there is a need to associate images from various spectral ranges such as infrared and visible: we have evaluated our approach using visible, Near InfraRed (NIR), Short Wavelength InfraRed (SWIR) and Long Wavelength InfraRed (LWIR).},
author = {Bonardi, Fabien and Ainouz, Samia and Boutteau, R{\'{e}}mi and Dupuis, Yohan and Savatier, Xavier and Vasseur, Pascal},
doi = {10.3390/s17051167},
file = {:home/nathan/Documents/Mendeley Desktop/Bonardi et al/Sensors/Bonardi et al.{\_}2017{\_}PHROG A Multimodal Feature for Place Recognition.pdf:pdf},
isbn = {3323295501},
issn = {1424-8220},
journal = {Sensors},
keywords = {cross,feature extraction,scene matching,spectral imaging,visual place recognition},
number = {6},
pages = {1167},
title = {{PHROG: A Multimodal Feature for Place Recognition}},
url = {http://www.mdpi.com/1424-8220/17/5/1167},
volume = {17},
year = {2017}
}
@article{Oliva2001,
abstract = {In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimen-sional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.},
author = {Oliva, Aude and Torralba, Antonio},
doi = {10.1023/A:1011139631724},
file = {:home/nathan/Documents/Mendeley Desktop/Oliva, Torralba/International Journal of Computer Vision (IJCV)/Oliva, Torralba{\_}2001{\_}Modeling the shape of the scene A holistic representation of the spatial envelope.pdf:pdf},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision (IJCV)},
keywords = {Energy spectrum,Natural images,Principal components,Scene recognition,Spatial layout},
number = {3},
pages = {145--175},
pmid = {1113597138},
title = {{Modeling the shape of the scene: A holistic representation of the spatial envelope}},
volume = {42},
year = {2001}
}
@inproceedings{Salas-Moreno2013,
abstract = {We present the major advantages of a new ‘object ori- ented' 3D SLAM paradigm, which takes full advantage in the loop of prior knowledge that many scenes consist of repeated, domain-specific objects and structures. As a hand-held depth camera browses a cluttered scene, real- time 3D object recognition and tracking provides 6DoF camera-object constraints which feed into an explicit graph of objects, continually refined by efficient pose-graph opti- misation. This offers the descriptive and predictive power of SLAM systems which perform dense surface reconstruc- tion, but with a huge representation compression. The ob- ject graph enables predictions for accurate ICP-based cam- era to model tracking at each live frame, and efficient ac- tive search for new objects in currently undescribed image regions. We demonstrate real-time incremental SLAM in large, cluttered environments, including loop closure, relo- calisation and the detection of moved objects, and of course the generation of an object level scene description with the potential to enable interaction.},
annote = {R{\'{e}}sum{\'{e}}
SLAM s{\'{e}}mantique utilisant des objets 3D pr{\'{e}}-enregistr{\'{e}}s pour se localiser. Sauvegarde des objets par scanning (kinect fusion) puis reconnaissance en temps r{\'{e}}el en utilisant des PPFs (Point-Pair Features). Sauvegarde de l'environnement par un graph reliant la position des diff{\'{e}}rents objets et de la cam{\'{e}}ra. Relocalisation et loop-closure par graph matching.

Ce qui est int{\'{e}}ressant
- L'astraction s{\'{e}}mantique
- Le temps r{\'{e}}el par GPGPU
- Feature 3D

Critiques
- Pas de description de la m{\'{e}}thode de graph matching
- Necessite d'avoir le mod{\`{e}}le des objets pr{\'{e}}-enregistr{\'{e}}

Computational load
Offline: - (SLAM online only)
Online: RT

Scalability
Scale: Tiny
Scalability potential: Depend on the vocalbulary size of 3D objets},
author = {Salas-Moreno, Renato F. and Newcombe, Richard A. and Strasdat, Hauke and Kelly, Paul H. J. and Davison, Andrew J.},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2013.178},
file = {:home/nathan/Documents/Mendeley Desktop/Salas-Moreno et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Salas-Moreno et al.{\_}2013{\_}SLAM Simultaneous localisation and mapping at the level of objects.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
keywords = {GPGPU,ICP,KinectFusion,SLAM,augmented reality,object recognition,object-oriented,objects,real-time,scene understanding},
pages = {1352--1359},
title = {{SLAM++: Simultaneous localisation and mapping at the level of objects}},
year = {2013}
}
@inproceedings{Leutenegger2011brisk,
author = {Leutenegger, Stefan and Chli, Margarita and Siegwart, Roland},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
organization = {IEEE},
pages = {2548--2555},
title = {{BRISK: Binary robust invariant scalable keypoints}},
year = {2011}
}
@article{Muhlfellner2015,
abstract = {It is anticipatedthat theMars ScienceLaboratory rover, namedCuriosity,will traverse 10–20 kmon the surface of Mars during its primary mission. In preparation for this traverse, Earth-based tests were performed using Mars weight vehicles. These vehicles were driven over Mars analog bedrock, cohesive soil, and cohesionless sand at various slopes. Vehicle slip was characterized on each of these terrains versus slope for direct upslope driving. Results show that slopes up to 22 degrees are traversable on smooth bedrock and that slopes up to 28 degrees are traversable on some cohesive soils. In cohesionless sand, results show a sharp transition between moderate slip on 10 degree slopes and vehicle embedding at 17 degrees. For cohesionless sand, data are also presented showing the relationship between vehicle slip and wheel sinkage. Side by side testing of the Mars Exploration Rover test vehicle and the Mars Science Laboratory test vehicle show how increased wheel diameter leads to better slope climbing ability in sand for vehicles with nearly identical ground pressure. Lastly, preliminary data from Curiosity's initial driving on Mars are presented and compared to the Earth-based testing, showing good agreement for the driving done during the first 250 Martian days.},
annote = {Use BRISK

Utilise plusieurs exp{\'{e}}riences d'une m{\^{e}}me carte qu'ils stockent dans une base de donn{\'{e}}e. La summary map consiste {\`{a}} prendre des keypoints pertinent dans la base de donn{\'{e}}e pour cr{\'{e}}er une map de petite taille pour la tache de localization {\`{a}} un moment donn{\'{e}}e.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {M{\"{u}}hlfellner, Peter and B{\"{u}}rki, Mathias and Bosse, Michael and Derendarz, Wojciech and Philippsen, Roland and Furgale, Paul},
doi = {10.1002/rob},
eprint = {10.1.1.91.5767},
file = {:home/nathan/Documents/Mendeley Desktop/M{\"{u}}hlfellner et al/Journal of Field Robotics/M{\"{u}}hlfellner et al.{\_}2015{\_}Summary Maps for Lifelong Visual Localization.pdf:pdf},
isbn = {9783902661623},
issn = {14746670},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
number = {0},
pages = {245--267},
pmid = {22164016},
title = {{Summary Maps for Lifelong Visual Localization}},
volume = {23},
year = {2015}
}
@inproceedings{Valentin2015,
annote = {R{\'{e}}sum{\'{e}}
Am{\'{e}}lioration de la m{\'{e}}thode de camera relocalization introduite par Shotton et al. 2013 par regression forest. Les auteurs reprennent la pipeline classique de la regression de pose au niveau des pixels en mod{\'{e}}lisant l'insertitude attach{\'{e}} a chacune des feuilles de l'arbre de regression sur la position du pixel par une mixture de Gaussienne. Ils utilisent ensuite cette information pour guider le RANSAC permettant de retrouver la pose de la camera.

Ce que je n'ai pas compris
- La m{\'{e}}thode d'optimisation de la position en utilisant l'insertitude des feuilles

Ce qui est int{\'{e}}ressant
- State of the art result sur le benchmark seven scenes

Critiques
- Pas d'indiquation de l'augmentation des temps de calcul par rapport {\`{a}} la m{\'{e}}thode originale

A approfondir
- Le RANSAC modifi{\'{e}} utilis{\'{e}}

Computational load
Offline: ?
Online: ?

Scalability
Scale: Room
Scalability potential: ?},
author = {Valentin, Julien and Fitzgibbon, Andrew and Nie{\ss}ner, Matthias and Shotton, Jamie and Torr, Philip H. S.},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/nathan/Documents/Mendeley Desktop/Valentin et al/Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Valentin et al.{\_}2015{\_}Exploiting Uncertainty in Regression Forests for Accurate Camera Relocalization.pdf:pdf},
isbn = {9781467369640},
pages = {4400--4408},
title = {{Exploiting Uncertainty in Regression Forests for Accurate Camera Relocalization}},
year = {2015}
}
@article{Chen2017b,
abstract = {In this work, we propose to utilize Convolutional Neural Networks to boost the performance of depth-induced salient object detection by capturing the high-level representative features for depth modality. We formulate the depth-induced saliency detection as a CNN-based cross-modal transfer problem to bridge the gap between the "data-hungry" nature of CNNs and the unavailability of sufficient labeled training data in depth modality. In the proposed approach, we leverage the auxiliary data from the source modality effectively by training the RGB saliency detection network to obtain the task-specific pre-understanding layers for the target modality. Meanwhile, we exploit the depth-specific information by pre-training a modality classification network that encourages modal-specific representations during the optimizing course. Thus, it could make the feature representations of the RGB and depth modalities as discriminative as possible. These two modules are pre-trained independently and then stitched to initialize and optimize the eventual depth-induced saliency detection model. Experiments demonstrate the effectiveness of the proposed novel pre-training strategy as well as the significant and consistent improvements of the proposed approach over other state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1703.00122},
author = {Chen, Hao and Li, Y F and Su, Dan},
eprint = {1703.00122},
file = {:home/nathan/Documents/Mendeley Desktop/Chen, Li, Su/HttpArxiv.OrgAbs1703.00122V1/Chen, Li, Su{\_}2017{\_}RGB-D Salient Object Detection Based on Discriminative Cross-modal Transfer Learning.pdf:pdf},
journal = {Http://Arxiv.Org/Abs/1703.00122V1},
pages = {1--10},
title = {{RGB-D Salient Object Detection Based on Discriminative Cross-modal Transfer Learning}},
year = {2017}
}
@inproceedings{Wu2014,
abstract = {— An important problem in robot simultaneous localization and mapping (SLAM) is loop closure detection. Recent studies of the problem have led to successful development of methods that are based on images captured by the robot. These methods tackle the issue of efficiency through data structures such as indexing and hierarchical (tree) organization of the image data that represent the robot map. In this paper, we offer an alternative approach and present a novel method for visual loopclosure detection. Our approach uses an extremely simple image representation, namely, a downsampled binarized version of the original image, combined with a highly efficient image similarity measure mutual information. As a result, our method is able to perform loop closure detection in a map with 20 million key locations in about 2.38 seconds on a commodity computer. The excellent performance of our method in terms of its low complexity and accuracy in experiments establishes it as a promising solution to loop closure detection in largescale robot maps.},
author = {Wu, Junjun and Zhang, Hong and Guan, Yisheng},
booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
file = {:home/nathan/Documents/Mendeley Desktop/Wu, Zhang, Guan/Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)/Wu, Zhang, Guan{\_}2014{\_}An Efficient Visual Loop Closure Detection Method in a Map of 20 Million Key Locations.pdf:pdf},
keywords = {Localization,Mapping,SLAM},
title = {{An Efficient Visual Loop Closure Detection Method in a Map of 20 Million Key Locations}},
year = {2014}
}
@inproceedings{Torii2011,
abstract = {We seek to predict the GPS location of a query image given a database of images localized on a map with known GPS locations. The contributions of this work are three-fold: (1) we formulate the image-based localization problem as a regression on an image graph with images as nodes and edges connecting close-by images; (2) we design a novel image matching procedure, which computes similarity between the query and pairs of database images using edges of the graph and considering linear combinations of their feature vectors. This improves generalization to unseen viewpoints and illumination conditions, while reducing the database size; (3) we demonstrate that the query location can be predicted by interpolating locations of matched images in the graph without the costly estimation of multi-view geometry. We demonstrate benefits of the proposed image matching scheme on the standard Oxford building benchmark, and show localization results on a database of 8,999 panoramic Google Street View images of Pittsburgh.},
annote = {R{\'{e}}sum{\'{e}}
M{\'{e}}thode de localisation bas{\'{e}} vision qui permet de retrouver la position GPS d'une image {\`{a}} partir d'un graph d'images geolocalis{\'{e}}s. Les auteurs utilisent un graph d'images sph{\'{e}}riques g{\'{e}}olocalis{\'{e}} reli{\'{e}} en fonction de leur proximit{\'{e}} (obtenu via google street view), et retrouve un set d'image similaire par m{\'{e}}thode indirect BoF (tf-idf weighted). Ils font ensuite un match du feature vector de la query dans un domain continu obtenu par interpolation des feature vectors des sous-images r{\'{e}}cup{\'{e}}r{\'{e}}es. Ils inteprollent finalement la position GPS de la query en pond{\'{e}}rant les diff{\'{e}}rentes positions des images similaires. 

Ce que je n'ai pas compris
- Pas tout bien lu


Ce qui est int{\'{e}}ressant
- M{\'{e}}thode relativement origninale
- Augment {\'{e}}galmement le taux d'image retrouv{\'{e}} sur des bases de donn{\'{e}}es non-sph{\'{e}}riques.
- Etat de l'art qui relie pas mal de m{\'{e}}thodes differentes de VML

Critiques
- Comparasion avec l'{\'{e}}tat de l'art ?
- Pas de temps de calcul
- On a besoin d'image sph{\'{e}}riques


Computational load
Offline: ?
Online: ?

Scalability
Scale: City
Scalability potential: Complicated},
author = {Torii, Akihiko and Sivic, Josef and Pajdla, Tomas},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision Workshop (ICCVW)},
doi = {10.1109/ICCVW.2011.6130230},
file = {:home/nathan/Documents/Mendeley Desktop/Torii, Sivic, Pajdla/Proceedings of the IEEE International Conference on Computer Vision Workshop (ICCVW)/Torii, Sivic, Pajdla{\_}2011{\_}Visual localization by linear combination of image descriptors.pdf:pdf},
isbn = {9781467300629},
title = {{Visual localization by linear combination of image descriptors}},
year = {2011}
}
